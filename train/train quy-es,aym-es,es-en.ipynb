{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AYM_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUY_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZER_PATH=/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZER_PATH = /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_path_es_en = \"/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/\"\n",
    "tokenized_path_quy_es = \"/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/\"\n",
    "tokenized_path_aym_es = \"/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZED_PATH_ES_EN=/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en\n",
      "env: TOKENIZED_PATH_QUY_ES=/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es\n",
      "env: TOKENIZED_PATH_AYM_ES=/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZED_PATH_ES_EN = /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en\n",
    "%env TOKENIZED_PATH_QUY_ES = /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es\n",
    "%env TOKENIZED_PATH_AYM_ES = /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BIN_DIR=/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "%env BIN_DIR = /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_DIR=/storage/master-thesis/models/quy-es_aym-es_es-en\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_DIR = /storage/master-thesis/models/quy-es_aym-es_es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $TOKENIZED_PATH_ES_EN\n",
    "! mkdir -p $TOKENIZED_PATH_QUY_ES\n",
    "! mkdir -p $TOKENIZED_PATH_AYM_ES\n",
    "! mkdir -p $BIN_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locale: Cannot set LC_CTYPE to default locale: No such file or directory\n",
      "locale: Cannot set LC_MESSAGES to default locale: No such file or directory\n",
      "locale: Cannot set LC_ALL to default locale: No such file or directory\n",
      "LANG=en_US.UTF-8\n",
      "LANGUAGE=\n",
      "LC_CTYPE=\"en_US.UTF-8\"\n",
      "LC_NUMERIC=\"en_US.UTF-8\"\n",
      "LC_TIME=\"en_US.UTF-8\"\n",
      "LC_COLLATE=\"en_US.UTF-8\"\n",
      "LC_MONETARY=\"en_US.UTF-8\"\n",
      "LC_MESSAGES=\"en_US.UTF-8\"\n",
      "LC_PAPER=\"en_US.UTF-8\"\n",
      "LC_NAME=\"en_US.UTF-8\"\n",
      "LC_ADDRESS=\"en_US.UTF-8\"\n",
      "LC_TELEPHONE=\"en_US.UTF-8\"\n",
      "LC_MEASUREMENT=\"en_US.UTF-8\"\n",
      "LC_IDENTIFICATION=\"en_US.UTF-8\"\n",
      "LC_ALL=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "! locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: update-locale: not found\n",
      "env: LANG=en_US.UTF-8\n",
      "env: LC_CTYPE=en_US.UTF-8\n",
      "env: LC_ALL=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "! update-locale LANG=en_US.UTF-8 LANGUAGE=en.UTF-8\n",
    "\n",
    "%env LANG=en_US.UTF-8\n",
    "%env LC_CTYPE=en_US.UTF-8\n",
    "%env LC_ALL=en_US.UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.10.3 in /usr/local/lib/python3.6/dist-packages (0.10.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers==0.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.9.1\n",
      "Uninstalling torch-1.9.1:\n",
      "  Successfully uninstalled torch-1.9.1\n",
      "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall torch torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.2%2Bcu113-cp36-cp36m-linux_x86_64.whl (1821.5 MB)\n",
      "\u001b[K     |################################| 1821.5 MB 1.5 kB/s eta 0:00:0101            | 761.9 MB 4.0 MB/s eta 0:04:26   |#################               | 1015.3 MB 3.9 MB/s eta 0:03:26     |#######################         | 1322.7 MB 1.4 MB/s eta 0:05:53\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.11.3%2Bcu113-cp36-cp36m-linux_x86_64.whl (24.6 MB)\n",
      "\u001b[K     |################################| 24.6 MB 110 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.10.2%2Bcu113-cp36-cp36m-linux_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |################################| 2.9 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (8.3.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.10.2+cu113 torchaudio-0.10.2+cu113 torchvision-0.11.3+cu113\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: fairseq 0.10.2\n",
      "Uninstalling fairseq-0.10.2:\n",
      "  Successfully uninstalled fairseq-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall fairseq -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairseq\n",
      "  Downloading fairseq-0.12.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.0 MB)\n",
      "\u001b[K     |################################| 11.0 MB 22.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.20.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
      "Collecting bitarray\n",
      "  Downloading bitarray-2.6.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "\u001b[K     |################################| 235 kB 90.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n",
      "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |################################| 123 kB 87.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.51.0)\n",
      "Collecting omegaconf<2.1\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (2.0.0)\n",
      "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.10.2+cu113)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.10.2+cu113)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.3)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2020.11.13)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.24)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (5.2.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq) (3.10.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.6/dist-packages (from omegaconf<2.1->fairseq) (5.4.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.4)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.3.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1,>=1.0.7->fairseq) (3.4.0)\n",
      "Installing collected packages: bitarray, omegaconf, hydra-core, fairseq\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.1.1\n",
      "    Uninstalling omegaconf-2.1.1:\n",
      "      Successfully uninstalled omegaconf-2.1.1\n",
      "  Attempting uninstall: hydra-core\n",
      "    Found existing installation: hydra-core 1.1.1\n",
      "    Uninstalling hydra-core-1.1.1:\n",
      "      Successfully uninstalled hydra-core-1.1.1\n",
      "Successfully installed bitarray-2.6.0 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fairseq # --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: apex 0.1\n",
      "Uninstalling apex-0.1:\n",
      "  Successfully uninstalled apex-0.1\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'apex': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm -r apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/NVIDIA/apex\n",
    "\n",
    "! pip install -v --disable-pip-version-check --no-cache-dir apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |################################| 125 kB 26.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.13.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (50.3.2)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.5.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tatoeba format to standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(EN_ES_CORPUS_DIR + \"original/valid.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"dev.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"dev.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "    en.close()\n",
    "    es.close()\n",
    "    \n",
    "with open(EN_ES_CORPUS_DIR + \"original/dev.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"train.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"train.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        if \"\\n\" in line[2] or \"\\n\" in line[3]:\n",
    "            continue\n",
    "        if len(line) != 4:\n",
    "            continue\n",
    "            \n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "        \n",
    "    en.close()\n",
    "    es.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "en_es_quy_aym_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "en_es_quy_aym_trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "en_es_quy_aym_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "en_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.en\" for split in [\"dev\", \"train\"]]\n",
    "es_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "quy_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/{split}.quy\" for split in [\"dev\", \"train\"]]\n",
    "es2_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "aym_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/{split}.aym\" for split in [\"dev\", \"train\"]]\n",
    "es3_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "\n",
    "files = en_files + es_files + es2_files + quy_files + aym_files + es3_files\n",
    "#print(files)\n",
    "en_es_quy_aym_tokenizer.train(files= files, trainer=en_es_quy_aym_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cat /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_files(tokenizer, files, extension, output_path):\n",
    "    for file in files:\n",
    "        print(f\"Reading file {file}\")\n",
    "        with open(file, encoding='utf8') as f:\n",
    "          lines = f.readlines()\n",
    "          tokenized_lines = tokenizer.encode_batch(lines)\n",
    "          tokenized_name = Path(file).stem\n",
    "          tokenized_name = output_path + tokenized_name + \".\" + extension\n",
    "          print(tokenized_name)\n",
    "          with open(tokenized_name, 'w', encoding='utf8') as wf:\n",
    "\n",
    "            wf.writelines([\" \".join(t.tokens) + \"\\n\" for t in tokenized_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_quy_aym_tokenizer.save(tokenizer_path + \"en-es-quy-aym-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.Tokenizer object at 0x62ad650>\n"
     ]
    }
   ],
   "source": [
    "print(en_es_quy_aym_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'hold', 'this', 'truth', 'to', 'be', 'self', 'evid', '##ently', 'that', 'everyone', 'is', 'created', 'equal', '?']\n"
     ]
    }
   ],
   "source": [
    "tok = en_es_quy_aym_tokenizer.encode(\"we hold this truth to be self evidently that everyone is created equal?\")\n",
    "print(tok.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.en\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/dev.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.en\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/train.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.es\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.es\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.quy\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/dev.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.quy\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/train.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.es\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.es\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/dev.aym\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/dev.aym\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/train.aym\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/train.aym\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/dev.es\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/train.es\n",
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/train.es\n"
     ]
    }
   ],
   "source": [
    "tokenize_files(en_es_quy_aym_tokenizer, en_files, \"en\", tokenized_path_es_en)\n",
    "tokenize_files(en_es_quy_aym_tokenizer, es_files, \"es\", tokenized_path_es_en)\n",
    "\n",
    "tokenize_files(en_es_quy_aym_tokenizer, quy_files, \"quy\", tokenized_path_quy_es)\n",
    "tokenize_files(en_es_quy_aym_tokenizer, es2_files, \"es\", tokenized_path_quy_es)\n",
    "\n",
    "tokenize_files(en_es_quy_aym_tokenizer, aym_files, \"aym\", tokenized_path_aym_es)\n",
    "tokenize_files(en_es_quy_aym_tokenizer, es3_files, \"es\", tokenized_path_aym_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.quy.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.aym.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.es.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.en.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "### Removing previous dict files\n",
    "! rm $BIN_DIR/dict.quy.txt\n",
    "! rm $BIN_DIR/dict.aym.txt\n",
    "! rm $BIN_DIR/dict.es.txt\n",
    "! rm $BIN_DIR/dict.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenating all training data\n",
    "! cat $TOKENIZED_PATH_QUY_ES/train.quy $TOKENIZED_PATH_QUY_ES/train.es $TOKENIZED_PATH_AYM_ES/train.aym $TOKENIZED_PATH_AYM_ES/train.es $TOKENIZED_PATH_ES_EN/train.es $TOKENIZED_PATH_ES_EN/train.en > $BIN_DIR/train.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Untitled.ipynb\n",
      " bar.txt\n",
      " \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/\n",
      " den.pt\n",
      " \u001b[01;34mdocker\u001b[0m/\n",
      " \u001b[01;34mdocs\u001b[0m/\n",
      " \u001b[01;34mexamples\u001b[0m/\n",
      " \u001b[01;34mmaster-thesis\u001b[0m/\n",
      " \u001b[01;34mmodel_cards\u001b[0m/\n",
      " norm.pt\n",
      " \u001b[01;34mnotebooks\u001b[0m/\n",
      " num.pt\n",
      "\u001b[01;34m'old scripts'\u001b[0m/\n",
      " pos.pt\n",
      " s.pt\n",
      " \u001b[01;34mscripts\u001b[0m/\n",
      " sj.pt\n",
      " \u001b[01;34msrc\u001b[0m/\n",
      " \u001b[01;34mtemplates\u001b[0m/\n",
      "'test 10nal-es + es-en model.ipynb'\n",
      "'test capsulenet quy-es + es-en model.ipynb'\n",
      "'test it-nl-ro-en model.ipynb'\n",
      "'test nl-it-en + de-en model.ipynb'\n",
      "'test quy-es + aym-es,en + es-en model.ipynb'\n",
      "'test quy-es + es-en model.ipynb'\n",
      "'test quy-es -> es-en model.ipynb'\n",
      "'test residual drop quy-es + es-en model.ipynb'\n",
      " \u001b[01;34mtests\u001b[0m/\n",
      "'train 10nal-es + es-en model.ipynb'\n",
      "'train aym-es,es-en.ipynb'\n",
      "'train capsulenet nl-it-en + de-en model.ipynb'\n",
      "'train capsulenet quy-es + es-en model.ipynb'\n",
      "'train es-en model.ipynb'\n",
      "'train it-nl-ro-en model.ipynb'\n",
      "'train nah-es,es-en.ipynb'\n",
      "'train nl-en,it-en,ro-en,en-de.ipynb'\n",
      "'train quy-es + aym-es,en + es-en model.ipynb'\n",
      "'train quy-es,aym-es,es-en.ipynb'\n",
      "'train quy-es,aym-es.ipynb'\n",
      "'train quy-es,es-en.ipynb'\n",
      "'train quy-es[pending].ipynb'\n",
      "'train residual drop quy-es + es-en model.ipynb'\n",
      " \u001b[01;34mutils\u001b[0m/\n",
      " vj.pt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-19 22:17:16 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=True, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='all', srcdict=None, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/train', use_plasma_view=False, user_dir=None, validpref=None, wandb_project=None, workers=20)\n",
      "2022-09-19 22:17:22 | INFO | fairseq_cli.preprocess | [all] Dictionary: 29080 types\n",
      "2022-09-19 22:17:34 | INFO | fairseq_cli.preprocess | [all] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/train.all: 657672 sents, 8684043 tokens, 0.0% replaced (by <unk>)\n",
      "2022-09-19 22:17:34 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang all \\\n",
    "    --trainpref $BIN_DIR/train \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --workers 20 \\\n",
    "    --only-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! echo $BIN_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-19 22:17:37 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='quy', srcdict='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.all.txt', suppress_crashes=False, target_lang='es', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/train', use_plasma_view=False, user_dir=None, validpref='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/dev', wandb_project=None, workers=20)\n",
      "2022-09-19 22:17:37 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 29080 types\n",
      "2022-09-19 22:17:40 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/train.quy: 125008 sents, 1939495 tokens, 0.0% replaced (by <unk>)\n",
      "2022-09-19 22:17:40 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 29080 types\n",
      "2022-09-19 22:17:41 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/dev.quy: 996 sents, 14613 tokens, 0.315% replaced (by <unk>)\n",
      "2022-09-19 22:17:41 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29080 types\n",
      "2022-09-19 22:17:44 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/train.es: 125008 sents, 2456643 tokens, 0.0% replaced (by <unk>)\n",
      "2022-09-19 22:17:44 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29080 types\n",
      "2022-09-19 22:17:46 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/quy-es/dev.es: 996 sents, 15133 tokens, 0.291% replaced (by <unk>)\n",
      "2022-09-19 22:17:46 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang quy --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_QUY_ES/train --validpref $TOKENIZED_PATH_QUY_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-19 22:17:48 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='aym', srcdict='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.all.txt', suppress_crashes=False, target_lang='es', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/train', use_plasma_view=False, user_dir=None, validpref='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/dev', wandb_project=None, workers=20)\n",
      "2022-09-19 22:17:49 | INFO | fairseq_cli.preprocess | [aym] Dictionary: 29080 types\n",
      "2022-09-19 22:17:50 | INFO | fairseq_cli.preprocess | [aym] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/train.aym: 6531 sents, 204191 tokens, 0.0% replaced (by <unk>)\n",
      "2022-09-19 22:17:50 | INFO | fairseq_cli.preprocess | [aym] Dictionary: 29080 types\n",
      "2022-09-19 22:17:50 | INFO | fairseq_cli.preprocess | [aym] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/dev.aym: 996 sents, 18588 tokens, 0.247% replaced (by <unk>)\n",
      "2022-09-19 22:17:50 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29080 types\n",
      "2022-09-19 22:17:51 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/train.es: 6531 sents, 188095 tokens, 0.0% replaced (by <unk>)\n",
      "2022-09-19 22:17:51 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29080 types\n",
      "2022-09-19 22:17:52 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/aym-es/dev.es: 996 sents, 15133 tokens, 0.291% replaced (by <unk>)\n",
      "2022-09-19 22:17:52 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang aym --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_AYM_ES/train --validpref $TOKENIZED_PATH_AYM_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing es dict from aym-es preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-19 22:17:54 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='es', srcdict='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.all.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/train', use_plasma_view=False, user_dir=None, validpref='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/dev', wandb_project=None, workers=20)\n",
      "2022-09-19 22:17:55 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29080 types\n",
      "2022-09-19 22:17:58 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/train.es: 197297 sents, 1897042 tokens, 0.0% replaced (by <unk>)\n",
      "2022-09-19 22:17:58 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29080 types\n",
      "2022-09-19 22:17:58 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/dev.es: 4643 sents, 51918 tokens, 0.0366% replaced (by <unk>)\n",
      "2022-09-19 22:17:58 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29080 types\n",
      "2022-09-19 22:18:01 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/train.en: 197297 sents, 1998577 tokens, 0.0% replaced (by <unk>)\n",
      "2022-09-19 22:18:01 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29080 types\n",
      "2022-09-19 22:18:02 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/es-en/dev.en: 4643 sents, 54570 tokens, 0.0367% replaced (by <unk>)\n",
      "2022-09-19 22:18:02 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang es --target-lang en \\\n",
    "    --trainpref $TOKENIZED_PATH_ES_EN/train --validpref $TOKENIZED_PATH_ES_EN/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 08:42:50 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 200, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/storage/master-thesis/models/quy-es_aym-es_es-en/', 'restore_file': '/storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.1, lang_dict=None, lang_pairs='quy-es,aym-es,es-en', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=200, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=10, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='/storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='/storage/master-thesis/models/quy-es_aym-es_es-en/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_dict=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_dict=None, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=None, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='translation_multi_simple_epoch', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=2, label_smoothing=0.1, lang_dict=None, lang_pairs='quy-es,aym-es,es-en', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=200, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=10, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='/storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='/storage/master-thesis/models/quy-es_aym-es_es-en/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_dict=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_dict=None, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=None, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2022-09-20 08:42:50 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.\n",
      "2022-09-20 08:42:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aym', 'en', 'es', 'quy']\n",
      "2022-09-20 08:42:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | [quy] dictionary: 29084 types\n",
      "2022-09-20 08:42:50 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 29084 types\n",
      "2022-09-20 08:42:51 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(29084, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(29084, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=29084, bias=False)\n",
      "  )\n",
      ")\n",
      "2022-09-20 08:42:51 | INFO | fairseq_cli.train | task: TranslationMultiSimpleEpochTask\n",
      "2022-09-20 08:42:51 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2022-09-20 08:42:51 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2022-09-20 08:42:51 | INFO | fairseq_cli.train | num. shared model params: 77,931,520 (num. trained: 77,931,520)\n",
      "2022-09-20 08:42:51 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2022-09-20 08:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n",
      "2022-09-20 08:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:quy-es': 1, 'main:aym-es': 1, 'main:es-en': 1}\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 29083; tgt_langtok: 29082\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/valid.quy-es.quy\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/valid.quy-es.es\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin valid quy-es 996 examples\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aym-es src_langtok: 29080; tgt_langtok: 29082\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/valid.aym-es.aym\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/valid.aym-es.es\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin valid aym-es 996 examples\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 29082; tgt_langtok: 29081\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.data_utils | loaded 4,643 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/valid.es-en.es\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.data_utils | loaded 4,643 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/valid.es-en.en\n",
      "2022-09-20 08:42:51 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin valid es-en 4643 examples\n",
      "2022-09-20 08:42:57 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
      "2022-09-20 08:42:57 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2022-09-20 08:42:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-09-20 08:42:57 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 15.747 GB ; name = Quadro RTX 5000                         \n",
      "2022-09-20 08:42:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2022-09-20 08:42:57 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2022-09-20 08:42:57 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
      "2022-09-20 08:42:57 | INFO | fairseq.trainer | Preparing to load checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint_last.pt\n",
      "2022-09-20 08:43:00 | INFO | fairseq.trainer | Loaded checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint_last.pt (epoch 45 @ 0 updates)\n",
      "2022-09-20 08:43:00 | INFO | fairseq.trainer | loading train data for epoch 45\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=45/None\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:quy-es': 1, 'main:aym-es': 1, 'main:es-en': 1}\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 29083; tgt_langtok: 29082\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.data_utils | loaded 125,008 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/train.quy-es.quy\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.data_utils | loaded 125,008 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/train.quy-es.es\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin train quy-es 125008 examples\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aym-es src_langtok: 29080; tgt_langtok: 29082\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.data_utils | loaded 6,531 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/train.aym-es.aym\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.data_utils | loaded 6,531 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/train.aym-es.es\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin train aym-es 6531 examples\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 29082; tgt_langtok: 29081\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.data_utils | loaded 197,297 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/train.es-en.es\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.data_utils | loaded 197,297 examples from: /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin/train.es-en.en\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es_aym-es_es-en/tokenizers/quy-aym-es-en/bin train es-en 197297 examples\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:quy-es', 125008), ('main:aym-es', 6531), ('main:es-en', 197297)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:quy-es': 125008, 'main:aym-es': 6531, 'main:es-en': 197297}; raw total size: 328836\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:quy-es': 125008, 'main:aym-es': 6531, 'main:es-en': 197297}; resampled total size: 328836\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.073435\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004489\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.093776\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:43:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009316\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.017317\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.121413\n",
      "2022-09-20 08:43:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:43:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 045:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 08:43:00 | INFO | fairseq.trainer | begin training epoch 45\n",
      "2022-09-20 08:43:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 045:   0%|                               | 4/1353 [00:00<03:13,  6.99it/s]2022-09-20 08:43:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 045:   4%|#                             | 49/1353 [00:05<02:16,  9.55it/s]2022-09-20 08:43:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 045:  65%|6| 883/1353 [01:34<00:49,  9.57it/s, loss=3.46, nll_loss=1.714, 2022-09-20 08:44:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 045: 100%|9| 1352/1353 [02:25<00:00,  9.30it/s, loss=3.538, nll_loss=1.8022022-09-20 08:45:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 08:45:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:45:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001862\n",
      "2022-09-20 08:45:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:45:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061367\n",
      "2022-09-20 08:45:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:45:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047264\n",
      "2022-09-20 08:45:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111287\n",
      "2022-09-20 08:45:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 045 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:   3%|1      | 1/35 [00:00<00:03,  8.98it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  14%|#      | 5/35 [00:00<00:02, 11.51it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  26%|#8     | 9/35 [00:00<00:01, 14.28it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  37%|##2   | 13/35 [00:00<00:01, 17.26it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  49%|##9   | 17/35 [00:00<00:00, 20.33it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  60%|###6  | 21/35 [00:00<00:00, 23.07it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  71%|####2 | 25/35 [00:00<00:00, 25.58it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  83%|####9 | 29/35 [00:00<00:00, 27.30it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  94%|#####6| 33/35 [00:01<00:00, 28.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 08:45:27 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.273 | nll_loss 3.674 | ppl 12.76 | wps 88566.7 | wpb 2613.5 | bsz 189.6 | num_updates 1350\n",
      "2022-09-20 08:45:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 1350 updates\n",
      "2022-09-20 08:45:27 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint45.pt\n",
      "2022-09-20 08:45:32 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint45.pt\n",
      "2022-09-20 08:45:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint45.pt (epoch 45 @ 1350 updates, score 5.273) (writing took 9.009596182964742 seconds)\n",
      "2022-09-20 08:45:36 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
      "2022-09-20 08:45:36 | INFO | train | epoch 045 | loss 3.489 | nll_loss 1.746 | ppl 3.35 | wps 31790.3 | ups 8.65 | wpb 3677.2 | bsz 242.7 | num_updates 1350 | lr 0.00016875 | gnorm 1.22 | loss_scale 16 | train_wall 142 | gb_free 12.2 | wall 160\n",
      "2022-09-20 08:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.097820\n",
      "2022-09-20 08:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:45:36 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 08:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009789\n",
      "2022-09-20 08:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015639\n",
      "2022-09-20 08:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.124272\n",
      "2022-09-20 08:45:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:45:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 046:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 08:45:42 | INFO | fairseq.trainer | begin training epoch 46\n",
      "2022-09-20 08:45:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 046:  12%|1| 157/1353 [00:16<02:05,  9.57it/s, loss=3.602, nll_loss=1.868,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85d0a0c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 046:  69%|6| 933/1353 [01:40<00:46,  9.08it/s, loss=3.64, nll_loss=1.918, 2022-09-20 08:47:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 046: 100%|9| 1352/1353 [02:25<00:00,  9.19it/s, loss=3.977, nll_loss=2.2942022-09-20 08:48:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 08:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001945\n",
      "2022-09-20 08:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060269\n",
      "2022-09-20 08:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046797\n",
      "2022-09-20 08:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109802\n",
      "2022-09-20 08:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 046 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:   3%|1      | 1/35 [00:00<00:04,  7.89it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  14%|#      | 5/35 [00:00<00:02, 10.25it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  23%|#5     | 8/35 [00:00<00:02, 12.41it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:01, 14.97it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:01, 17.77it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 20.72it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 23.46it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 25.57it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  89%|#####3| 31/35 [00:01<00:00, 27.28it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset: 100%|######| 35/35 [00:01<00:00, 29.90it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 08:48:09 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.282 | nll_loss 3.685 | ppl 12.86 | wps 83214 | wpb 2613.5 | bsz 189.6 | num_updates 2702 | best_loss 5.273\n",
      "2022-09-20 08:48:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2702 updates\n",
      "2022-09-20 08:48:09 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint46.pt\n",
      "2022-09-20 08:48:17 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint46.pt\n",
      "2022-09-20 08:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint46.pt (epoch 46 @ 2702 updates, score 5.282) (writing took 10.062213576864451 seconds)\n",
      "2022-09-20 08:48:20 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
      "2022-09-20 08:48:20 | INFO | train | epoch 046 | loss 3.699 | nll_loss 1.982 | ppl 3.95 | wps 30311.2 | ups 8.25 | wpb 3675.3 | bsz 242.7 | num_updates 2702 | lr 0.00033775 | gnorm 1.388 | loss_scale 8 | train_wall 142 | gb_free 12.6 | wall 323\n",
      "2022-09-20 08:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089042\n",
      "2022-09-20 08:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:48:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 08:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009733\n",
      "2022-09-20 08:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016477\n",
      "2022-09-20 08:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116249\n",
      "2022-09-20 08:48:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:48:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 047:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 08:48:20 | INFO | fairseq.trainer | begin training epoch 47\n",
      "2022-09-20 08:48:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 047:   8%| | 102/1353 [00:11<02:10,  9.58it/s, loss=3.726, nll_loss=2.008,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85d0a0c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8568df5f8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 047: 100%|9| 1352/1353 [02:23<00:00,  9.58it/s, loss=4.128, nll_loss=2.4712022-09-20 08:50:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 08:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001972\n",
      "2022-09-20 08:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060730\n",
      "2022-09-20 08:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047825\n",
      "2022-09-20 08:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111159\n",
      "2022-09-20 08:50:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 047 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.87it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.96it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 27.78it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.26it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 30.83it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 31.80it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 32.26it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 32.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 08:50:46 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.319 | nll_loss 3.728 | ppl 13.25 | wps 89669.9 | wpb 2613.5 | bsz 189.6 | num_updates 4055 | best_loss 5.273\n",
      "2022-09-20 08:50:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4055 updates\n",
      "2022-09-20 08:50:46 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint47.pt\n",
      "2022-09-20 08:50:50 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint47.pt\n",
      "2022-09-20 08:50:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint47.pt (epoch 47 @ 4055 updates, score 5.319) (writing took 5.367280563805252 seconds)\n",
      "2022-09-20 08:50:51 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
      "2022-09-20 08:50:51 | INFO | train | epoch 047 | loss 3.977 | nll_loss 2.295 | ppl 4.91 | wps 32917.3 | ups 8.96 | wpb 3674.2 | bsz 243 | num_updates 4055 | lr 0.000496598 | gnorm 1.473 | loss_scale 8 | train_wall 140 | gb_free 12.1 | wall 474\n",
      "2022-09-20 08:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091223\n",
      "2022-09-20 08:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:50:51 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 08:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009319\n",
      "2022-09-20 08:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015988\n",
      "2022-09-20 08:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117564\n",
      "2022-09-20 08:50:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:50:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 048:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 08:50:52 | INFO | fairseq.trainer | begin training epoch 48\n",
      "2022-09-20 08:50:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 048:   2%|6                             | 30/1353 [00:03<02:28,  8.88it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85d0a0c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8568df5f8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876748>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8534ae0b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 048: 100%|9| 1352/1353 [02:24<00:00,  9.49it/s, loss=4.115, nll_loss=2.4612022-09-20 08:53:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 08:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002011\n",
      "2022-09-20 08:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060524\n",
      "2022-09-20 08:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046645\n",
      "2022-09-20 08:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109829\n",
      "2022-09-20 08:53:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 048 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 19.76it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 22.53it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 24.78it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 26.89it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 28.81it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 30.25it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 31.06it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 31.44it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 32.61it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 08:53:18 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.231 | nll_loss 3.637 | ppl 12.44 | wps 88351.2 | wpb 2613.5 | bsz 189.6 | num_updates 5408 | best_loss 5.231\n",
      "2022-09-20 08:53:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 5408 updates\n",
      "2022-09-20 08:53:18 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint48.pt\n",
      "2022-09-20 08:53:21 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint48.pt\n",
      "2022-09-20 08:53:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint48.pt (epoch 48 @ 5408 updates, score 5.231) (writing took 7.9065117130521685 seconds)\n",
      "2022-09-20 08:53:30 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
      "2022-09-20 08:53:30 | INFO | train | epoch 048 | loss 4.106 | nll_loss 2.443 | ppl 5.44 | wps 31277.3 | ups 8.51 | wpb 3674.2 | bsz 243 | num_updates 5408 | lr 0.000430013 | gnorm 1.446 | loss_scale 8 | train_wall 141 | gb_free 12.1 | wall 633\n",
      "2022-09-20 08:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087633\n",
      "2022-09-20 08:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:53:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 08:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009639\n",
      "2022-09-20 08:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015304\n",
      "2022-09-20 08:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113515\n",
      "2022-09-20 08:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:53:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 049:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 08:53:30 | INFO | fairseq.trainer | begin training epoch 49\n",
      "2022-09-20 08:53:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 049:   0%|                               | 1/1353 [00:00<03:03,  7.37it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85379ecf8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 049:   0%|                               | 2/1353 [00:00<02:50,  7.92it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85d0a0c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8568df5f8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876748>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8534ae0b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85368c0b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 049: 100%|9| 1352/1353 [02:24<00:00,  9.14it/s, loss=4.025, nll_loss=2.3582022-09-20 08:55:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 08:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001824\n",
      "2022-09-20 08:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060566\n",
      "2022-09-20 08:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046784\n",
      "2022-09-20 08:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110107\n",
      "2022-09-20 08:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 049 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.63it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.88it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 27.77it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.33it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 30.69it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 31.71it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 31.95it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 32.00it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset: 100%|######| 35/35 [00:01<00:00, 33.87it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 08:55:56 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.157 | nll_loss 3.556 | ppl 11.76 | wps 88277.5 | wpb 2613.5 | bsz 189.6 | num_updates 6761 | best_loss 5.157\n",
      "2022-09-20 08:55:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 6761 updates\n",
      "2022-09-20 08:55:56 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint49.pt\n",
      "2022-09-20 08:56:05 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint49.pt\n",
      "2022-09-20 08:56:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint49.pt (epoch 49 @ 6761 updates, score 5.157) (writing took 13.174064868828282 seconds)\n",
      "2022-09-20 08:56:10 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
      "2022-09-20 08:56:10 | INFO | train | epoch 049 | loss 4.02 | nll_loss 2.346 | ppl 5.08 | wps 31211 | ups 8.49 | wpb 3674.2 | bsz 243 | num_updates 6761 | lr 0.000384587 | gnorm 1.356 | loss_scale 8 | train_wall 141 | gb_free 12.3 | wall 793\n",
      "2022-09-20 08:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088282\n",
      "2022-09-20 08:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:56:10 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 08:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009272\n",
      "2022-09-20 08:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015001\n",
      "2022-09-20 08:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113535\n",
      "2022-09-20 08:56:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:56:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 050:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 08:56:10 | INFO | fairseq.trainer | begin training epoch 50\n",
      "2022-09-20 08:56:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 050:  22%|2| 297/1353 [00:32<01:52,  9.41it/s, loss=3.936, nll_loss=2.247,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 050: 100%|9| 1352/1353 [02:24<00:00,  9.19it/s, loss=4.131, nll_loss=2.4722022-09-20 08:58:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 08:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003764\n",
      "2022-09-20 08:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060983\n",
      "2022-09-20 08:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046489\n",
      "2022-09-20 08:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111866\n",
      "2022-09-20 08:58:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 050 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 16.76it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  14%|#      | 5/35 [00:00<00:01, 18.76it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  23%|#5     | 8/35 [00:00<00:01, 20.25it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:01, 22.37it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 26.83it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 28.90it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 30.25it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  89%|#####3| 31/35 [00:01<00:00, 31.32it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 08:58:36 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.151 | nll_loss 3.544 | ppl 11.67 | wps 83260.6 | wpb 2613.5 | bsz 189.6 | num_updates 8114 | best_loss 5.151\n",
      "2022-09-20 08:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 8114 updates\n",
      "2022-09-20 08:58:36 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint50.pt\n",
      "2022-09-20 08:58:39 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint50.pt\n",
      "2022-09-20 08:58:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint50.pt (epoch 50 @ 8114 updates, score 5.151) (writing took 8.09842960187234 seconds)\n",
      "2022-09-20 08:58:44 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
      "2022-09-20 08:58:44 | INFO | train | epoch 050 | loss 3.944 | nll_loss 2.261 | ppl 4.79 | wps 32171.8 | ups 8.76 | wpb 3674.2 | bsz 243 | num_updates 8114 | lr 0.000351061 | gnorm 1.301 | loss_scale 8 | train_wall 141 | gb_free 12.2 | wall 947\n",
      "2022-09-20 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087119\n",
      "2022-09-20 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:58:44 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009954\n",
      "2022-09-20 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016379\n",
      "2022-09-20 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114493\n",
      "2022-09-20 08:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 08:58:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 051:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 08:58:44 | INFO | fairseq.trainer | begin training epoch 51\n",
      "2022-09-20 08:58:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538357b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 051:  24%|2| 322/1353 [00:34<01:50,  9.29it/s, loss=3.758, nll_loss=2.05, Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538e2a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 051: 100%|9| 1352/1353 [02:24<00:00,  9.25it/s, loss=4.031, nll_loss=2.3622022-09-20 09:01:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001952\n",
      "2022-09-20 09:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061951\n",
      "2022-09-20 09:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047490\n",
      "2022-09-20 09:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112048\n",
      "2022-09-20 09:01:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 051 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:02, 16.41it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  14%|#      | 5/35 [00:00<00:01, 18.40it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  23%|#5     | 8/35 [00:00<00:01, 19.86it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:01, 21.94it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 24.14it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 26.41it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 28.05it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 29.27it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  89%|#####3| 31/35 [00:01<00:00, 30.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:01:11 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.129 | nll_loss 3.524 | ppl 11.5 | wps 81254.4 | wpb 2613.5 | bsz 189.6 | num_updates 9467 | best_loss 5.129\n",
      "2022-09-20 09:01:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9467 updates\n",
      "2022-09-20 09:01:11 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint51.pt\n",
      "2022-09-20 09:01:14 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint51.pt\n",
      "2022-09-20 09:01:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint51.pt (epoch 51 @ 9467 updates, score 5.129) (writing took 7.933173511875793 seconds)\n",
      "2022-09-20 09:01:19 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
      "2022-09-20 09:01:19 | INFO | train | epoch 051 | loss 3.874 | nll_loss 2.182 | ppl 4.54 | wps 32136.1 | ups 8.75 | wpb 3674.2 | bsz 243 | num_updates 9467 | lr 0.000325008 | gnorm 1.279 | loss_scale 8 | train_wall 142 | gb_free 12.3 | wall 1102\n",
      "2022-09-20 09:01:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:01:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088244\n",
      "2022-09-20 09:01:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:01:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:01:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009665\n",
      "2022-09-20 09:01:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:01:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.019327\n",
      "2022-09-20 09:01:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118271\n",
      "2022-09-20 09:01:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:01:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 052:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:01:19 | INFO | fairseq.trainer | begin training epoch 52\n",
      "2022-09-20 09:01:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8527fb518>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 052:  24%|2| 328/1353 [00:35<01:52,  9.08it/s, loss=3.873, nll_loss=2.174,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538e2a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539500b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 052: 100%|9| 1352/1353 [02:24<00:00,  9.43it/s, loss=3.853, nll_loss=2.1652022-09-20 09:03:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002015\n",
      "2022-09-20 09:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061901\n",
      "2022-09-20 09:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047132\n",
      "2022-09-20 09:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111733\n",
      "2022-09-20 09:03:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 052 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 24.41it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 26.54it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 28.29it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.78it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 31.22it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 32.27it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 32.71it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 33.01it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:03:45 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.108 | nll_loss 3.492 | ppl 11.25 | wps 90137.8 | wpb 2613.5 | bsz 189.6 | num_updates 10820 | best_loss 5.108\n",
      "2022-09-20 09:03:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10820 updates\n",
      "2022-09-20 09:03:45 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint52.pt\n",
      "2022-09-20 09:03:54 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint52.pt\n",
      "2022-09-20 09:03:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint52.pt (epoch 52 @ 10820 updates, score 5.108) (writing took 13.358289360068738 seconds)\n",
      "2022-09-20 09:03:58 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
      "2022-09-20 09:03:58 | INFO | train | epoch 052 | loss 3.813 | nll_loss 2.114 | ppl 4.33 | wps 31157.5 | ups 8.48 | wpb 3674.2 | bsz 243 | num_updates 10820 | lr 0.000304009 | gnorm 1.256 | loss_scale 8 | train_wall 141 | gb_free 12.1 | wall 1261\n",
      "2022-09-20 09:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091259\n",
      "2022-09-20 09:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:03:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010225\n",
      "2022-09-20 09:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016044\n",
      "2022-09-20 09:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118516\n",
      "2022-09-20 09:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:03:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 053:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:03:58 | INFO | fairseq.trainer | begin training epoch 53\n",
      "2022-09-20 09:03:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 053:  24%|2| 321/1353 [00:34<01:53,  9.08it/s, loss=3.778, nll_loss=2.067,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538e2a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539500b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8535640f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 053: 100%|9| 1352/1353 [02:24<00:00,  9.18it/s, loss=3.737, nll_loss=2.0342022-09-20 09:06:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002023\n",
      "2022-09-20 09:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068990\n",
      "2022-09-20 09:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061909\n",
      "2022-09-20 09:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.133690\n",
      "2022-09-20 09:06:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 053 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 22.87it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.06it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 26.80it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 28.27it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 29.45it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 30.42it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 30.84it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 31.19it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset: 100%|######| 35/35 [00:01<00:00, 32.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:06:24 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.121 | nll_loss 3.503 | ppl 11.34 | wps 85029.9 | wpb 2613.5 | bsz 189.6 | num_updates 12173 | best_loss 5.108\n",
      "2022-09-20 09:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 12173 updates\n",
      "2022-09-20 09:06:24 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint53.pt\n",
      "2022-09-20 09:06:28 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint53.pt\n",
      "2022-09-20 09:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint53.pt (epoch 53 @ 12173 updates, score 5.121) (writing took 5.3407080268952996 seconds)\n",
      "2022-09-20 09:06:30 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
      "2022-09-20 09:06:30 | INFO | train | epoch 053 | loss 3.762 | nll_loss 2.057 | ppl 4.16 | wps 32845.8 | ups 8.94 | wpb 3674.2 | bsz 243 | num_updates 12173 | lr 0.000286617 | gnorm 1.249 | loss_scale 8 | train_wall 141 | gb_free 12.1 | wall 1413\n",
      "2022-09-20 09:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088838\n",
      "2022-09-20 09:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:06:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009542\n",
      "2022-09-20 09:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016375\n",
      "2022-09-20 09:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.115796\n",
      "2022-09-20 09:06:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:06:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 054:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:06:30 | INFO | fairseq.trainer | begin training epoch 54\n",
      "2022-09-20 09:06:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 054:  18%|1| 237/1353 [00:25<02:01,  9.15it/s, loss=3.613, nll_loss=1.888,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538e2a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539500b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8535640f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e9c50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538767f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 054: 100%|9| 1352/1353 [02:24<00:00,  9.61it/s, loss=3.747, nll_loss=2.0462022-09-20 09:08:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004013\n",
      "2022-09-20 09:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061949\n",
      "2022-09-20 09:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048134\n",
      "2022-09-20 09:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115116\n",
      "2022-09-20 09:08:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 054 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 17.13it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 20.06it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 22.74it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 25.38it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 27.67it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 29.41it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 30.65it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 31.47it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 32.84it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:08:56 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.143 | nll_loss 3.537 | ppl 11.61 | wps 89833.3 | wpb 2613.5 | bsz 189.6 | num_updates 13526 | best_loss 5.108\n",
      "2022-09-20 09:08:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 13526 updates\n",
      "2022-09-20 09:08:56 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint54.pt\n",
      "2022-09-20 09:09:00 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint54.pt\n",
      "2022-09-20 09:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint54.pt (epoch 54 @ 13526 updates, score 5.143) (writing took 5.427224640036002 seconds)\n",
      "2022-09-20 09:09:01 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
      "2022-09-20 09:09:01 | INFO | train | epoch 054 | loss 3.72 | nll_loss 2.009 | ppl 4.02 | wps 32738.4 | ups 8.91 | wpb 3674.2 | bsz 243 | num_updates 13526 | lr 0.000271904 | gnorm 1.245 | loss_scale 8 | train_wall 141 | gb_free 12.4 | wall 1565\n",
      "2022-09-20 09:09:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:09:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086585\n",
      "2022-09-20 09:09:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:09:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:09:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008488\n",
      "2022-09-20 09:09:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:09:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015884\n",
      "2022-09-20 09:09:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.111944\n",
      "2022-09-20 09:09:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:09:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 055:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:09:02 | INFO | fairseq.trainer | begin training epoch 55\n",
      "2022-09-20 09:09:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 055:   0%|                               | 2/1353 [00:00<03:02,  7.40it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85357b9b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 055:  16%|1| 216/1353 [00:23<02:02,  9.26it/s, loss=3.449, nll_loss=1.708,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538e2a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539500b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8535640f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e9c50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538767f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9240>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 055:  34%|3| 463/1353 [00:49<01:36,  9.22it/s, loss=3.773, nll_loss=2.065,2022-09-20 09:09:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 055: 100%|9| 1352/1353 [02:24<00:00,  9.15it/s, loss=3.822, nll_loss=2.1232022-09-20 09:11:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001973\n",
      "2022-09-20 09:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060423\n",
      "2022-09-20 09:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047040\n",
      "2022-09-20 09:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110051\n",
      "2022-09-20 09:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 055 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.58it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.83it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 27.72it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.32it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 30.86it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 31.96it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 32.31it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 32.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:11:27 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.102 | nll_loss 3.491 | ppl 11.24 | wps 89744.5 | wpb 2613.5 | bsz 189.6 | num_updates 14878 | best_loss 5.102\n",
      "2022-09-20 09:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 14878 updates\n",
      "2022-09-20 09:11:27 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint55.pt\n",
      "2022-09-20 09:11:31 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint55.pt\n",
      "2022-09-20 09:11:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint55.pt (epoch 55 @ 14878 updates, score 5.102) (writing took 8.036653982941061 seconds)\n",
      "2022-09-20 09:11:36 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
      "2022-09-20 09:11:36 | INFO | train | epoch 055 | loss 3.679 | nll_loss 1.963 | ppl 3.9 | wps 32122.2 | ups 8.74 | wpb 3673.9 | bsz 243.1 | num_updates 14878 | lr 0.000259255 | gnorm 1.252 | loss_scale 4 | train_wall 141 | gb_free 12.1 | wall 1719\n",
      "2022-09-20 09:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090777\n",
      "2022-09-20 09:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:11:36 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010483\n",
      "2022-09-20 09:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.017061\n",
      "2022-09-20 09:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119379\n",
      "2022-09-20 09:11:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:11:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 056:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:11:36 | INFO | fairseq.trainer | begin training epoch 56\n",
      "2022-09-20 09:11:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85382a438>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 056:  14%|1| 191/1353 [00:20<02:09,  9.00it/s, loss=3.587, nll_loss=1.855,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538e2a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539500b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8535640f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e9c50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538767f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9240>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85348a2b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 056: 100%|9| 1352/1353 [02:24<00:00,  9.27it/s, loss=3.658, nll_loss=1.9432022-09-20 09:14:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001988\n",
      "2022-09-20 09:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068656\n",
      "2022-09-20 09:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.061699\n",
      "2022-09-20 09:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.132983\n",
      "2022-09-20 09:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 056 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 19.29it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 21.92it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 24.27it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 26.31it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 27.92it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 29.12it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 30.07it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 30.62it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 31.71it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:14:03 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 5.082 | nll_loss 3.472 | ppl 11.1 | wps 85449.7 | wpb 2613.5 | bsz 189.6 | num_updates 16231 | best_loss 5.082\n",
      "2022-09-20 09:14:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 16231 updates\n",
      "2022-09-20 09:14:03 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint56.pt\n",
      "2022-09-20 09:14:11 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint56.pt\n",
      "2022-09-20 09:14:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint56.pt (epoch 56 @ 16231 updates, score 5.082) (writing took 12.964202289935201 seconds)\n",
      "2022-09-20 09:14:18 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
      "2022-09-20 09:14:18 | INFO | train | epoch 056 | loss 3.643 | nll_loss 1.922 | ppl 3.79 | wps 30759.6 | ups 8.37 | wpb 3674.2 | bsz 243 | num_updates 16231 | lr 0.000248215 | gnorm 1.239 | loss_scale 4 | train_wall 141 | gb_free 12.2 | wall 1881\n",
      "2022-09-20 09:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091572\n",
      "2022-09-20 09:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:14:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010521\n",
      "2022-09-20 09:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.017500\n",
      "2022-09-20 09:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.120625\n",
      "2022-09-20 09:14:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:14:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 057:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:14:18 | INFO | fairseq.trainer | begin training epoch 57\n",
      "2022-09-20 09:14:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 057:  10%| | 132/1353 [00:14<02:11,  9.31it/s, loss=3.731, nll_loss=2.016,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538e2a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539500b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8535640f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e9c50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538767f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9240>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85348a2b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a6048>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 057: 100%|9| 1352/1353 [02:25<00:00,  9.04it/s, loss=3.678, nll_loss=1.9622022-09-20 09:16:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:16:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:16:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001951\n",
      "2022-09-20 09:16:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:16:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059946\n",
      "2022-09-20 09:16:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:16:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046505\n",
      "2022-09-20 09:16:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109191\n",
      "2022-09-20 09:16:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 057 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.23it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.21it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 27.19it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 28.90it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 30.47it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 31.59it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 31.92it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 32.33it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:16:44 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.089 | nll_loss 3.472 | ppl 11.1 | wps 88542.3 | wpb 2613.5 | bsz 189.6 | num_updates 17584 | best_loss 5.082\n",
      "2022-09-20 09:16:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 17584 updates\n",
      "2022-09-20 09:16:44 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint57.pt\n",
      "2022-09-20 09:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint57.pt\n",
      "2022-09-20 09:16:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint57.pt (epoch 57 @ 17584 updates, score 5.089) (writing took 5.275048540905118 seconds)\n",
      "2022-09-20 09:16:50 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
      "2022-09-20 09:16:50 | INFO | train | epoch 057 | loss 3.61 | nll_loss 1.885 | ppl 3.69 | wps 32714.2 | ups 8.9 | wpb 3674.2 | bsz 243 | num_updates 17584 | lr 0.000238474 | gnorm 1.251 | loss_scale 4 | train_wall 142 | gb_free 12.4 | wall 2033\n",
      "2022-09-20 09:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088036\n",
      "2022-09-20 09:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:16:50 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010488\n",
      "2022-09-20 09:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.017518\n",
      "2022-09-20 09:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117065\n",
      "2022-09-20 09:16:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:16:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 058:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:16:50 | INFO | fairseq.trainer | begin training epoch 58\n",
      "2022-09-20 09:16:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 058:   4%| | 60/1353 [00:06<02:13,  9.68it/s, loss=3.616, nll_loss=1.893, Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538e2a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539500b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8535640f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e9c50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538767f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9240>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85348a2b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a6048>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85382a7b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853829a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 058: 100%|9| 1352/1353 [02:24<00:00,  9.29it/s, loss=3.448, nll_loss=1.7092022-09-20 09:19:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001882\n",
      "2022-09-20 09:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060582\n",
      "2022-09-20 09:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046752\n",
      "2022-09-20 09:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109828\n",
      "2022-09-20 09:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 058 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 19.64it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 22.45it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 24.88it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 27.14it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 29.09it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 30.59it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 31.47it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 32.04it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 33.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:19:16 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 5.068 | nll_loss 3.45 | ppl 10.93 | wps 90045.8 | wpb 2613.5 | bsz 189.6 | num_updates 18937 | best_loss 5.068\n",
      "2022-09-20 09:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 18937 updates\n",
      "2022-09-20 09:19:16 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint58.pt\n",
      "2022-09-20 09:19:20 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint58.pt\n",
      "2022-09-20 09:19:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint58.pt (epoch 58 @ 18937 updates, score 5.068) (writing took 8.07207093713805 seconds)\n",
      "2022-09-20 09:19:26 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
      "2022-09-20 09:19:26 | INFO | train | epoch 058 | loss 3.581 | nll_loss 1.852 | ppl 3.61 | wps 31709.2 | ups 8.63 | wpb 3674.2 | bsz 243 | num_updates 18937 | lr 0.000229797 | gnorm 1.242 | loss_scale 4 | train_wall 141 | gb_free 12.2 | wall 2190\n",
      "2022-09-20 09:19:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085731\n",
      "2022-09-20 09:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:19:27 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009123\n",
      "2022-09-20 09:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015006\n",
      "2022-09-20 09:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.110847\n",
      "2022-09-20 09:19:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:19:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 059:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:19:27 | INFO | fairseq.trainer | begin training epoch 59\n",
      "2022-09-20 09:19:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 059:   0%|                               | 4/1353 [00:00<02:54,  7.71it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85348abe0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 059:   1%|3                             | 14/1353 [00:01<02:25,  9.21it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb852553358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538e2a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539500b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8535640f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e9c50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8538767f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9240>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85348a2b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a6048>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85382a7b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853829a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85362bc88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 059: 100%|9| 1352/1353 [02:24<00:00,  9.27it/s, loss=3.444, nll_loss=1.7042022-09-20 09:21:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001879\n",
      "2022-09-20 09:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061464\n",
      "2022-09-20 09:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046664\n",
      "2022-09-20 09:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110653\n",
      "2022-09-20 09:21:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 059 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.59it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.83it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 27.66it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.23it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 30.58it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 31.51it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 31.77it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 32.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:21:53 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 5.131 | nll_loss 3.517 | ppl 11.45 | wps 88449.2 | wpb 2613.5 | bsz 189.6 | num_updates 20290 | best_loss 5.068\n",
      "2022-09-20 09:21:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 20290 updates\n",
      "2022-09-20 09:21:53 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint59.pt\n",
      "2022-09-20 09:22:02 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint59.pt\n",
      "2022-09-20 09:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint59.pt (epoch 59 @ 20290 updates, score 5.131) (writing took 10.14172727917321 seconds)\n",
      "2022-09-20 09:22:08 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
      "2022-09-20 09:22:08 | INFO | train | epoch 059 | loss 3.552 | nll_loss 1.82 | ppl 3.53 | wps 30778.7 | ups 8.38 | wpb 3674.2 | bsz 243 | num_updates 20290 | lr 0.000222003 | gnorm 1.25 | loss_scale 4 | train_wall 141 | gb_free 12.1 | wall 2351\n",
      "2022-09-20 09:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087838\n",
      "2022-09-20 09:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:22:08 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009337\n",
      "2022-09-20 09:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015723\n",
      "2022-09-20 09:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113862\n",
      "2022-09-20 09:22:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:22:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 060:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:22:08 | INFO | fairseq.trainer | begin training epoch 60\n",
      "2022-09-20 09:22:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 060:  26%|2| 351/1353 [00:37<01:51,  9.01it/s, loss=3.565, nll_loss=1.831,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 060: 100%|9| 1352/1353 [02:25<00:00,  9.03it/s, loss=3.624, nll_loss=1.9022022-09-20 09:24:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002072\n",
      "2022-09-20 09:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067782\n",
      "2022-09-20 09:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.060245\n",
      "2022-09-20 09:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130738\n",
      "2022-09-20 09:24:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 060 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 19.38it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 22.11it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 24.51it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 26.58it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 28.28it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 29.55it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 30.32it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 30.90it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 32.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:24:35 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 5.072 | nll_loss 3.454 | ppl 10.96 | wps 86494.2 | wpb 2613.5 | bsz 189.6 | num_updates 21643 | best_loss 5.068\n",
      "2022-09-20 09:24:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 21643 updates\n",
      "2022-09-20 09:24:35 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint60.pt\n",
      "2022-09-20 09:24:39 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint60.pt\n",
      "2022-09-20 09:24:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint60.pt (epoch 60 @ 21643 updates, score 5.072) (writing took 5.367561092134565 seconds)\n",
      "2022-09-20 09:24:40 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
      "2022-09-20 09:24:40 | INFO | train | epoch 060 | loss 3.528 | nll_loss 1.791 | ppl 3.46 | wps 32612.2 | ups 8.88 | wpb 3674.2 | bsz 243 | num_updates 21643 | lr 0.000214952 | gnorm 1.261 | loss_scale 4 | train_wall 142 | gb_free 12.1 | wall 2504\n",
      "2022-09-20 09:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089363\n",
      "2022-09-20 09:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:24:40 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010442\n",
      "2022-09-20 09:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016824\n",
      "2022-09-20 09:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117669\n",
      "2022-09-20 09:24:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:24:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 061:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:24:41 | INFO | fairseq.trainer | begin training epoch 61\n",
      "2022-09-20 09:24:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 061:  19%|1| 257/1353 [00:27<01:57,  9.29it/s, loss=3.273, nll_loss=1.507,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 061: 100%|9| 1352/1353 [02:24<00:00,  9.53it/s, loss=3.611, nll_loss=1.8872022-09-20 09:27:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:27:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:27:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002073\n",
      "2022-09-20 09:27:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061723\n",
      "2022-09-20 09:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046037\n",
      "2022-09-20 09:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110476\n",
      "2022-09-20 09:27:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 061 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 061 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.76it/s]\u001b[A\n",
      "epoch 061 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 26.12it/s]\u001b[A\n",
      "epoch 061 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 28.03it/s]\u001b[A\n",
      "epoch 061 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.60it/s]\u001b[A\n",
      "epoch 061 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 30.94it/s]\u001b[A\n",
      "epoch 061 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 32.12it/s]\u001b[A\n",
      "epoch 061 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 31.68it/s]\u001b[A\n",
      "epoch 061 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 32.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:27:07 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 5.067 | nll_loss 3.456 | ppl 10.98 | wps 89011.6 | wpb 2613.5 | bsz 189.6 | num_updates 22996 | best_loss 5.067\n",
      "2022-09-20 09:27:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 22996 updates\n",
      "2022-09-20 09:27:07 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint61.pt\n",
      "2022-09-20 09:27:10 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint61.pt\n",
      "2022-09-20 09:27:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint61.pt (epoch 61 @ 22996 updates, score 5.067) (writing took 8.070406156824902 seconds)\n",
      "2022-09-20 09:27:15 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
      "2022-09-20 09:27:15 | INFO | train | epoch 061 | loss 3.505 | nll_loss 1.766 | ppl 3.4 | wps 32157.3 | ups 8.75 | wpb 3674.2 | bsz 243 | num_updates 22996 | lr 0.000208533 | gnorm 1.258 | loss_scale 4 | train_wall 141 | gb_free 12.2 | wall 2658\n",
      "2022-09-20 09:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090353\n",
      "2022-09-20 09:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:27:15 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010648\n",
      "2022-09-20 09:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016277\n",
      "2022-09-20 09:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118302\n",
      "2022-09-20 09:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:27:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 062:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:27:15 | INFO | fairseq.trainer | begin training epoch 62\n",
      "2022-09-20 09:27:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 062:   0%|                               | 3/1353 [00:00<02:46,  8.10it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853829cf8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 062:  20%|1| 264/1353 [00:28<01:57,  9.26it/s, loss=3.422, nll_loss=1.671,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb92a38ba90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 062: 100%|9| 1352/1353 [02:24<00:00,  9.28it/s, loss=3.47, nll_loss=1.731,2022-09-20 09:29:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003631\n",
      "2022-09-20 09:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061184\n",
      "2022-09-20 09:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048234\n",
      "2022-09-20 09:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.113708\n",
      "2022-09-20 09:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 062 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 18.75it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 21.60it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 24.07it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 26.47it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 28.56it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 30.04it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 30.99it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 31.53it/s]\u001b[A\n",
      "epoch 062 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 32.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:29:41 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 5.087 | nll_loss 3.471 | ppl 11.09 | wps 89028.7 | wpb 2613.5 | bsz 189.6 | num_updates 24349 | best_loss 5.067\n",
      "2022-09-20 09:29:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24349 updates\n",
      "2022-09-20 09:29:41 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint62.pt\n",
      "2022-09-20 09:29:45 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint62.pt\n",
      "2022-09-20 09:29:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint62.pt (epoch 62 @ 24349 updates, score 5.087) (writing took 4.982546048006043 seconds)\n",
      "2022-09-20 09:29:46 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
      "2022-09-20 09:29:46 | INFO | train | epoch 062 | loss 3.48 | nll_loss 1.738 | ppl 3.34 | wps 32868.3 | ups 8.95 | wpb 3674.2 | bsz 243 | num_updates 24349 | lr 0.000202656 | gnorm 1.262 | loss_scale 4 | train_wall 141 | gb_free 12.1 | wall 2809\n",
      "2022-09-20 09:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092188\n",
      "2022-09-20 09:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:29:46 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011515\n",
      "2022-09-20 09:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.017836\n",
      "2022-09-20 09:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.122612\n",
      "2022-09-20 09:29:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:29:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 063:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:29:51 | INFO | fairseq.trainer | begin training epoch 63\n",
      "2022-09-20 09:29:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 063:  22%|2| 298/1353 [00:31<01:55,  9.17it/s, loss=3.379, nll_loss=1.621,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb92a38ba90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853800fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 063: 100%|9| 1351/1353 [02:23<00:00,  9.08it/s, loss=3.659, nll_loss=1.9342022-09-20 09:32:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004018\n",
      "2022-09-20 09:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059272\n",
      "2022-09-20 09:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047050\n",
      "2022-09-20 09:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111016\n",
      "2022-09-20 09:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 063 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 18.50it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 21.26it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 23.80it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 26.15it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 27.46it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 29.26it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 30.48it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 31.06it/s]\u001b[A\n",
      "epoch 063 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 32.16it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:32:17 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 5.075 | nll_loss 3.463 | ppl 11.03 | wps 87205.1 | wpb 2613.5 | bsz 189.6 | num_updates 25702 | best_loss 5.067\n",
      "2022-09-20 09:32:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 25702 updates\n",
      "2022-09-20 09:32:17 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint63.pt\n",
      "2022-09-20 09:32:26 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint63.pt\n",
      "2022-09-20 09:32:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint63.pt (epoch 63 @ 25702 updates, score 5.075) (writing took 10.573345039971173 seconds)\n",
      "2022-09-20 09:32:30 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
      "2022-09-20 09:32:30 | INFO | train | epoch 063 | loss 3.459 | nll_loss 1.714 | ppl 3.28 | wps 30285.5 | ups 8.24 | wpb 3674.2 | bsz 243 | num_updates 25702 | lr 0.00019725 | gnorm 1.271 | loss_scale 4 | train_wall 141 | gb_free 12.3 | wall 2974\n",
      "2022-09-20 09:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.093320\n",
      "2022-09-20 09:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:32:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010308\n",
      "2022-09-20 09:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.014895\n",
      "2022-09-20 09:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.119567\n",
      "2022-09-20 09:32:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:32:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 064:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:32:31 | INFO | fairseq.trainer | begin training epoch 64\n",
      "2022-09-20 09:32:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 064:  18%|1| 243/1353 [00:25<02:03,  8.96it/s, loss=3.374, nll_loss=1.614,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb92a38ba90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853800fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8537aa6a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 064: 100%|9| 1352/1353 [02:24<00:00,  9.46it/s, loss=3.389, nll_loss=1.6382022-09-20 09:34:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004020\n",
      "2022-09-20 09:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061238\n",
      "2022-09-20 09:34:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046500\n",
      "2022-09-20 09:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112433\n",
      "2022-09-20 09:34:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 064 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 18.88it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 21.69it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 24.13it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 26.49it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 28.47it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 30.01it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 30.93it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 31.59it/s]\u001b[A\n",
      "epoch 064 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 32.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:34:57 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 5.102 | nll_loss 3.496 | ppl 11.28 | wps 88815.2 | wpb 2613.5 | bsz 189.6 | num_updates 27055 | best_loss 5.067\n",
      "2022-09-20 09:34:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 27055 updates\n",
      "2022-09-20 09:34:57 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint64.pt\n",
      "2022-09-20 09:35:00 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint64.pt\n",
      "2022-09-20 09:35:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint64.pt (epoch 64 @ 27055 updates, score 5.102) (writing took 5.399005410959944 seconds)\n",
      "2022-09-20 09:35:02 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
      "2022-09-20 09:35:02 | INFO | train | epoch 064 | loss 3.44 | nll_loss 1.692 | ppl 3.23 | wps 32748 | ups 8.91 | wpb 3674.2 | bsz 243 | num_updates 27055 | lr 0.000192254 | gnorm 1.274 | loss_scale 4 | train_wall 141 | gb_free 12.1 | wall 3125\n",
      "2022-09-20 09:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087872\n",
      "2022-09-20 09:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:35:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009991\n",
      "2022-09-20 09:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015337\n",
      "2022-09-20 09:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114201\n",
      "2022-09-20 09:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:35:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 065:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:35:02 | INFO | fairseq.trainer | begin training epoch 65\n",
      "2022-09-20 09:35:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 065:  12%|1| 156/1353 [00:16<02:08,  9.28it/s, loss=3.281, nll_loss=1.513,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb92a38ba90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853800fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8537aa6a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e97b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8533f0278>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 065: 100%|9| 1351/1353 [02:24<00:00,  9.57it/s, loss=3.497, nll_loss=1.7572022-09-20 09:37:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001992\n",
      "2022-09-20 09:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060728\n",
      "2022-09-20 09:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046729\n",
      "2022-09-20 09:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110236\n",
      "2022-09-20 09:37:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 065 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 065 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.78it/s]\u001b[A\n",
      "epoch 065 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.95it/s]\u001b[A\n",
      "epoch 065 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 27.55it/s]\u001b[A\n",
      "epoch 065 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.10it/s]\u001b[A\n",
      "epoch 065 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 30.56it/s]\u001b[A\n",
      "epoch 065 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 31.62it/s]\u001b[A\n",
      "epoch 065 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 32.16it/s]\u001b[A\n",
      "epoch 065 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 32.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:37:28 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 5.104 | nll_loss 3.497 | ppl 11.29 | wps 89040.7 | wpb 2613.5 | bsz 189.6 | num_updates 28408 | best_loss 5.067\n",
      "2022-09-20 09:37:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 28408 updates\n",
      "2022-09-20 09:37:28 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint65.pt\n",
      "2022-09-20 09:37:31 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint65.pt\n",
      "2022-09-20 09:37:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint65.pt (epoch 65 @ 28408 updates, score 5.104) (writing took 5.140991498017684 seconds)\n",
      "2022-09-20 09:37:33 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
      "2022-09-20 09:37:33 | INFO | train | epoch 065 | loss 3.422 | nll_loss 1.671 | ppl 3.19 | wps 32874.1 | ups 8.95 | wpb 3674.2 | bsz 243 | num_updates 28408 | lr 0.00018762 | gnorm 1.281 | loss_scale 4 | train_wall 141 | gb_free 12.1 | wall 3277\n",
      "2022-09-20 09:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088771\n",
      "2022-09-20 09:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:37:33 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010132\n",
      "2022-09-20 09:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016758\n",
      "2022-09-20 09:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.116667\n",
      "2022-09-20 09:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:37:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 066:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:37:34 | INFO | fairseq.trainer | begin training epoch 66\n",
      "2022-09-20 09:37:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 066:   0%|                               | 1/1353 [00:00<03:01,  7.46it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8527fbe48>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 066:  15%|1| 203/1353 [00:21<02:02,  9.38it/s, loss=3.338, nll_loss=1.576,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb92a38ba90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853800fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8537aa6a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e97b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8533f0278>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb850117b70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 066: 100%|9| 1352/1353 [02:24<00:00,  9.59it/s, loss=3.444, nll_loss=1.7, 2022-09-20 09:39:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002071\n",
      "2022-09-20 09:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060982\n",
      "2022-09-20 09:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046097\n",
      "2022-09-20 09:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109810\n",
      "2022-09-20 09:39:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 066 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 18.52it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 21.32it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 23.86it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 26.19it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 28.16it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 29.63it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 30.45it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 31.00it/s]\u001b[A\n",
      "epoch 066 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 32.13it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:40:00 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 5.114 | nll_loss 3.498 | ppl 11.3 | wps 87518.1 | wpb 2613.5 | bsz 189.6 | num_updates 29761 | best_loss 5.067\n",
      "2022-09-20 09:40:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 29761 updates\n",
      "2022-09-20 09:40:00 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint66.pt\n",
      "2022-09-20 09:40:03 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint66.pt\n",
      "2022-09-20 09:40:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint66.pt (epoch 66 @ 29761 updates, score 5.114) (writing took 5.003630795050412 seconds)\n",
      "2022-09-20 09:40:07 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
      "2022-09-20 09:40:07 | INFO | train | epoch 066 | loss 3.403 | nll_loss 1.65 | ppl 3.14 | wps 32330.4 | ups 8.8 | wpb 3674.2 | bsz 243 | num_updates 29761 | lr 0.000183306 | gnorm 1.289 | loss_scale 4 | train_wall 141 | gb_free 12.3 | wall 3430\n",
      "2022-09-20 09:40:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:40:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086564\n",
      "2022-09-20 09:40:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:40:07 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:40:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010075\n",
      "2022-09-20 09:40:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:40:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016115\n",
      "2022-09-20 09:40:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113790\n",
      "2022-09-20 09:40:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:40:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 067:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:40:12 | INFO | fairseq.trainer | begin training epoch 67\n",
      "2022-09-20 09:40:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 067:   9%| | 120/1353 [00:12<02:14,  9.14it/s, loss=3.461, nll_loss=1.712,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb92a38ba90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853800fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8537aa6a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e97b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8533f0278>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb850117b70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85357b898>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 067: 100%|9| 1351/1353 [02:24<00:00,  9.16it/s, loss=3.426, nll_loss=1.6792022-09-20 09:42:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002032\n",
      "2022-09-20 09:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060957\n",
      "2022-09-20 09:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046782\n",
      "2022-09-20 09:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110463\n",
      "2022-09-20 09:42:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 067 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 067 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.51it/s]\u001b[A\n",
      "epoch 067 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.76it/s]\u001b[A\n",
      "epoch 067 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 27.75it/s]\u001b[A\n",
      "epoch 067 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.43it/s]\u001b[A\n",
      "epoch 067 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 31.02it/s]\u001b[A\n",
      "epoch 067 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 32.21it/s]\u001b[A\n",
      "epoch 067 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 32.66it/s]\u001b[A\n",
      "epoch 067 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 33.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:42:38 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 5.096 | nll_loss 3.482 | ppl 11.17 | wps 90551.7 | wpb 2613.5 | bsz 189.6 | num_updates 31114 | best_loss 5.067\n",
      "2022-09-20 09:42:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 31114 updates\n",
      "2022-09-20 09:42:38 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint67.pt\n",
      "2022-09-20 09:42:47 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint67.pt\n",
      "2022-09-20 09:42:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint67.pt (epoch 67 @ 31114 updates, score 5.096) (writing took 10.18525174818933 seconds)\n",
      "2022-09-20 09:42:49 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
      "2022-09-20 09:42:49 | INFO | train | epoch 067 | loss 3.385 | nll_loss 1.63 | ppl 3.09 | wps 30780.4 | ups 8.38 | wpb 3674.2 | bsz 243 | num_updates 31114 | lr 0.000179276 | gnorm 1.297 | loss_scale 8 | train_wall 141 | gb_free 12.6 | wall 3592\n",
      "2022-09-20 09:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088269\n",
      "2022-09-20 09:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:42:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009793\n",
      "2022-09-20 09:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.015901\n",
      "2022-09-20 09:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.114942\n",
      "2022-09-20 09:42:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:42:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 068:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:42:49 | INFO | fairseq.trainer | begin training epoch 68\n",
      "2022-09-20 09:42:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 068:   3%|9                             | 43/1353 [00:04<02:18,  9.43it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb92a38ba90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853800fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8537aa6a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e97b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8533f0278>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb850117b70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85357b898>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853835b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 068: 100%|9| 1351/1353 [02:25<00:00,  9.59it/s, loss=3.442, nll_loss=1.6952022-09-20 09:45:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002033\n",
      "2022-09-20 09:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060771\n",
      "2022-09-20 09:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046169\n",
      "2022-09-20 09:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109595\n",
      "2022-09-20 09:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 068 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 19.06it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:  17%|#2     | 6/35 [00:00<00:01, 21.87it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:  29%|#7    | 10/35 [00:00<00:01, 24.37it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:  40%|##4   | 14/35 [00:00<00:00, 26.77it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:  51%|###   | 18/35 [00:00<00:00, 28.62it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 30.15it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 31.04it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:  86%|#####1| 30/35 [00:00<00:00, 31.65it/s]\u001b[A\n",
      "epoch 068 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 32.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:45:15 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 5.114 | nll_loss 3.502 | ppl 11.33 | wps 89156.2 | wpb 2613.5 | bsz 189.6 | num_updates 32467 | best_loss 5.067\n",
      "2022-09-20 09:45:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 32467 updates\n",
      "2022-09-20 09:45:15 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint68.pt\n",
      "2022-09-20 09:45:19 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint68.pt\n",
      "2022-09-20 09:45:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint68.pt (epoch 68 @ 32467 updates, score 5.114) (writing took 5.0786884180270135 seconds)\n",
      "2022-09-20 09:45:25 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
      "2022-09-20 09:45:25 | INFO | train | epoch 068 | loss 3.37 | nll_loss 1.613 | ppl 3.06 | wps 31749.7 | ups 8.64 | wpb 3674.2 | bsz 243 | num_updates 32467 | lr 0.000175501 | gnorm 1.303 | loss_scale 8 | train_wall 142 | gb_free 12.2 | wall 3748\n",
      "2022-09-20 09:45:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:45:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086433\n",
      "2022-09-20 09:45:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:45:25 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:45:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009258\n",
      "2022-09-20 09:45:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:45:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016381\n",
      "2022-09-20 09:45:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.113060\n",
      "2022-09-20 09:45:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:45:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 069:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:45:25 | INFO | fairseq.trainer | begin training epoch 69\n",
      "2022-09-20 09:45:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 069:   1%|2                              | 9/1353 [00:01<02:28,  9.05it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8539b4c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853876a20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb92a38ba90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853800fd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8537aa6a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536e97b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8533f0278>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb850117b70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85357b898>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853835b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8501176d8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb85357bb38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 069: 100%|9| 1352/1353 [02:24<00:00,  9.46it/s, loss=3.348, nll_loss=1.5932022-09-20 09:47:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001899\n",
      "2022-09-20 09:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060829\n",
      "2022-09-20 09:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046058\n",
      "2022-09-20 09:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109479\n",
      "2022-09-20 09:47:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 069 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 069 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.94it/s]\u001b[A\n",
      "epoch 069 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.80it/s]\u001b[A\n",
      "epoch 069 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 27.67it/s]\u001b[A\n",
      "epoch 069 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.19it/s]\u001b[A\n",
      "epoch 069 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 30.73it/s]\u001b[A\n",
      "epoch 069 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 31.75it/s]\u001b[A\n",
      "epoch 069 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 32.03it/s]\u001b[A\n",
      "epoch 069 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 32.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:47:51 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 5.114 | nll_loss 3.509 | ppl 11.39 | wps 88125.4 | wpb 2613.5 | bsz 189.6 | num_updates 33820 | best_loss 5.067\n",
      "2022-09-20 09:47:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 33820 updates\n",
      "2022-09-20 09:47:51 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint69.pt\n",
      "2022-09-20 09:48:00 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint69.pt\n",
      "2022-09-20 09:48:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint69.pt (epoch 69 @ 33820 updates, score 5.114) (writing took 10.077862915117294 seconds)\n",
      "2022-09-20 09:48:06 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
      "2022-09-20 09:48:06 | INFO | train | epoch 069 | loss 3.354 | nll_loss 1.594 | ppl 3.02 | wps 30868.9 | ups 8.4 | wpb 3674.2 | bsz 243 | num_updates 33820 | lr 0.000171954 | gnorm 1.305 | loss_scale 8 | train_wall 141 | gb_free 12.1 | wall 3909\n",
      "2022-09-20 09:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086029\n",
      "2022-09-20 09:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:48:06 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013271\n",
      "2022-09-20 09:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.018106\n",
      "2022-09-20 09:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.118443\n",
      "2022-09-20 09:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:48:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 070:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:48:06 | INFO | fairseq.trainer | begin training epoch 70\n",
      "2022-09-20 09:48:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 070:  29%|2| 386/1353 [00:41<01:42,  9.40it/s, loss=3.306, nll_loss=1.537,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853829b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 070: 100%|9| 1351/1353 [02:24<00:00,  9.52it/s, loss=3.307, nll_loss=1.5462022-09-20 09:50:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002202\n",
      "2022-09-20 09:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061947\n",
      "2022-09-20 09:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047883\n",
      "2022-09-20 09:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.113480\n",
      "2022-09-20 09:50:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 070 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:   6%|3      | 2/35 [00:00<00:01, 16.56it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  14%|#      | 5/35 [00:00<00:01, 18.59it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  23%|#5     | 8/35 [00:00<00:01, 20.08it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:01, 22.27it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 26.75it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  63%|###7  | 22/35 [00:00<00:00, 27.57it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  74%|####4 | 26/35 [00:00<00:00, 29.32it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  86%|#####1| 30/35 [00:01<00:00, 30.47it/s]\u001b[A\n",
      "epoch 070 | valid on 'valid' subset:  97%|#####8| 34/35 [00:01<00:00, 31.97it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:50:33 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 5.075 | nll_loss 3.459 | ppl 11 | wps 81402.8 | wpb 2613.5 | bsz 189.6 | num_updates 35173 | best_loss 5.067\n",
      "2022-09-20 09:50:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 35173 updates\n",
      "2022-09-20 09:50:33 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint70.pt\n",
      "2022-09-20 09:50:36 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint70.pt\n",
      "2022-09-20 09:50:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint70.pt (epoch 70 @ 35173 updates, score 5.075) (writing took 5.268488327972591 seconds)\n",
      "2022-09-20 09:50:43 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
      "2022-09-20 09:50:43 | INFO | train | epoch 070 | loss 3.34 | nll_loss 1.579 | ppl 2.99 | wps 31765.3 | ups 8.65 | wpb 3674.2 | bsz 243 | num_updates 35173 | lr 0.000168615 | gnorm 1.311 | loss_scale 8 | train_wall 141 | gb_free 12.1 | wall 4066\n",
      "2022-09-20 09:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091158\n",
      "2022-09-20 09:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:50:43 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[34006]\n",
      "2022-09-20 09:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009298\n",
      "2022-09-20 09:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.016054\n",
      "2022-09-20 09:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.117510\n",
      "2022-09-20 09:50:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:50:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1353\n",
      "epoch 071:   0%|                                       | 0/1353 [00:00<?, ?it/s]2022-09-20 09:50:43 | INFO | fairseq.trainer | begin training epoch 71\n",
      "2022-09-20 09:50:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 071:  29%|2| 395/1353 [00:42<01:47,  8.90it/s, loss=3.238, nll_loss=1.465,Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb853829b38>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb850117278>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fb8536a9b00>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "epoch 071: 100%|9| 1352/1353 [02:24<00:00,  9.65it/s, loss=3.478, nll_loss=1.7352022-09-20 09:53:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2022-09-20 09:53:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2022-09-20 09:53:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003956\n",
      "2022-09-20 09:53:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060863\n",
      "2022-09-20 09:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2022-09-20 09:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047106\n",
      "2022-09-20 09:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112717\n",
      "2022-09-20 09:53:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 071 | valid on 'valid' subset:   0%|               | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 071 | valid on 'valid' subset:   9%|6      | 3/35 [00:00<00:01, 23.05it/s]\u001b[A\n",
      "epoch 071 | valid on 'valid' subset:  20%|#4     | 7/35 [00:00<00:01, 25.41it/s]\u001b[A\n",
      "epoch 071 | valid on 'valid' subset:  31%|#8    | 11/35 [00:00<00:00, 27.41it/s]\u001b[A\n",
      "epoch 071 | valid on 'valid' subset:  43%|##5   | 15/35 [00:00<00:00, 29.06it/s]\u001b[A\n",
      "epoch 071 | valid on 'valid' subset:  54%|###2  | 19/35 [00:00<00:00, 30.69it/s]\u001b[A\n",
      "epoch 071 | valid on 'valid' subset:  66%|###9  | 23/35 [00:00<00:00, 31.79it/s]\u001b[A\n",
      "epoch 071 | valid on 'valid' subset:  77%|####6 | 27/35 [00:00<00:00, 32.21it/s]\u001b[A\n",
      "epoch 071 | valid on 'valid' subset:  89%|#####3| 31/35 [00:00<00:00, 32.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2022-09-20 09:53:09 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 5.115 | nll_loss 3.508 | ppl 11.37 | wps 89554.6 | wpb 2613.5 | bsz 189.6 | num_updates 36526 | best_loss 5.067\n",
      "2022-09-20 09:53:09 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 10 runs\n",
      "2022-09-20 09:53:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 36526 updates\n",
      "2022-09-20 09:53:09 | INFO | fairseq.trainer | Saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint71.pt\n",
      "2022-09-20 09:53:12 | INFO | fairseq.trainer | Finished saving checkpoint to /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint71.pt\n",
      "2022-09-20 09:53:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /storage/master-thesis/models/quy-es_aym-es_es-en/checkpoint71.pt (epoch 71 @ 36526 updates, score 5.115) (writing took 5.089884181972593 seconds)\n",
      "2022-09-20 09:53:18 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
      "2022-09-20 09:53:18 | INFO | train | epoch 071 | loss 3.325 | nll_loss 1.562 | ppl 2.95 | wps 31929 | ups 8.69 | wpb 3674.2 | bsz 243 | num_updates 36526 | lr 0.000165462 | gnorm 1.321 | loss_scale 8 | train_wall 141 | gb_free 12.3 | wall 4222\n",
      "2022-09-20 09:53:18 | INFO | fairseq_cli.train | done training in 4218.4 seconds\n"
     ]
    }
   ],
   "source": [
    "! fairseq-train $BIN_DIR \\\n",
    "    --arch=transformer --share-all-embeddings \\\n",
    "    --task translation_multi_simple_epoch --lang-pairs quy-es,aym-es,es-en \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --restore-file $MODEL_DIR/checkpoint_last.pt \\\n",
    "    --save-dir $MODEL_DIR/ \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --reset-optimizer \\\n",
    "    --encoder-langtok \"src\" \\\n",
    "    --decoder-langtok \\\n",
    "    --fp16 \\\n",
    "    --max-epoch 200 \\\n",
    "    --patience 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "! echo $MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "cd /storage/master-thesis/models/aym-es+es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm dict.en.txt\n",
    "! rm dict.es.txt\n",
    "! rm dict.aym.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5K\t/notebooks/CITATION.cff\n",
      "5.5K\t/notebooks/CODE_OF_CONDUCT.md\n",
      "15K\t/notebooks/CONTRIBUTING.md\n",
      "19K\t/notebooks/ISSUES.md\n",
      "12K\t/notebooks/LICENSE\n",
      "512\t/notebooks/MANIFEST.in\n",
      "3.5K\t/notebooks/Makefile\n",
      "41K\t/notebooks/README.md\n",
      "41K\t/notebooks/README_zh-hans.md\n",
      "42K\t/notebooks/README_zh-hant.md\n",
      "12K\t/notebooks/docker\n",
      "4.6M\t/notebooks/docs\n",
      "5.0M\t/notebooks/examples\n",
      "8.5K\t/notebooks/hubconf.py\n",
      "7.8G\t/notebooks/master-thesis\n",
      "1.5K\t/notebooks/model_cards\n",
      "9.5K\t/notebooks/notebooks\n",
      "512\t/notebooks/pyproject.toml\n",
      "64K\t/notebooks/scripts\n",
      "1.0K\t/notebooks/setup.cfg\n",
      "13K\t/notebooks/setup.py\n",
      "13M\t/notebooks/src\n",
      "731K\t/notebooks/templates\n",
      "4.5K\t/notebooks/test quy-es -> es-en model.ipynb\n",
      "6.8M\t/notebooks/tests\n",
      "147K\t/notebooks/train es-en model.ipynb\n",
      "158K\t/notebooks/train quy-es + es-en model.ipynb\n",
      "160K\t/notebooks/utils\n",
      "3.5K\t/notebooks/valohai.yaml\n",
      "7.9G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\t/notebooks/master-thesis/Untitled.ipynb\n",
      "362M\t/notebooks/master-thesis/corpora\n",
      "7.5G\t/notebooks/master-thesis/models\n",
      "7.8G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6G\t/notebooks/master-thesis/models/es-en\n",
      "802M\t/notebooks/master-thesis/models/quy-es\n",
      "5.2G\t/notebooks/master-thesis/models/quy-es+es-en\n",
      "7.5G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19M\t/apex\n",
      "5.0M\t/bin\n",
      "4.0K\t/boot\n",
      "24K\t/content\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! du -shc /*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CITATION.cff         \u001b[0m\u001b[01;34mdocker\u001b[0m/          setup.py\n",
      " CODE_OF_CONDUCT.md   \u001b[01;34mdocs\u001b[0m/            \u001b[01;34msrc\u001b[0m/\n",
      " CONTRIBUTING.md      \u001b[01;34mexamples\u001b[0m/        \u001b[01;34mtemplates\u001b[0m/\n",
      " ISSUES.md            hubconf.py      'test quy-es -> es-en model.ipynb'\n",
      " LICENSE              \u001b[01;34mmaster-thesis\u001b[0m/   \u001b[01;34mtests\u001b[0m/\n",
      " MANIFEST.in          \u001b[01;34mmodel_cards\u001b[0m/    'train es-en model.ipynb'\n",
      " Makefile             \u001b[01;34mnotebooks\u001b[0m/      'train quy-es + es-en model.ipynb'\n",
      " README.md            pyproject.toml   \u001b[01;34mutils\u001b[0m/\n",
      " README_zh-hans.md    \u001b[01;34mscripts\u001b[0m/         valohai.yaml\n",
      " README_zh-hant.md    setup.cfg\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
