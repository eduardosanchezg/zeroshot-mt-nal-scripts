{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_IT_CORPUS_DIR = \"/notebooks/master-thesis/corpora/iwslt17/en-it/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_NL_CORPUS_DIR = \"/notebooks/master-thesis/corpora/iwslt17/en-nl/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_RO_CORPUS_DIR = \"/notebooks/master-thesis/corpora/iwslt17/en-ro/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_NL_CORPUS_DIR = \"/notebooks/master-thesis/corpora/iwslt17/it-nl/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_RO_CORPUS_DIR = \"/notebooks/master-thesis/corpora/iwslt17/it-ro/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NL_RO_CORPUS_DIR = \"/notebooks/master-thesis/corpora/iwslt17/nl-ro/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZER_PATH=/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZER_PATH = /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_path_en_it = \"/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/\"\n",
    "tokenized_path_en_nl = \"/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/\"\n",
    "tokenized_path_en_ro = \"/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/\"\n",
    "tokenized_path_it_nl = \"/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/it-nl/\"\n",
    "tokenized_path_it_ro = \"/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/it-ro/\"\n",
    "tokenized_path_nl_ro = \"/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/nl-ro/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZED_PATH_EN_IT=/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it\n",
      "env: TOKENIZED_PATH_EN_NL=/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl\n",
      "env: TOKENIZED_PATH_EN_RO=/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro\n",
      "env: TOKENIZED_PATH_IT_NL=/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/it-nl\n",
      "env: TOKENIZED_PATH_IT_RO=/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/it-ro\n",
      "env: TOKENIZED_PATH_NL_RO=/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/nl-ro\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZED_PATH_EN_IT = /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it\n",
    "%env TOKENIZED_PATH_EN_NL = /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl\n",
    "%env TOKENIZED_PATH_EN_RO = /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro\n",
    "%env TOKENIZED_PATH_IT_NL = /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/it-nl\n",
    "%env TOKENIZED_PATH_IT_RO = /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/it-ro\n",
    "%env TOKENIZED_PATH_NL_RO = /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/nl-ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BIN_DIR=/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin\n"
     ]
    }
   ],
   "source": [
    "%env BIN_DIR = /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_DIR=/storage/master-thesis/models/it-nl-ro-en\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_DIR = /storage/master-thesis/models/it-nl-ro-en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "it_nl_ro_en_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "it_nl_ro_en_trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=30000, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "it_nl_ro_en_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "en_it_files = [EN_IT_CORPUS_DIR + f\"{split}.en\" for split in [\"valid\",\"train\"]]\n",
    "it_en_files = [EN_IT_CORPUS_DIR + f\"{split}.it\" for split in [\"valid\",\"train\"]]\n",
    "\n",
    "en_nl_files = [EN_NL_CORPUS_DIR + f\"{split}.en\" for split in [\"valid\",\"train\"]]\n",
    "nl_en_files = [EN_NL_CORPUS_DIR + f\"{split}.nl\" for split in [\"valid\",\"train\"]]\n",
    "\n",
    "en_ro_files = [EN_RO_CORPUS_DIR + f\"{split}.en\" for split in [\"valid\",\"train\"]]\n",
    "ro_en_files = [EN_RO_CORPUS_DIR + f\"{split}.ro\" for split in [\"valid\",\"train\"]]\n",
    "\n",
    "\n",
    "files = en_it_files + it_en_files + en_nl_files + nl_en_files + en_ro_files + ro_en_files\n",
    "it_nl_ro_en_tokenizer.train(files= files, trainer=it_nl_ro_en_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_files(tokenizer, files, extension, output_path):\n",
    "    for file in files:\n",
    "        print(f\"Reading file {file}\")\n",
    "        with open(file, encoding='utf8') as f:\n",
    "          lines = f.readlines()\n",
    "          tokenized_lines = tokenizer.encode_batch(lines)\n",
    "          tokenized_name = Path(file).stem\n",
    "          tokenized_name = output_path + tokenized_name + \".\" + extension\n",
    "          print(tokenized_name)\n",
    "          with open(tokenized_name, 'w', encoding='utf8') as wf:\n",
    "\n",
    "            wf.writelines([\" \".join(t.tokens) + \"\\n\" for t in tokenized_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_nl_ro_en_tokenizer.save(tokenizer_path + \"it_nl_ro_en_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'hold', 'this', 'truth', 'to', 'be', 'self', 'evident', '##ly', 'that', 'everyone', 'is', 'created', 'equally', '?']\n"
     ]
    }
   ],
   "source": [
    "tok = it_nl_ro_en_tokenizer.encode(\"we hold this truth to be self evidently that everyone is created equal?\")\n",
    "print(tok.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-it/valid.en\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/valid.en\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-it/train.en\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/train.en\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-it/valid.it\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/valid.it\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-it/train.it\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/train.it\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-nl/valid.en\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/valid.en\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-nl/train.en\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/train.en\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-nl/valid.nl\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/valid.nl\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-nl/train.nl\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/train.nl\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-ro/valid.en\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/valid.en\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-ro/train.en\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/train.en\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-ro/valid.ro\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/valid.ro\n",
      "Reading file /notebooks/master-thesis/corpora/iwslt17/en-ro/train.ro\n",
      "/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/train.ro\n"
     ]
    }
   ],
   "source": [
    "tokenize_files(it_nl_ro_en_tokenizer, en_it_files, \"en\", tokenized_path_en_it)\n",
    "tokenize_files(it_nl_ro_en_tokenizer, it_en_files, \"it\", tokenized_path_en_it)\n",
    "\n",
    "tokenize_files(it_nl_ro_en_tokenizer, en_nl_files, \"en\", tokenized_path_en_nl)\n",
    "tokenize_files(it_nl_ro_en_tokenizer, nl_en_files, \"nl\", tokenized_path_en_nl)\n",
    "\n",
    "tokenize_files(it_nl_ro_en_tokenizer, en_ro_files, \"en\", tokenized_path_en_ro)\n",
    "tokenize_files(it_nl_ro_en_tokenizer, ro_en_files, \"ro\", tokenized_path_en_ro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenating all training data\n",
    "! cat $TOKENIZED_PATH_EN_IT/train.en $TOKENIZED_PATH_EN_IT/train.it \\\n",
    "    $TOKENIZED_PATH_EN_NL/train.en $TOKENIZED_PATH_EN_NL/train.nl \\\n",
    "    $TOKENIZED_PATH_EN_RO/train.en $TOKENIZED_PATH_EN_RO/train.ro \\\n",
    "    > $BIN_DIR/train.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-11 15:45:51 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='all', srcdict=None, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train', user_dir=None, validpref=None, workers=20)\n",
      "2021-11-11 15:46:01 | INFO | fairseq_cli.preprocess | [all] Dictionary: 29264 types\n",
      "2021-11-11 15:46:21 | INFO | fairseq_cli.preprocess | [all] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.all: 870630 sents, 19874472 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:46:21 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang all \\\n",
    "    --trainpref $BIN_DIR/train \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --workers 20 \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --only-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-11 15:49:54 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='en', srcdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', target_lang='it', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/train', user_dir=None, validpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/valid', workers=20)\n",
      "2021-11-11 15:49:55 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:49:58 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/train.en: 145105 sents, 3331065 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:49:58 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:49:59 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/valid.en: 929 sents, 24214 tokens, 0.0661% replaced by <unk>\n",
      "2021-11-11 15:49:59 | INFO | fairseq_cli.preprocess | [it] Dictionary: 29264 types\n",
      "2021-11-11 15:50:03 | INFO | fairseq_cli.preprocess | [it] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/train.it: 145105 sents, 3366219 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:03 | INFO | fairseq_cli.preprocess | [it] Dictionary: 29264 types\n",
      "2021-11-11 15:50:03 | INFO | fairseq_cli.preprocess | [it] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/valid.it: 929 sents, 23621 tokens, 0.0593% replaced by <unk>\n",
      "2021-11-11 15:50:03 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang en --target-lang it \\\n",
    "    --trainpref $TOKENIZED_PATH_EN_IT/train --validpref $TOKENIZED_PATH_EN_IT/valid \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-11 15:50:11 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='it', srcdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', target_lang='en', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/train', user_dir=None, validpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/valid', workers=20)\n",
      "2021-11-11 15:50:11 | INFO | fairseq_cli.preprocess | [it] Dictionary: 29264 types\n",
      "2021-11-11 15:50:15 | INFO | fairseq_cli.preprocess | [it] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/train.it: 145105 sents, 3366219 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:15 | INFO | fairseq_cli.preprocess | [it] Dictionary: 29264 types\n",
      "2021-11-11 15:50:16 | INFO | fairseq_cli.preprocess | [it] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/valid.it: 929 sents, 23621 tokens, 0.0593% replaced by <unk>\n",
      "2021-11-11 15:50:16 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:50:19 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/train.en: 145105 sents, 3331065 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:19 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:50:20 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-it/valid.en: 929 sents, 24214 tokens, 0.0661% replaced by <unk>\n",
      "2021-11-11 15:50:20 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang it --target-lang en \\\n",
    "    --trainpref $TOKENIZED_PATH_EN_IT/train --validpref $TOKENIZED_PATH_EN_IT/valid \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-11 15:50:22 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='en', srcdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', target_lang='nl', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/train', user_dir=None, validpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/valid', workers=20)\n",
      "2021-11-11 15:50:22 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:50:26 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/train.en: 145105 sents, 3331065 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:26 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:50:26 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/valid.en: 1003 sents, 24290 tokens, 0.0659% replaced by <unk>\n",
      "2021-11-11 15:50:26 | INFO | fairseq_cli.preprocess | [nl] Dictionary: 29264 types\n",
      "2021-11-11 15:50:30 | INFO | fairseq_cli.preprocess | [nl] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/train.nl: 145105 sents, 3184795 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:30 | INFO | fairseq_cli.preprocess | [nl] Dictionary: 29264 types\n",
      "2021-11-11 15:50:30 | INFO | fairseq_cli.preprocess | [nl] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/valid.nl: 1003 sents, 22716 tokens, 0.0616% replaced by <unk>\n",
      "2021-11-11 15:50:30 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang en --target-lang nl \\\n",
    "    --trainpref $TOKENIZED_PATH_EN_NL/train --validpref $TOKENIZED_PATH_EN_NL/valid \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-11 15:50:32 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='nl', srcdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', target_lang='en', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/train', user_dir=None, validpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/valid', workers=20)\n",
      "2021-11-11 15:50:32 | INFO | fairseq_cli.preprocess | [nl] Dictionary: 29264 types\n",
      "2021-11-11 15:50:36 | INFO | fairseq_cli.preprocess | [nl] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/train.nl: 145105 sents, 3184795 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:36 | INFO | fairseq_cli.preprocess | [nl] Dictionary: 29264 types\n",
      "2021-11-11 15:50:37 | INFO | fairseq_cli.preprocess | [nl] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/valid.nl: 1003 sents, 22716 tokens, 0.0616% replaced by <unk>\n",
      "2021-11-11 15:50:37 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:50:40 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/train.en: 145105 sents, 3331065 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:40 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:50:41 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-nl/valid.en: 1003 sents, 24290 tokens, 0.0659% replaced by <unk>\n",
      "2021-11-11 15:50:41 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang nl --target-lang en \\\n",
    "    --trainpref $TOKENIZED_PATH_EN_NL/train --validpref $TOKENIZED_PATH_EN_NL/valid \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-11 15:50:42 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='en', srcdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', target_lang='ro', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/train', user_dir=None, validpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/valid', workers=20)\n",
      "2021-11-11 15:50:43 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:50:46 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/train.en: 145105 sents, 3331065 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:46 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:50:47 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/valid.en: 914 sents, 24213 tokens, 0.0661% replaced by <unk>\n",
      "2021-11-11 15:50:47 | INFO | fairseq_cli.preprocess | [ro] Dictionary: 29264 types\n",
      "2021-11-11 15:50:51 | INFO | fairseq_cli.preprocess | [ro] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/train.ro: 145105 sents, 3330263 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:51 | INFO | fairseq_cli.preprocess | [ro] Dictionary: 29264 types\n",
      "2021-11-11 15:50:51 | INFO | fairseq_cli.preprocess | [ro] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/valid.ro: 914 sents, 24901 tokens, 0.0562% replaced by <unk>\n",
      "2021-11-11 15:50:51 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang en --target-lang ro \\\n",
    "    --trainpref $TOKENIZED_PATH_EN_RO/train --validpref $TOKENIZED_PATH_EN_RO/valid \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-11 15:50:53 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='ro', srcdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', target_lang='en', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/train', user_dir=None, validpref='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/valid', workers=20)\n",
      "2021-11-11 15:50:54 | INFO | fairseq_cli.preprocess | [ro] Dictionary: 29264 types\n",
      "2021-11-11 15:50:58 | INFO | fairseq_cli.preprocess | [ro] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/train.ro: 145105 sents, 3330263 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:50:58 | INFO | fairseq_cli.preprocess | [ro] Dictionary: 29264 types\n",
      "2021-11-11 15:50:59 | INFO | fairseq_cli.preprocess | [ro] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/valid.ro: 914 sents, 24901 tokens, 0.0562% replaced by <unk>\n",
      "2021-11-11 15:50:59 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:51:02 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/train.en: 145105 sents, 3331065 tokens, 0.0% replaced by <unk>\n",
      "2021-11-11 15:51:02 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29264 types\n",
      "2021-11-11 15:51:03 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/en-ro/valid.en: 914 sents, 24213 tokens, 0.0661% replaced by <unk>\n",
      "2021-11-11 15:51:03 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang ro --target-lang en \\\n",
    "    --trainpref $TOKENIZED_PATH_EN_RO/train --validpref $TOKENIZED_PATH_EN_RO/valid \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/it-nl-ro-en\n"
     ]
    }
   ],
   "source": [
    "cd /storage/master-thesis/models/it-nl-ro-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm ./*.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-16 08:42:14 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=2, label_smoothing=0.1, lang_dict=None, lang_pairs='en-it,it-en,en-nl,nl-en,en-ro,ro-en', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[5e-05], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='/storage/master-thesis/models/it-nl-ro-en/checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='/storage/master-thesis/models/it-nl-ro-en/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')\n",
      "2021-12-16 08:42:14 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.\n",
      "2021-12-16 08:42:14 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['en', 'it', 'nl', 'ro']\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 29268 types\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | [it] dictionary: 29268 types\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | [nl] dictionary: 29268 types\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | [ro] dictionary: 29268 types\n",
      "2021-12-16 08:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n",
      "2021-12-16 08:42:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:en-it': 1, 'main:it-en': 1, 'main:en-nl': 1, 'main:nl-en': 1, 'main:en-ro': 1, 'main:ro-en': 1}\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-it src_langtok: 29264; tgt_langtok: 29265\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 929 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.en-it.en\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 929 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.en-it.it\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin valid en-it 929 examples\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:it-en src_langtok: 29265; tgt_langtok: 29264\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 929 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.it-en.it\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 929 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.it-en.en\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin valid it-en 929 examples\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-nl src_langtok: 29264; tgt_langtok: 29266\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 1003 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.en-nl.en\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 1003 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.en-nl.nl\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin valid en-nl 1003 examples\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:nl-en src_langtok: 29266; tgt_langtok: 29264\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 1003 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.nl-en.nl\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 1003 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.nl-en.en\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin valid nl-en 1003 examples\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-ro src_langtok: 29264; tgt_langtok: 29267\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 914 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.en-ro.en\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 914 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.en-ro.ro\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin valid en-ro 914 examples\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ro-en src_langtok: 29267; tgt_langtok: 29264\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 914 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.ro-en.ro\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.data_utils | loaded 914 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/valid.ro-en.en\n",
      "2021-12-16 08:42:15 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin valid ro-en 914 examples\n",
      "2021-12-16 08:42:16 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(29268, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(29268, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=29268, bias=False)\n",
      "  )\n",
      ")\n",
      "2021-12-16 08:42:16 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)\n",
      "2021-12-16 08:42:16 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
      "2021-12-16 08:42:16 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
      "2021-12-16 08:42:16 | INFO | fairseq_cli.train | num. model params: 59123712 (num. trained: 59123712)\n",
      "2021-12-16 08:42:21 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
      "2021-12-16 08:42:21 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2021-12-16 08:42:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-12-16 08:42:21 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 7.795 GB ; name = Quadro RTX 4000                         \n",
      "2021-12-16 08:42:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-12-16 08:42:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2021-12-16 08:42:21 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None\n",
      "2021-12-16 08:42:57 | INFO | fairseq.trainer | loaded checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint_last.pt (epoch 186 @ 0 updates)\n",
      "2021-12-16 08:42:57 | INFO | fairseq.optim.adam | using FusedAdam\n",
      "2021-12-16 08:42:57 | INFO | fairseq.trainer | loading train data for epoch 186\n",
      "2021-12-16 08:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=186/None\n",
      "2021-12-16 08:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:42:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-12-16 08:42:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:en-it': 1, 'main:it-en': 1, 'main:en-nl': 1, 'main:nl-en': 1, 'main:en-ro': 1, 'main:ro-en': 1}\n",
      "2021-12-16 08:42:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-it src_langtok: 29264; tgt_langtok: 29265\n",
      "2021-12-16 08:42:57 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.en-it.en\n",
      "2021-12-16 08:42:57 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.en-it.it\n",
      "2021-12-16 08:42:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin train en-it 145105 examples\n",
      "2021-12-16 08:42:57 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:it-en src_langtok: 29265; tgt_langtok: 29264\n",
      "2021-12-16 08:42:58 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.it-en.it\n",
      "2021-12-16 08:42:58 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.it-en.en\n",
      "2021-12-16 08:42:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin train it-en 145105 examples\n",
      "2021-12-16 08:42:58 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-nl src_langtok: 29264; tgt_langtok: 29266\n",
      "2021-12-16 08:42:59 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.en-nl.en\n",
      "2021-12-16 08:42:59 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.en-nl.nl\n",
      "2021-12-16 08:42:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin train en-nl 145105 examples\n",
      "2021-12-16 08:42:59 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:nl-en src_langtok: 29266; tgt_langtok: 29264\n",
      "2021-12-16 08:42:59 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.nl-en.nl\n",
      "2021-12-16 08:43:00 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.nl-en.en\n",
      "2021-12-16 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin train nl-en 145105 examples\n",
      "2021-12-16 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:en-ro src_langtok: 29264; tgt_langtok: 29267\n",
      "2021-12-16 08:43:00 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.en-ro.en\n",
      "2021-12-16 08:43:00 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.en-ro.ro\n",
      "2021-12-16 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin train en-ro 145105 examples\n",
      "2021-12-16 08:43:00 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:ro-en src_langtok: 29267; tgt_langtok: 29264\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.ro-en.ro\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.data_utils | loaded 145105 examples from: /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin/train.ro-en.en\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/it-nl-ro-en/tokenizers/it-nl-ro-en/bin train ro-en 145105 examples\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:en-it', 145105), ('main:it-en', 145105), ('main:en-nl', 145105), ('main:nl-en', 145105), ('main:en-ro', 145105), ('main:ro-en', 145105)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat\n",
      "2021-12-16 08:43:01 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 870630\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 870630; virtual dataset size 870630\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=186/shard_epoch=1\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:en-it': 145105, 'main:it-en': 145105, 'main:en-nl': 145105, 'main:nl-en': 145105, 'main:en-ro': 145105, 'main:ro-en': 145105}; raw total size: 870630\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:en-it': 145105, 'main:it-en': 145105, 'main:en-nl': 145105, 'main:nl-en': 145105, 'main:en-ro': 145105, 'main:ro-en': 145105}; resampled total size: 870630\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.092095\n",
      "2021-12-16 08:43:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 08:43:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.017810\n",
      "2021-12-16 08:43:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.334847\n",
      "2021-12-16 08:43:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:43:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017467\n",
      "2021-12-16 08:43:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.324094\n",
      "2021-12-16 08:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.677685\n",
      "2021-12-16 08:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 08:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.273315\n",
      "2021-12-16 08:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017339\n",
      "2021-12-16 08:43:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.264423\n",
      "2021-12-16 08:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.556189\n",
      "2021-12-16 08:43:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 186:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 08:43:07 | INFO | fairseq.trainer | begin training epoch 186\n",
      "epoch 186:   0%|                               | 4/5691 [00:00<22:50,  4.15it/s]2021-12-16 08:43:08 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
      "epoch 186:   8%| | 477/5691 [01:11<13:29,  6.44it/s, loss=3.488, nll_loss=1.805,2021-12-16 08:44:18 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
      "epoch 186:  95%|9| 5381/5691 [13:40<00:46,  6.69it/s, loss=3.667, nll_loss=2.0022021-12-16 08:56:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 186: 100%|9| 5690/5691 [14:28<00:00,  6.68it/s, loss=3.527, nll_loss=1.8462021-12-16 08:57:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002066\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072636\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044127\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120524\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001885\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.075006\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044409\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122682\n",
      "2021-12-16 08:57:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 186 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:07,  7.67it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  9.39it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:04, 11.24it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:03, 12.92it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 14.48it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.92it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 17.12it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.78it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.78it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 19.22it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.20it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.85it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 20.08it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 20.03it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:01, 19.97it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 20.13it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 20.07it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 19.95it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 20.05it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.55it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.56it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 08:57:38 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 4.474 | nll_loss 2.811 | ppl 7.02 | wps 49137.6 | wpb 2494.1 | bsz 94.9 | num_updates 5688\n",
      "2021-12-16 08:57:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 08:57:42 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint186.pt (epoch 186 @ 5688 updates, score 4.474) (writing took 4.4163020653650165 seconds)\n",
      "2021-12-16 08:57:43 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)\n",
      "2021-12-16 08:57:43 | INFO | train | epoch 186 | loss 3.535 | nll_loss 1.857 | ppl 3.62 | wps 23353.6 | ups 6.41 | wpb 3645.2 | bsz 153 | num_updates 5688 | lr 4.19296e-05 | gnorm 2.32 | loss_scale 16 | train_wall 1697 | wall 0\n",
      "2021-12-16 08:57:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=187/shard_epoch=1\n",
      "2021-12-16 08:57:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=187/shard_epoch=2\n",
      "2021-12-16 08:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 08:57:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.011882\n",
      "2021-12-16 08:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.321170\n",
      "2021-12-16 08:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.016426\n",
      "2021-12-16 08:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.414130\n",
      "2021-12-16 08:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.752866\n",
      "2021-12-16 08:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 08:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.275177\n",
      "2021-12-16 08:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017443\n",
      "2021-12-16 08:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.433751\n",
      "2021-12-16 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.727475\n",
      "2021-12-16 08:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 187:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 08:57:48 | INFO | fairseq.trainer | begin training epoch 187\n",
      "epoch 187: 100%|9| 5690/5691 [14:32<00:00,  6.71it/s, loss=3.647, nll_loss=1.98,2021-12-16 09:12:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001688\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064751\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042089\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109263\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001407\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064460\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041174\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107675\n",
      "2021-12-16 09:12:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 187 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:07,  8.14it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:   7%|4      | 4/60 [00:00<00:05,  9.94it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  12%|8      | 7/60 [00:00<00:04, 11.79it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  17%|#     | 10/60 [00:00<00:03, 13.57it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  22%|#3    | 13/60 [00:00<00:03, 15.13it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  27%|#6    | 16/60 [00:00<00:02, 16.43it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 17.22it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.85it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.86it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 19.28it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.44it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.95it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 20.12it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 20.00it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:00, 20.01it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 20.19it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 20.14it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 20.10it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 20.18it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.69it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.66it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 09:12:25 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 4.484 | nll_loss 2.815 | ppl 7.04 | wps 49345.4 | wpb 2494.1 | bsz 94.9 | num_updates 11379 | best_loss 4.474\n",
      "2021-12-16 09:12:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 09:12:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint187.pt (epoch 187 @ 11379 updates, score 4.484) (writing took 3.7550701797008514 seconds)\n",
      "2021-12-16 09:12:28 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)\n",
      "2021-12-16 09:12:28 | INFO | train | epoch 187 | loss 3.594 | nll_loss 1.92 | ppl 3.78 | wps 23419.9 | ups 6.42 | wpb 3645.2 | bsz 153 | num_updates 11379 | lr 2.96448e-05 | gnorm 2.356 | loss_scale 16 | train_wall 848 | wall 0\n",
      "2021-12-16 09:12:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=188/shard_epoch=2\n",
      "2021-12-16 09:12:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=188/shard_epoch=3\n",
      "2021-12-16 09:12:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:12:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012052\n",
      "2021-12-16 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.331511\n",
      "2021-12-16 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.018376\n",
      "2021-12-16 09:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.316051\n",
      "2021-12-16 09:12:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.667081\n",
      "2021-12-16 09:12:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:12:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.285746\n",
      "2021-12-16 09:12:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017729\n",
      "2021-12-16 09:12:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.406391\n",
      "2021-12-16 09:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.710998\n",
      "2021-12-16 09:12:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 188:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 09:12:34 | INFO | fairseq.trainer | begin training epoch 188\n",
      "epoch 188: 100%|9| 5690/5691 [14:33<00:00,  6.58it/s, loss=3.558, nll_loss=1.8812021-12-16 09:27:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001821\n",
      "2021-12-16 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064708\n",
      "2021-12-16 09:27:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040634\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107835\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001496\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064845\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040622\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107501\n",
      "2021-12-16 09:27:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 188 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:07,  7.52it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  9.22it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:04, 11.09it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:03, 12.84it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 14.46it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.98it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 17.16it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.75it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.74it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 19.15it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.38it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.88it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 20.03it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.88it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:01, 19.84it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 20.04it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 20.08it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 20.00it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 20.06it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.59it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 09:27:11 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 4.468 | nll_loss 2.799 | ppl 6.96 | wps 49198.8 | wpb 2494.1 | bsz 94.9 | num_updates 17070 | best_loss 4.468\n",
      "2021-12-16 09:27:11 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 09:27:15 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint188.pt (epoch 188 @ 17070 updates, score 4.468) (writing took 4.478164377622306 seconds)\n",
      "2021-12-16 09:27:15 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)\n",
      "2021-12-16 09:27:15 | INFO | train | epoch 188 | loss 3.566 | nll_loss 1.89 | ppl 3.71 | wps 23389.5 | ups 6.42 | wpb 3645.2 | bsz 153 | num_updates 17070 | lr 2.42038e-05 | gnorm 2.322 | loss_scale 16 | train_wall 848 | wall 0\n",
      "2021-12-16 09:27:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=189/shard_epoch=3\n",
      "2021-12-16 09:27:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=189/shard_epoch=4\n",
      "2021-12-16 09:27:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:27:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.013394\n",
      "2021-12-16 09:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.338472\n",
      "2021-12-16 09:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.020304\n",
      "2021-12-16 09:27:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.325942\n",
      "2021-12-16 09:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.685847\n",
      "2021-12-16 09:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.285658\n",
      "2021-12-16 09:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017331\n",
      "2021-12-16 09:27:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.312216\n",
      "2021-12-16 09:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.616361\n",
      "2021-12-16 09:27:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 189:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 09:27:21 | INFO | fairseq.trainer | begin training epoch 189\n",
      "epoch 189: 100%|9| 5690/5691 [14:32<00:00,  6.83it/s, loss=3.543, nll_loss=1.8652021-12-16 09:41:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001910\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064772\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.050235\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117703\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001684\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070761\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053150\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.126444\n",
      "2021-12-16 09:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 189 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  7.09it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  8.77it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:05, 10.62it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 12.40it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 14.08it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.66it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.88it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.57it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.61it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 19.06it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.29it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.85it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 19.99it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.89it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:01, 19.95it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 20.19it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 20.11it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 20.05it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 20.09it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.57it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.55it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 09:41:57 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 4.458 | nll_loss 2.788 | ppl 6.91 | wps 49254.1 | wpb 2494.1 | bsz 94.9 | num_updates 22761 | best_loss 4.458\n",
      "2021-12-16 09:41:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 09:42:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint189.pt (epoch 189 @ 22761 updates, score 4.458) (writing took 4.332206594757736 seconds)\n",
      "2021-12-16 09:42:01 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)\n",
      "2021-12-16 09:42:01 | INFO | train | epoch 189 | loss 3.549 | nll_loss 1.872 | ppl 3.66 | wps 23422.5 | ups 6.43 | wpb 3645.2 | bsz 153 | num_updates 22761 | lr 2.09606e-05 | gnorm 2.315 | loss_scale 32 | train_wall 847 | wall 0\n",
      "2021-12-16 09:42:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=190/shard_epoch=4\n",
      "2021-12-16 09:42:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=190/shard_epoch=5\n",
      "2021-12-16 09:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:42:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.013940\n",
      "2021-12-16 09:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.336409\n",
      "2021-12-16 09:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017457\n",
      "2021-12-16 09:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.315317\n",
      "2021-12-16 09:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.670363\n",
      "2021-12-16 09:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.278725\n",
      "2021-12-16 09:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017985\n",
      "2021-12-16 09:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:42:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.371950\n",
      "2021-12-16 09:42:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.669774\n",
      "2021-12-16 09:42:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 190:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 09:42:06 | INFO | fairseq.trainer | begin training epoch 190\n",
      "epoch 190:  11%|1| 613/5691 [01:34<12:58,  6.52it/s, loss=3.545, nll_loss=1.866,2021-12-16 09:43:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 190: 100%|9| 5690/5691 [14:30<00:00,  6.26it/s, loss=3.518, nll_loss=1.8382021-12-16 09:56:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001670\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064282\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041273\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108175\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001399\n",
      "2021-12-16 09:56:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063668\n",
      "2021-12-16 09:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040639\n",
      "2021-12-16 09:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106251\n",
      "2021-12-16 09:56:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 190 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:07,  7.58it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:   7%|4      | 4/60 [00:00<00:05,  9.34it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  12%|8      | 7/60 [00:00<00:04, 11.18it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  17%|#     | 10/60 [00:00<00:03, 12.95it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 14.46it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.95it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 17.16it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.75it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.73it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 19.18it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.39it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.90it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 20.07it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.90it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:01, 19.88it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 20.00it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 19.88it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 19.86it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 19.89it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  90%|#####4| 54/60 [00:02<00:00, 19.37it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  93%|#####6| 56/60 [00:02<00:00, 19.29it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 09:56:41 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 4.461 | nll_loss 2.797 | ppl 6.95 | wps 48973.9 | wpb 2494.1 | bsz 94.9 | num_updates 28451 | best_loss 4.458\n",
      "2021-12-16 09:56:41 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 09:56:44 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint190.pt (epoch 190 @ 28451 updates, score 4.461) (writing took 3.347434313967824 seconds)\n",
      "2021-12-16 09:56:44 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)\n",
      "2021-12-16 09:56:44 | INFO | train | epoch 190 | loss 3.538 | nll_loss 1.859 | ppl 3.63 | wps 23486.5 | ups 6.44 | wpb 3645.2 | bsz 153 | num_updates 28451 | lr 1.87478e-05 | gnorm 2.322 | loss_scale 16 | train_wall 846 | wall 0\n",
      "2021-12-16 09:56:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=191/shard_epoch=5\n",
      "2021-12-16 09:56:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=191/shard_epoch=6\n",
      "2021-12-16 09:56:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:56:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012655\n",
      "2021-12-16 09:56:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.337022\n",
      "2021-12-16 09:56:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017747\n",
      "2021-12-16 09:56:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.320602\n",
      "2021-12-16 09:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.676518\n",
      "2021-12-16 09:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 09:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.283341\n",
      "2021-12-16 09:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017552\n",
      "2021-12-16 09:56:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 09:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.494134\n",
      "2021-12-16 09:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.796125\n",
      "2021-12-16 09:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 191:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 09:56:50 | INFO | fairseq.trainer | begin training epoch 191\n",
      "epoch 191: 100%|9| 5690/5691 [14:31<00:00,  6.35it/s, loss=3.556, nll_loss=1.88,2021-12-16 10:11:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001830\n",
      "2021-12-16 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066095\n",
      "2021-12-16 10:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042085\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110718\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001583\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065470\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042968\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110784\n",
      "2021-12-16 10:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 191 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:07,  7.63it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  9.35it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:04, 11.19it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:03, 12.93it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 14.50it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.90it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 17.16it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.86it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.89it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 19.36it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.53it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 20.04it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 20.21it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 20.10it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:00, 20.02it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 20.20it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 20.16it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 20.13it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 20.24it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.74it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 10:11:25 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 4.449 | nll_loss 2.779 | ppl 6.86 | wps 49313.3 | wpb 2494.1 | bsz 94.9 | num_updates 34142 | best_loss 4.449\n",
      "2021-12-16 10:11:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 10:11:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint191.pt (epoch 191 @ 34142 updates, score 4.449) (writing took 4.352162350900471 seconds)\n",
      "2021-12-16 10:11:29 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)\n",
      "2021-12-16 10:11:29 | INFO | train | epoch 191 | loss 3.528 | nll_loss 1.849 | ppl 3.6 | wps 23440 | ups 6.43 | wpb 3645.2 | bsz 153 | num_updates 34142 | lr 1.71142e-05 | gnorm 2.326 | loss_scale 16 | train_wall 847 | wall 0\n",
      "2021-12-16 10:11:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=192/shard_epoch=6\n",
      "2021-12-16 10:11:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=192/shard_epoch=7\n",
      "2021-12-16 10:11:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:11:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.013985\n",
      "2021-12-16 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.329776\n",
      "2021-12-16 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017098\n",
      "2021-12-16 10:11:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.342022\n",
      "2021-12-16 10:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.690155\n",
      "2021-12-16 10:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.280494\n",
      "2021-12-16 10:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.022537\n",
      "2021-12-16 10:11:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:11:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.313030\n",
      "2021-12-16 10:11:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.617188\n",
      "2021-12-16 10:11:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 192:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 10:11:35 | INFO | fairseq.trainer | begin training epoch 192\n",
      "epoch 192: 100%|9| 5690/5691 [14:33<00:00,  6.77it/s, loss=3.521, nll_loss=1.8422021-12-16 10:26:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001850\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068821\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043138\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115759\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001619\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064823\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044413\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111552\n",
      "2021-12-16 10:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 192 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  6.73it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  8.38it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:05, 10.19it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 11.97it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 13.65it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.25it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.58it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.37it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:02, 18.38it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 18.87it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.03it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  50%|###   | 30/60 [00:01<00:01, 19.29it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  55%|###3  | 33/60 [00:01<00:01, 19.64it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  60%|###6  | 36/60 [00:01<00:01, 19.78it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  63%|###8  | 38/60 [00:01<00:01, 19.69it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:01, 19.53it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 19.85it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 19.95it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 19.93it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 20.01it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.53it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 10:26:11 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 4.45 | nll_loss 2.785 | ppl 6.89 | wps 48794.7 | wpb 2494.1 | bsz 94.9 | num_updates 39833 | best_loss 4.449\n",
      "2021-12-16 10:26:11 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 10:26:15 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint192.pt (epoch 192 @ 39833 updates, score 4.45) (writing took 3.330123201943934 seconds)\n",
      "2021-12-16 10:26:15 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)\n",
      "2021-12-16 10:26:15 | INFO | train | epoch 192 | loss 3.522 | nll_loss 1.842 | ppl 3.59 | wps 23422.3 | ups 6.43 | wpb 3645.2 | bsz 153 | num_updates 39833 | lr 1.58445e-05 | gnorm 2.332 | loss_scale 32 | train_wall 848 | wall 0\n",
      "2021-12-16 10:26:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=193/shard_epoch=7\n",
      "2021-12-16 10:26:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=193/shard_epoch=8\n",
      "2021-12-16 10:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:26:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012675\n",
      "2021-12-16 10:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.361518\n",
      "2021-12-16 10:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.020690\n",
      "2021-12-16 10:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.324353\n",
      "2021-12-16 10:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.707704\n",
      "2021-12-16 10:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.282658\n",
      "2021-12-16 10:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017483\n",
      "2021-12-16 10:26:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:26:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.401894\n",
      "2021-12-16 10:26:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.703187\n",
      "2021-12-16 10:26:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 193:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 10:26:20 | INFO | fairseq.trainer | begin training epoch 193\n",
      "epoch 193: 100%|9| 5690/5691 [14:33<00:00,  6.62it/s, loss=3.502, nll_loss=1.8212021-12-16 10:40:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 10:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001711\n",
      "2021-12-16 10:40:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063451\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041183\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107051\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001418\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064913\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041954\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109196\n",
      "2021-12-16 10:40:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 193 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  7.34it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:   7%|4      | 4/60 [00:00<00:06,  9.08it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  12%|8      | 7/60 [00:00<00:04, 10.92it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 12.61it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 14.24it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.76it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.96it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.64it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.58it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 19.05it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.25it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.77it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 19.89it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.81it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  65%|###9  | 39/60 [00:02<00:01, 19.79it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  70%|####1 | 42/60 [00:02<00:00, 19.97it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  75%|####5 | 45/60 [00:02<00:00, 19.78it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  80%|####8 | 48/60 [00:02<00:00, 19.87it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  85%|#####1| 51/60 [00:02<00:00, 19.96it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  88%|#####3| 53/60 [00:02<00:00, 19.72it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.37it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 10:40:58 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 4.447 | nll_loss 2.785 | ppl 6.89 | wps 48965.9 | wpb 2494.1 | bsz 94.9 | num_updates 45524 | best_loss 4.447\n",
      "2021-12-16 10:40:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 10:41:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint193.pt (epoch 193 @ 45524 updates, score 4.447) (writing took 4.340846129693091 seconds)\n",
      "2021-12-16 10:41:02 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)\n",
      "2021-12-16 10:41:02 | INFO | train | epoch 193 | loss 3.516 | nll_loss 1.836 | ppl 3.57 | wps 23376.4 | ups 6.41 | wpb 3645.2 | bsz 153 | num_updates 45524 | lr 1.48211e-05 | gnorm 2.341 | loss_scale 32 | train_wall 849 | wall 0\n",
      "2021-12-16 10:41:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=194/shard_epoch=8\n",
      "2021-12-16 10:41:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=194/shard_epoch=9\n",
      "2021-12-16 10:41:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:41:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012970\n",
      "2021-12-16 10:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.324715\n",
      "2021-12-16 10:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017100\n",
      "2021-12-16 10:41:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.330089\n",
      "2021-12-16 10:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.673038\n",
      "2021-12-16 10:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.278114\n",
      "2021-12-16 10:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.018776\n",
      "2021-12-16 10:41:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.363674\n",
      "2021-12-16 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.661678\n",
      "2021-12-16 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 194:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 10:41:08 | INFO | fairseq.trainer | begin training epoch 194\n",
      "epoch 194: 100%|9| 5690/5691 [14:33<00:00,  6.37it/s, loss=3.483, nll_loss=1.8012021-12-16 10:55:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001803\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063847\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042011\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108305\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001550\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065418\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042093\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109655\n",
      "2021-12-16 10:55:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 194 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  6.80it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  8.46it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:05, 10.28it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 12.05it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 13.77it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.37it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.72it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.55it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.53it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 18.97it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  48%|##9   | 29/60 [00:01<00:01, 19.36it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  53%|###2  | 32/60 [00:01<00:01, 19.82it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  58%|###5  | 35/60 [00:01<00:01, 20.12it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  63%|###8  | 38/60 [00:01<00:01, 19.95it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  68%|####1 | 41/60 [00:02<00:00, 20.05it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  73%|####3 | 44/60 [00:02<00:00, 20.14it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  78%|####7 | 47/60 [00:02<00:00, 20.20it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  83%|##### | 50/60 [00:02<00:00, 20.13it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  88%|#####3| 53/60 [00:02<00:00, 19.81it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.35it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 10:55:45 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 4.444 | nll_loss 2.77 | ppl 6.82 | wps 49146.7 | wpb 2494.1 | bsz 94.9 | num_updates 51215 | best_loss 4.444\n",
      "2021-12-16 10:55:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 10:55:49 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint194.pt (epoch 194 @ 51215 updates, score 4.444) (writing took 4.277315312996507 seconds)\n",
      "2021-12-16 10:55:50 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)\n",
      "2021-12-16 10:55:50 | INFO | train | epoch 194 | loss 3.511 | nll_loss 1.83 | ppl 3.56 | wps 23383.2 | ups 6.41 | wpb 3645.2 | bsz 153 | num_updates 51215 | lr 1.39734e-05 | gnorm 2.344 | loss_scale 32 | train_wall 848 | wall 0\n",
      "2021-12-16 10:55:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=195/shard_epoch=9\n",
      "2021-12-16 10:55:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=195/shard_epoch=10\n",
      "2021-12-16 10:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:55:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.014018\n",
      "2021-12-16 10:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.327355\n",
      "2021-12-16 10:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015825\n",
      "2021-12-16 10:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.378181\n",
      "2021-12-16 10:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.722514\n",
      "2021-12-16 10:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 10:55:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.284212\n",
      "2021-12-16 10:55:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017857\n",
      "2021-12-16 10:55:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 10:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.385266\n",
      "2021-12-16 10:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.688569\n",
      "2021-12-16 10:55:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 195:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 10:55:55 | INFO | fairseq.trainer | begin training epoch 195\n",
      "epoch 195:  20%|1| 1112/5691 [02:51<11:45,  6.49it/s, loss=3.497, nll_loss=1.8142021-12-16 10:58:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 195: 100%|9| 5690/5691 [14:31<00:00,  6.40it/s, loss=3.464, nll_loss=1.78,2021-12-16 11:10:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001823\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066606\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040976\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110088\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001782\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073839\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054350\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130848\n",
      "2021-12-16 11:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 195 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  6.78it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  8.43it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:05, 10.23it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 11.99it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 13.68it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.27it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.60it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.40it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:02, 18.41it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 18.90it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.18it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.69it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 19.89it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.86it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:01, 19.86it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 20.04it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 20.05it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 19.89it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 19.97it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.46it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  95%|#####6| 57/60 [00:02<00:00, 19.46it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  98%|#####8| 59/60 [00:03<00:00, 19.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 11:10:30 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 4.447 | nll_loss 2.779 | ppl 6.87 | wps 48775.9 | wpb 2494.1 | bsz 94.9 | num_updates 56905 | best_loss 4.444\n",
      "2021-12-16 11:10:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 11:10:33 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint195.pt (epoch 195 @ 56905 updates, score 4.447) (writing took 3.277908035553992 seconds)\n",
      "2021-12-16 11:10:33 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)\n",
      "2021-12-16 11:10:33 | INFO | train | epoch 195 | loss 3.506 | nll_loss 1.825 | ppl 3.54 | wps 23464 | ups 6.44 | wpb 3645.4 | bsz 153 | num_updates 56905 | lr 1.32564e-05 | gnorm 2.353 | loss_scale 16 | train_wall 847 | wall 0\n",
      "2021-12-16 11:10:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=196/shard_epoch=10\n",
      "2021-12-16 11:10:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=196/shard_epoch=11\n",
      "2021-12-16 11:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:10:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012883\n",
      "2021-12-16 11:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.320749\n",
      "2021-12-16 11:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015928\n",
      "2021-12-16 11:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.297622\n",
      "2021-12-16 11:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.635588\n",
      "2021-12-16 11:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.275529\n",
      "2021-12-16 11:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015407\n",
      "2021-12-16 11:10:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:10:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.494418\n",
      "2021-12-16 11:10:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.786475\n",
      "2021-12-16 11:10:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 196:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 11:10:39 | INFO | fairseq.trainer | begin training epoch 196\n",
      "epoch 196: 100%|9| 5690/5691 [14:31<00:00,  6.46it/s, loss=3.528, nll_loss=1.8512021-12-16 11:25:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002003\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064626\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043195\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110520\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001364\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064857\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040687\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107487\n",
      "2021-12-16 11:25:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 196 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  7.13it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  8.82it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:05, 10.67it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 12.45it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 14.14it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.66it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.79it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.51it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.59it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 19.03it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.20it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.74it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 19.97it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.85it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 20.07it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 20.05it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 20.01it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 20.08it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.57it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 11:25:14 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 4.441 | nll_loss 2.778 | ppl 6.86 | wps 49007.6 | wpb 2494.1 | bsz 94.9 | num_updates 62596 | best_loss 4.441\n",
      "2021-12-16 11:25:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 11:25:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint196.pt (epoch 196 @ 62596 updates, score 4.441) (writing took 5.298999154008925 seconds)\n",
      "2021-12-16 11:25:20 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)\n",
      "2021-12-16 11:25:20 | INFO | train | epoch 196 | loss 3.503 | nll_loss 1.822 | ppl 3.53 | wps 23402.5 | ups 6.42 | wpb 3645.2 | bsz 153 | num_updates 62596 | lr 1.26394e-05 | gnorm 2.362 | loss_scale 16 | train_wall 846 | wall 0\n",
      "2021-12-16 11:25:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=197/shard_epoch=11\n",
      "2021-12-16 11:25:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=197/shard_epoch=12\n",
      "2021-12-16 11:25:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:25:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012443\n",
      "2021-12-16 11:25:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.368468\n",
      "2021-12-16 11:25:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.019718\n",
      "2021-12-16 11:25:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.751904\n",
      "2021-12-16 11:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:03.141263\n",
      "2021-12-16 11:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.283157\n",
      "2021-12-16 11:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.020148\n",
      "2021-12-16 11:25:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.493316\n",
      "2021-12-16 11:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.797794\n",
      "2021-12-16 11:25:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 197:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 11:25:26 | INFO | fairseq.trainer | begin training epoch 197\n",
      "epoch 197: 100%|9| 5690/5691 [14:32<00:00,  6.40it/s, loss=3.448, nll_loss=1.7632021-12-16 11:39:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001837\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070099\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041361\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114048\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001646\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071511\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.056592\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130756\n",
      "2021-12-16 11:39:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 197 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  6.75it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  8.40it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:05, 10.20it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 11.98it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 13.70it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.31it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.66it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.40it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:02, 18.42it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 18.92it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.22it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.73it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 19.89it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.85it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 20.02it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 19.97it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 19.87it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  87%|#####2| 52/60 [00:02<00:00, 19.94it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  90%|#####4| 54/60 [00:02<00:00, 19.52it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  93%|#####6| 56/60 [00:02<00:00, 19.45it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 11:40:02 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 4.449 | nll_loss 2.782 | ppl 6.88 | wps 48870.2 | wpb 2494.1 | bsz 94.9 | num_updates 68287 | best_loss 4.441\n",
      "2021-12-16 11:40:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 11:40:06 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint197.pt (epoch 197 @ 68287 updates, score 4.449) (writing took 3.284636849537492 seconds)\n",
      "2021-12-16 11:40:06 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)\n",
      "2021-12-16 11:40:06 | INFO | train | epoch 197 | loss 3.499 | nll_loss 1.818 | ppl 3.53 | wps 23416 | ups 6.42 | wpb 3645.2 | bsz 153 | num_updates 68287 | lr 1.21013e-05 | gnorm 2.369 | loss_scale 16 | train_wall 847 | wall 0\n",
      "2021-12-16 11:40:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=198/shard_epoch=12\n",
      "2021-12-16 11:40:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=198/shard_epoch=13\n",
      "2021-12-16 11:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:40:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012688\n",
      "2021-12-16 11:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.335038\n",
      "2021-12-16 11:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.018295\n",
      "2021-12-16 11:40:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.308641\n",
      "2021-12-16 11:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.663133\n",
      "2021-12-16 11:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.285854\n",
      "2021-12-16 11:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.016725\n",
      "2021-12-16 11:40:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.363071\n",
      "2021-12-16 11:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.666803\n",
      "2021-12-16 11:40:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 198:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 11:40:11 | INFO | fairseq.trainer | begin training epoch 198\n",
      "epoch 198:  43%|4| 2457/5691 [06:19<08:27,  6.37it/s, loss=3.481, nll_loss=1.7992021-12-16 11:46:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 198: 100%|9| 5690/5691 [14:33<00:00,  6.54it/s, loss=3.551, nll_loss=1.8762021-12-16 11:54:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001790\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067335\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041260\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111239\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001517\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069732\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041341\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.113176\n",
      "2021-12-16 11:54:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 198 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  6.85it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:   7%|4      | 4/60 [00:00<00:06,  8.55it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  12%|8      | 7/60 [00:00<00:05, 10.38it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 12.09it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 13.75it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.33it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.62it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.36it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:02, 18.40it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 18.95it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.18it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.71it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 19.85it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.77it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  65%|###9  | 39/60 [00:02<00:01, 19.74it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  70%|####1 | 42/60 [00:02<00:00, 20.00it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  75%|####5 | 45/60 [00:02<00:00, 19.95it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  80%|####8 | 48/60 [00:02<00:00, 20.04it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  85%|#####1| 51/60 [00:02<00:00, 20.04it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  90%|#####4| 54/60 [00:02<00:00, 19.58it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  93%|#####6| 56/60 [00:02<00:00, 19.42it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.29it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 11:54:48 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 4.442 | nll_loss 2.773 | ppl 6.84 | wps 48812.4 | wpb 2494.1 | bsz 94.9 | num_updates 73977 | best_loss 4.441\n",
      "2021-12-16 11:54:48 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 11:54:53 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint198.pt (epoch 198 @ 73977 updates, score 4.442) (writing took 4.064240640960634 seconds)\n",
      "2021-12-16 11:54:53 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)\n",
      "2021-12-16 11:54:53 | INFO | train | epoch 198 | loss 3.496 | nll_loss 1.814 | ppl 3.52 | wps 23391.1 | ups 6.42 | wpb 3645.3 | bsz 153 | num_updates 73977 | lr 1.16266e-05 | gnorm 2.378 | loss_scale 16 | train_wall 848 | wall 0\n",
      "2021-12-16 11:54:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=199/shard_epoch=13\n",
      "2021-12-16 11:54:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=199/shard_epoch=14\n",
      "2021-12-16 11:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:54:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012213\n",
      "2021-12-16 11:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.333908\n",
      "2021-12-16 11:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017764\n",
      "2021-12-16 11:54:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.388704\n",
      "2021-12-16 11:54:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.741498\n",
      "2021-12-16 11:54:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 11:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.269790\n",
      "2021-12-16 11:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.018362\n",
      "2021-12-16 11:54:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 11:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.537216\n",
      "2021-12-16 11:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.826515\n",
      "2021-12-16 11:54:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 199:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 11:54:58 | INFO | fairseq.trainer | begin training epoch 199\n",
      "epoch 199: 100%|9| 5690/5691 [14:31<00:00,  6.60it/s, loss=3.492, nll_loss=1.8122021-12-16 12:09:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002022\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064410\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041395\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108622\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001598\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068480\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043123\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114153\n",
      "2021-12-16 12:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 199 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  6.68it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  8.34it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:05, 10.17it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 11.96it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 13.65it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.23it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.57it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.36it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:02, 18.45it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 18.97it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.18it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.73it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 19.89it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.73it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  65%|###9  | 39/60 [00:02<00:01, 19.63it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  70%|####1 | 42/60 [00:02<00:00, 19.79it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  73%|####3 | 44/60 [00:02<00:00, 19.83it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  78%|####7 | 47/60 [00:02<00:00, 19.96it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  83%|##### | 50/60 [00:02<00:00, 20.09it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  88%|#####3| 53/60 [00:02<00:00, 19.78it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.24it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.27it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset: 100%|######| 60/60 [00:03<00:00, 19.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 12:09:33 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 4.433 | nll_loss 2.766 | ppl 6.8 | wps 48798.8 | wpb 2494.1 | bsz 94.9 | num_updates 79668 | best_loss 4.433\n",
      "2021-12-16 12:09:33 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 12:09:38 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint199.pt (epoch 199 @ 79668 updates, score 4.433) (writing took 4.898236894980073 seconds)\n",
      "2021-12-16 12:09:38 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)\n",
      "2021-12-16 12:09:38 | INFO | train | epoch 199 | loss 3.493 | nll_loss 1.811 | ppl 3.51 | wps 23420 | ups 6.42 | wpb 3645.2 | bsz 153 | num_updates 79668 | lr 1.12036e-05 | gnorm 2.384 | loss_scale 16 | train_wall 846 | wall 0\n",
      "2021-12-16 12:09:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=200/shard_epoch=14\n",
      "2021-12-16 12:09:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=200/shard_epoch=15\n",
      "2021-12-16 12:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:09:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012788\n",
      "2021-12-16 12:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.335190\n",
      "2021-12-16 12:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015312\n",
      "2021-12-16 12:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.347501\n",
      "2021-12-16 12:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.699116\n",
      "2021-12-16 12:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.275774\n",
      "2021-12-16 12:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015663\n",
      "2021-12-16 12:09:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.311861\n",
      "2021-12-16 12:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.604406\n",
      "2021-12-16 12:09:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 200:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 12:09:44 | INFO | fairseq.trainer | begin training epoch 200\n",
      "epoch 200: 100%|9| 5690/5691 [14:32<00:00,  6.47it/s, loss=3.515, nll_loss=1.8382021-12-16 12:24:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001840\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063675\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041718\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107891\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001509\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064995\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040696\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107810\n",
      "2021-12-16 12:24:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 200 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:08,  6.69it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  8.35it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:05, 10.19it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 11.97it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 13.69it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.29it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.65it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.44it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:02, 18.45it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 18.95it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  47%|##8   | 28/60 [00:01<00:01, 19.17it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  52%|###1  | 31/60 [00:01<00:01, 19.75it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  57%|###4  | 34/60 [00:01<00:01, 19.97it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  62%|###7  | 37/60 [00:01<00:01, 19.87it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  67%|####  | 40/60 [00:02<00:01, 19.74it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  72%|####3 | 43/60 [00:02<00:00, 19.93it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  77%|####6 | 46/60 [00:02<00:00, 19.95it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  82%|####9 | 49/60 [00:02<00:00, 19.92it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  85%|#####1| 51/60 [00:02<00:00, 19.90it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  88%|#####3| 53/60 [00:02<00:00, 19.54it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.10it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 12:24:20 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 4.437 | nll_loss 2.772 | ppl 6.83 | wps 48850.6 | wpb 2494.1 | bsz 94.9 | num_updates 85359 | best_loss 4.433\n",
      "2021-12-16 12:24:20 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 12:24:24 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint200.pt (epoch 200 @ 85359 updates, score 4.437) (writing took 3.71515859849751 seconds)\n",
      "2021-12-16 12:24:24 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)\n",
      "2021-12-16 12:24:24 | INFO | train | epoch 200 | loss 3.49 | nll_loss 1.809 | ppl 3.5 | wps 23430.6 | ups 6.43 | wpb 3645.2 | bsz 153 | num_updates 85359 | lr 1.08237e-05 | gnorm 2.392 | loss_scale 16 | train_wall 847 | wall 0\n",
      "2021-12-16 12:24:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=201/shard_epoch=15\n",
      "2021-12-16 12:24:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=201/shard_epoch=16\n",
      "2021-12-16 12:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:24:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012291\n",
      "2021-12-16 12:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.326021\n",
      "2021-12-16 12:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017819\n",
      "2021-12-16 12:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.318326\n",
      "2021-12-16 12:24:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.663287\n",
      "2021-12-16 12:24:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.276711\n",
      "2021-12-16 12:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.019258\n",
      "2021-12-16 12:24:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:24:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.570564\n",
      "2021-12-16 12:24:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.867658\n",
      "2021-12-16 12:24:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 201:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 12:24:29 | INFO | fairseq.trainer | begin training epoch 201\n",
      "epoch 201:  52%|5| 2969/5691 [07:35<07:00,  6.48it/s, loss=3.513, nll_loss=1.8332021-12-16 12:32:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 201: 100%|9| 5690/5691 [14:32<00:00,  6.66it/s, loss=3.532, nll_loss=1.8552021-12-16 12:39:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001799\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066558\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042296\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111614\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001648\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065498\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042247\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110325\n",
      "2021-12-16 12:39:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 201 | valid on 'valid' subset:   0%|               | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:   2%|1      | 1/60 [00:00<00:09,  6.55it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:   5%|3      | 3/60 [00:00<00:06,  8.20it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  10%|7      | 6/60 [00:00<00:05,  9.98it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  15%|#      | 9/60 [00:00<00:04, 11.77it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  20%|#2    | 12/60 [00:00<00:03, 13.54it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  25%|#5    | 15/60 [00:00<00:02, 15.19it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  30%|#8    | 18/60 [00:00<00:02, 16.55it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  33%|##    | 20/60 [00:01<00:02, 17.40it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  38%|##3   | 23/60 [00:01<00:01, 18.51it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  43%|##6   | 26/60 [00:01<00:01, 19.02it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  48%|##9   | 29/60 [00:01<00:01, 19.39it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  53%|###2  | 32/60 [00:01<00:01, 19.82it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  58%|###5  | 35/60 [00:01<00:01, 20.12it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  63%|###8  | 38/60 [00:01<00:01, 19.90it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  68%|####1 | 41/60 [00:02<00:00, 19.95it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  73%|####3 | 44/60 [00:02<00:00, 20.05it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  78%|####7 | 47/60 [00:02<00:00, 20.09it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  83%|##### | 50/60 [00:02<00:00, 20.18it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  88%|#####3| 53/60 [00:02<00:00, 19.84it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  92%|#####5| 55/60 [00:02<00:00, 19.45it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  97%|#####8| 58/60 [00:02<00:00, 19.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-12-16 12:39:05 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 4.446 | nll_loss 2.785 | ppl 6.89 | wps 49212.9 | wpb 2494.1 | bsz 94.9 | num_updates 91049 | best_loss 4.433\n",
      "2021-12-16 12:39:05 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-12-16 12:39:09 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/it-nl-ro-en/checkpoint201.pt (epoch 201 @ 91049 updates, score 4.446) (writing took 3.7065955540165305 seconds)\n",
      "2021-12-16 12:39:09 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)\n",
      "2021-12-16 12:39:09 | INFO | train | epoch 201 | loss 3.488 | nll_loss 1.806 | ppl 3.5 | wps 23424.5 | ups 6.43 | wpb 3645.3 | bsz 153 | num_updates 91049 | lr 1.048e-05 | gnorm 2.4 | loss_scale 16 | train_wall 847 | wall 0\n",
      "2021-12-16 12:39:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=202/shard_epoch=16\n",
      "2021-12-16 12:39:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=202/shard_epoch=17\n",
      "2021-12-16 12:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:39:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.012435\n",
      "2021-12-16 12:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.320185\n",
      "2021-12-16 12:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015956\n",
      "2021-12-16 12:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.483548\n",
      "2021-12-16 12:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.820858\n",
      "2021-12-16 12:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-12-16 12:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.283894\n",
      "2021-12-16 12:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017077\n",
      "2021-12-16 12:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-12-16 12:39:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:02.363444\n",
      "2021-12-16 12:39:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:02.665590\n",
      "2021-12-16 12:39:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 202:   0%|                                       | 0/5691 [00:00<?, ?it/s]2021-12-16 12:39:15 | INFO | fairseq.trainer | begin training epoch 202\n",
      "epoch 202:  41%|4| 2321/5691 [05:55<08:21,  6.72it/s, loss=3.428, nll_loss=1.74,^C\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 352, in cli_main\n",
      "    distributed_utils.call_main(args, main)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/distributed_utils.py\", line 301, in call_main\n",
      "    main(args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 125, in main\n",
      "    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq_cli/train.py\", line 208, in train\n",
      "    log_output = trainer.train_step(samples)\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 52, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/trainer.py\", line 486, in train_step\n",
      "    ignore_grad=is_dummy_batch,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/tasks/fairseq_task.py\", line 416, in train_step\n",
      "    loss, sample_size, logging_output = criterion(model, sample)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/criterions/label_smoothed_cross_entropy.py\", line 69, in forward\n",
      "    net_output = model(**sample[\"net_input\"])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/models/transformer.py\", line 270, in forward\n",
      "    src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/models/transformer.py\", line 425, in forward\n",
      "    x = layer(x, encoder_padding_mask)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/modules/transformer_layer.py\", line 137, in forward\n",
      "    attn_mask=attn_mask,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fairseq/modules/multihead_attention.py\", line 186, in forward\n",
      "    v_proj_weight=self.v_proj.weight,\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 5072, in multi_head_attention_forward\n",
      "    new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! fairseq-train $BIN_DIR \\\n",
    "    --arch=transformer --share-all-embeddings \\\n",
    "    --task translation_multi_simple_epoch --lang-pairs en-it,it-en,en-nl,nl-en,en-ro,ro-en \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-5 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --restore-file $MODEL_DIR/checkpoint_last.pt \\\n",
    "    --save-dir $MODEL_DIR/ \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --reset-optimizer \\\n",
    "    --encoder-langtok \"src\" \\\n",
    "    --decoder-langtok \\\n",
    "    --fp16 \n",
    "    #--update-freq 2\n",
    "    #original lr: 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "! echo $MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "cd /storage/master-thesis/models/quy-es+es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm checkpoint108.pt\n",
    "! rm checkpoint109.pt\n",
    "! rm checkpoint_best.pt\n",
    "! rm checkpoint_last.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm dict.en.txt\n",
    "! rm dict.es.txt\n",
    "! rm dict.quy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5K\t/notebooks/CITATION.cff\n",
      "5.5K\t/notebooks/CODE_OF_CONDUCT.md\n",
      "15K\t/notebooks/CONTRIBUTING.md\n",
      "19K\t/notebooks/ISSUES.md\n",
      "12K\t/notebooks/LICENSE\n",
      "512\t/notebooks/MANIFEST.in\n",
      "3.5K\t/notebooks/Makefile\n",
      "41K\t/notebooks/README.md\n",
      "41K\t/notebooks/README_zh-hans.md\n",
      "42K\t/notebooks/README_zh-hant.md\n",
      "12K\t/notebooks/docker\n",
      "4.6M\t/notebooks/docs\n",
      "5.0M\t/notebooks/examples\n",
      "8.5K\t/notebooks/hubconf.py\n",
      "7.8G\t/notebooks/master-thesis\n",
      "1.5K\t/notebooks/model_cards\n",
      "9.5K\t/notebooks/notebooks\n",
      "512\t/notebooks/pyproject.toml\n",
      "64K\t/notebooks/scripts\n",
      "1.0K\t/notebooks/setup.cfg\n",
      "13K\t/notebooks/setup.py\n",
      "13M\t/notebooks/src\n",
      "731K\t/notebooks/templates\n",
      "4.5K\t/notebooks/test quy-es -> es-en model.ipynb\n",
      "6.8M\t/notebooks/tests\n",
      "147K\t/notebooks/train es-en model.ipynb\n",
      "158K\t/notebooks/train quy-es + es-en model.ipynb\n",
      "160K\t/notebooks/utils\n",
      "3.5K\t/notebooks/valohai.yaml\n",
      "7.9G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\t/notebooks/master-thesis/Untitled.ipynb\n",
      "362M\t/notebooks/master-thesis/corpora\n",
      "7.5G\t/notebooks/master-thesis/models\n",
      "7.8G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6G\t/notebooks/master-thesis/models/es-en\n",
      "802M\t/notebooks/master-thesis/models/quy-es\n",
      "5.2G\t/notebooks/master-thesis/models/quy-es+es-en\n",
      "7.5G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19M\t/apex\n",
      "5.0M\t/bin\n",
      "4.0K\t/boot\n",
      "24K\t/content\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! du -shc /*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CITATION.cff         \u001b[0m\u001b[01;34mdocker\u001b[0m/          setup.py\n",
      " CODE_OF_CONDUCT.md   \u001b[01;34mdocs\u001b[0m/            \u001b[01;34msrc\u001b[0m/\n",
      " CONTRIBUTING.md      \u001b[01;34mexamples\u001b[0m/        \u001b[01;34mtemplates\u001b[0m/\n",
      " ISSUES.md            hubconf.py      'test quy-es -> es-en model.ipynb'\n",
      " LICENSE              \u001b[01;34mmaster-thesis\u001b[0m/   \u001b[01;34mtests\u001b[0m/\n",
      " MANIFEST.in          \u001b[01;34mmodel_cards\u001b[0m/    'train es-en model.ipynb'\n",
      " Makefile             \u001b[01;34mnotebooks\u001b[0m/      'train quy-es + es-en model.ipynb'\n",
      " README.md            pyproject.toml   \u001b[01;34mutils\u001b[0m/\n",
      " README_zh-hans.md    \u001b[01;34mscripts\u001b[0m/         valohai.yaml\n",
      " README_zh-hant.md    setup.cfg\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
