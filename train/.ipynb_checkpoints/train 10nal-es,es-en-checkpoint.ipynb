{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUY_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNI_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/ashaninka-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AYM_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BZD_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/bribri-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GN_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/guarani-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OTO_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/hnahnu-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAH_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAR_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/raramuri-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHP_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/shipibo_konibo-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCH_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/wixarika-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZER_PATH=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZER_PATH = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_path_es_en = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/\"\n",
    "tokenized_path_quy_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/\"\n",
    "tokenized_path_cni_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/\"\n",
    "tokenized_path_aym_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/\"\n",
    "tokenized_path_bzd_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/\"\n",
    "tokenized_path_gn_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/\"\n",
    "tokenized_path_oto_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/\"\n",
    "tokenized_path_nah_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/\"\n",
    "tokenized_path_tar_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/\"\n",
    "tokenized_path_shp_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/\"\n",
    "tokenized_path_hch_es = \"/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZED_PATH_ES_EN=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en\n",
      "env: TOKENIZED_PATH_QUY_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es\n",
      "env: TOKENIZED_PATH_CNI_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es\n",
      "env: TOKENIZED_PATH_AYM_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es\n",
      "env: TOKENIZED_PATH_BZD_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es\n",
      "env: TOKENIZED_PATH_GN_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es\n",
      "env: TOKENIZED_PATH_OTO_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es\n",
      "env: TOKENIZED_PATH_NAH_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es\n",
      "env: TOKENIZED_PATH_TAR_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es\n",
      "env: TOKENIZED_PATH_SHP_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es\n",
      "env: TOKENIZED_PATH_HCH_ES=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZED_PATH_ES_EN = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en\n",
    "%env TOKENIZED_PATH_QUY_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es\n",
    "%env TOKENIZED_PATH_CNI_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es\n",
    "%env TOKENIZED_PATH_AYM_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es\n",
    "%env TOKENIZED_PATH_BZD_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es\n",
    "%env TOKENIZED_PATH_GN_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es\n",
    "%env TOKENIZED_PATH_OTO_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es\n",
    "%env TOKENIZED_PATH_NAH_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es\n",
    "%env TOKENIZED_PATH_TAR_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es\n",
    "%env TOKENIZED_PATH_SHP_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es\n",
    "%env TOKENIZED_PATH_HCH_ES = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BIN_DIR=/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "%env BIN_DIR = /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_DIR=/storage/master-thesis/models/10nal-es+es-en\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_DIR = /storage/master-thesis/models/10nal-es+es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint131.pt  checkpoint_best.pt  \u001b[0m\u001b[01;34mtokenizers\u001b[0m/\n",
      "checkpoint132.pt  checkpoint_last.pt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.10.3 in /usr/local/lib/python3.6/dist-packages (0.10.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers==0.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (0.10.2)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.51.0)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.24)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
      "Requirement already satisfied: hydra-core in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.1.1)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.3)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2020.11.13)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (2.0.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.6.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (5.2.2)\n",
      "Requirement already satisfied: omegaconf==2.1.* in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (2.1.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (4.8)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.4)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.3.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.18.2)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq) (5.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tatoeba format to standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(EN_ES_CORPUS_DIR + \"original/valid.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"dev.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"dev.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "    en.close()\n",
    "    es.close()\n",
    "    \n",
    "with open(EN_ES_CORPUS_DIR + \"original/dev.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"train.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"train.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        if \"\\n\" in line[2] or \"\\n\" in line[3]:\n",
    "            continue\n",
    "        if len(line) != 4:\n",
    "            continue\n",
    "            \n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "        \n",
    "    en.close()\n",
    "    es.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "nal_es_en_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "nal_es_en_trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=30000, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "nal_es_en_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "en_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.en\" for split in [\"dev\", \"train\"]]\n",
    "es_en_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "quy_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/{split}.quy\" for split in [\"dev\", \"dict\", \"train\"]]\n",
    "es_quy_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/{split}.es\" for split in [\"dev\", \"dict\", \"train\"]]\n",
    "\n",
    "cni_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/ashaninka-spanish/{split}.cni\" for split in [\"dev\", \"train\"]]\n",
    "es_cni_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/ashaninka-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "aym_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/{split}.aym\" for split in [\"dev\", \"train\"]]\n",
    "es_aym_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "bzd_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/bribri-spanish/{split}.bzd\" for split in [\"dev\", \"train\"]]\n",
    "es_bzd_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/bribri-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "gn_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/guarani-spanish/{split}.gn\" for split in [\"dev\", \"train\"]]\n",
    "es_gn_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/guarani-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "oto_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/hnahnu-spanish/{split}.oto\" for split in [\"dev\", \"train\"]]\n",
    "es_oto_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/hnahnu-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "nah_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish/{split}.nah\" for split in [\"dev\", \"train\"]]\n",
    "es_nah_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "tar_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/raramuri-spanish/{split}.tar\" for split in [\"dev\", \"train\"]]\n",
    "es_tar_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/raramuri-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "shp_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/shipibo_konibo-spanish/{split}.shp\" for split in [\"dev\", \"train\"]]\n",
    "es_shp_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/shipibo_konibo-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "hch_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/wixarika-spanish/{split}.hch\" for split in [\"dev\", \"train\"]]\n",
    "es_hch_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/wixarika-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "\n",
    "files = en_files + es_en_files + quy_files + es_quy_files + cni_files + es_cni_files + aym_files + es_aym_files + bzd_files + es_bzd_files + gn_files + es_gn_files + oto_files + es_oto_files + nah_files + es_nah_files + tar_files + es_tar_files + shp_files + es_shp_files + hch_files + es_hch_files\n",
    "nal_es_en_tokenizer.train(files= files, trainer=nal_es_en_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_files(tokenizer, files, extension, output_path):\n",
    "    for file in files:\n",
    "        print(f\"Reading file {file}\")\n",
    "        with open(file, encoding='utf8') as f:\n",
    "          lines = f.readlines()\n",
    "          tokenized_lines = tokenizer.encode_batch(lines)\n",
    "          tokenized_name = Path(file).stem\n",
    "          tokenized_name = output_path + tokenized_name + \".\" + extension\n",
    "          print(tokenized_name)\n",
    "          with open(tokenized_name, 'w', encoding='utf8') as wf:\n",
    "\n",
    "            wf.writelines([\" \".join(t.tokens) + \"\\n\" for t in tokenized_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory '/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "mkdir /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en\n"
     ]
    }
   ],
   "source": [
    "mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nal_es_en_tokenizer.save(tokenizer_path + \"nal_es_en_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.Tokenizer object at 0x7f07d401d430>\n"
     ]
    }
   ],
   "source": [
    "print(nal_es_en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'hold', 'this', 'truth', 'to', 'be', 'self', 'evid', '##ent', 'that', 'everyone', 'is', 'created', 'equal', '?']\n"
     ]
    }
   ],
   "source": [
    "tok = nal_es_en_tokenizer.encode(\"we hold this truth to be self evident that everyone is created equal?\")\n",
    "print(tok.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.en\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/dev.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.en\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/train.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.quy\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/dev.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dict.quy\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/dict.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.quy\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/train.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dict.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/dict.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/ashaninka-spanish/dev.cni\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/dev.cni\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/ashaninka-spanish/train.cni\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/train.cni\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/ashaninka-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/ashaninka-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/dev.aym\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/dev.aym\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/train.aym\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/train.aym\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/aymara-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/bribri-spanish/dev.bzd\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/dev.bzd\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/bribri-spanish/train.bzd\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/train.bzd\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/bribri-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/bribri-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/guarani-spanish/dev.gn\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/dev.gn\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/guarani-spanish/train.gn\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/train.gn\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/guarani-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/guarani-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/hnahnu-spanish/dev.oto\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/dev.oto\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/hnahnu-spanish/train.oto\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/train.oto\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/hnahnu-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/hnahnu-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish/dev.nah\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/dev.nah\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish/train.nah\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/train.nah\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/raramuri-spanish/dev.tar\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/dev.tar\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/raramuri-spanish/train.tar\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/train.tar\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/raramuri-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/raramuri-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/shipibo_konibo-spanish/dev.shp\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/dev.shp\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/shipibo_konibo-spanish/train.shp\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/train.shp\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/shipibo_konibo-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/shipibo_konibo-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/wixarika-spanish/dev.hch\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/dev.hch\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/wixarika-spanish/train.hch\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/train.hch\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/wixarika-spanish/dev.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/wixarika-spanish/train.es\n",
      "/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/train.es\n"
     ]
    }
   ],
   "source": [
    "tokenize_files(nal_es_en_tokenizer, en_files, \"en\", tokenized_path_es_en)\n",
    "tokenize_files(nal_es_en_tokenizer, es_en_files, \"es\", tokenized_path_es_en)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, quy_files, \"quy\", tokenized_path_quy_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_quy_files, \"es\", tokenized_path_quy_es)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, cni_files, \"cni\", tokenized_path_cni_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_cni_files, \"es\", tokenized_path_cni_es)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, aym_files, \"aym\", tokenized_path_aym_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_aym_files, \"es\", tokenized_path_aym_es)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, bzd_files, \"bzd\", tokenized_path_bzd_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_bzd_files, \"es\", tokenized_path_bzd_es)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, gn_files, \"gn\", tokenized_path_gn_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_gn_files, \"es\", tokenized_path_gn_es)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, oto_files, \"oto\", tokenized_path_oto_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_oto_files, \"es\", tokenized_path_oto_es)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, nah_files, \"nah\", tokenized_path_nah_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_nah_files, \"es\", tokenized_path_nah_es)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, tar_files, \"tar\", tokenized_path_tar_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_tar_files, \"es\", tokenized_path_tar_es)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, shp_files, \"shp\", tokenized_path_shp_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_shp_files, \"es\", tokenized_path_shp_es)\n",
    "\n",
    "tokenize_files(nal_es_en_tokenizer, hch_files, \"hch\", tokenized_path_hch_es)\n",
    "tokenize_files(nal_es_en_tokenizer, es_hch_files, \"es\", tokenized_path_hch_es)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp ./bin/train.all /notebooks/train.all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.quy.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.es.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.en.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "### Removing previous dict files\n",
    "! rm $BIN_DIR/dict.quy.txt\n",
    "! rm $BIN_DIR/dict.es.txt\n",
    "! rm $BIN_DIR/dict.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenating all training data\n",
    "! cat $TOKENIZED_PATH_QUY_ES/train.quy $TOKENIZED_PATH_QUY_ES/train.es \\\n",
    "    $TOKENIZED_PATH_ES_EN/train.es $TOKENIZED_PATH_ES_EN/train.en \\\n",
    "    $TOKENIZED_PATH_CNI_ES/train.cni $TOKENIZED_PATH_CNI_ES/train.es \\\n",
    "    $TOKENIZED_PATH_AYM_ES/train.aym $TOKENIZED_PATH_AYM_ES/train.es \\\n",
    "    $TOKENIZED_PATH_BZD_ES/train.bzd $TOKENIZED_PATH_BZD_ES/train.es \\\n",
    "    $TOKENIZED_PATH_GN_ES/train.gn $TOKENIZED_PATH_GN_ES/train.es \\\n",
    "    $TOKENIZED_PATH_OTO_ES/train.oto $TOKENIZED_PATH_OTO_ES/train.es \\\n",
    "    $TOKENIZED_PATH_NAH_ES/train.nah $TOKENIZED_PATH_NAH_ES/train.es \\\n",
    "    $TOKENIZED_PATH_TAR_ES/train.tar $TOKENIZED_PATH_TAR_ES/train.es \\\n",
    "    $TOKENIZED_PATH_SHP_ES/train.shp $TOKENIZED_PATH_SHP_ES/train.es \\\n",
    "    $TOKENIZED_PATH_HCH_ES/train.hch $TOKENIZED_PATH_HCH_ES/train.es \\\n",
    "    > $BIN_DIR/train.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:21:46 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='all', srcdict=None, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train', user_dir=None, validpref=None, workers=20)\n",
      "2021-10-09 20:21:55 | INFO | fairseq_cli.preprocess | [all] Dictionary: 29288 types\n",
      "2021-10-09 20:22:17 | INFO | fairseq_cli.preprocess | [all] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.all: 851142 sents, 13156484 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:22:17 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang all \\\n",
    "    --trainpref $BIN_DIR/train \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --workers 20 \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --only-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:25:22 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='quy', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/dev', workers=20)\n",
      "2021-10-09 20:25:23 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 29288 types\n",
      "2021-10-09 20:25:26 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/train.quy: 125008 sents, 2054979 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:25:26 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 29288 types\n",
      "2021-10-09 20:25:26 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/dev.quy: 996 sents, 14950 tokens, 0.522% replaced by <unk>\n",
      "2021-10-09 20:25:26 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:25:30 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/train.es: 125008 sents, 2495001 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:25:31 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:25:31 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/quy-es/dev.es: 996 sents, 14827 tokens, 0.567% replaced by <unk>\n",
      "2021-10-09 20:25:31 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang quy --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_QUY_ES/train --validpref $TOKENIZED_PATH_QUY_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing es dict from quy-es preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:25:59 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='es', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='en', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/dev', workers=20)\n",
      "2021-10-09 20:25:59 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:26:03 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/train.es: 197297 sents, 1919429 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:26:03 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:26:03 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/dev.es: 4643 sents, 52469 tokens, 0.0286% replaced by <unk>\n",
      "2021-10-09 20:26:03 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29288 types\n",
      "2021-10-09 20:26:07 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/train.en: 197297 sents, 2052515 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:26:07 | INFO | fairseq_cli.preprocess | [en] Dictionary: 29288 types\n",
      "2021-10-09 20:26:07 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/es-en/dev.en: 4643 sents, 56132 tokens, 0.0285% replaced by <unk>\n",
      "2021-10-09 20:26:07 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang es --target-lang en \\\n",
    "    --trainpref $TOKENIZED_PATH_ES_EN/train --validpref $TOKENIZED_PATH_ES_EN/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:35:50 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='cni', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/dev', workers=20)\n",
      "2021-10-09 20:35:50 | INFO | fairseq_cli.preprocess | [cni] Dictionary: 29288 types\n",
      "2021-10-09 20:35:51 | INFO | fairseq_cli.preprocess | [cni] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/train.cni: 3883 sents, 83922 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:35:51 | INFO | fairseq_cli.preprocess | [cni] Dictionary: 29288 types\n",
      "2021-10-09 20:35:51 | INFO | fairseq_cli.preprocess | [cni] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/dev.cni: 883 sents, 18507 tokens, 0.297% replaced by <unk>\n",
      "2021-10-09 20:35:51 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:35:52 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/train.es: 3883 sents, 73219 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:35:52 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:35:52 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/cni-es/dev.es: 883 sents, 12871 tokens, 0.443% replaced by <unk>\n",
      "2021-10-09 20:35:52 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang cni --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_CNI_ES/train --validpref $TOKENIZED_PATH_CNI_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:35:54 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='aym', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/dev', workers=20)\n",
      "2021-10-09 20:35:54 | INFO | fairseq_cli.preprocess | [aym] Dictionary: 29288 types\n",
      "2021-10-09 20:35:55 | INFO | fairseq_cli.preprocess | [aym] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/train.aym: 6531 sents, 215301 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:35:55 | INFO | fairseq_cli.preprocess | [aym] Dictionary: 29288 types\n",
      "2021-10-09 20:35:56 | INFO | fairseq_cli.preprocess | [aym] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/dev.aym: 996 sents, 19533 tokens, 0.425% replaced by <unk>\n",
      "2021-10-09 20:35:56 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:35:56 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/train.es: 6531 sents, 188008 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:35:56 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:35:57 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/aym-es/dev.es: 996 sents, 14827 tokens, 0.567% replaced by <unk>\n",
      "2021-10-09 20:35:57 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang aym --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_AYM_ES/train --validpref $TOKENIZED_PATH_AYM_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:35:58 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='bzd', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/dev', workers=20)\n",
      "2021-10-09 20:35:59 | INFO | fairseq_cli.preprocess | [bzd] Dictionary: 29288 types\n",
      "2021-10-09 20:35:59 | INFO | fairseq_cli.preprocess | [bzd] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/train.bzd: 7508 sents, 96717 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:35:59 | INFO | fairseq_cli.preprocess | [bzd] Dictionary: 29288 types\n",
      "2021-10-09 20:36:00 | INFO | fairseq_cli.preprocess | [bzd] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/dev.bzd: 996 sents, 25240 tokens, 0.329% replaced by <unk>\n",
      "2021-10-09 20:36:00 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:00 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/train.es: 7508 sents, 68704 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:00 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:00 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bzd-es/dev.es: 996 sents, 14827 tokens, 0.567% replaced by <unk>\n",
      "2021-10-09 20:36:00 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang bzd --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_BZD_ES/train --validpref $TOKENIZED_PATH_BZD_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:36:02 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='gn', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/dev', workers=20)\n",
      "2021-10-09 20:36:02 | INFO | fairseq_cli.preprocess | [gn] Dictionary: 29288 types\n",
      "2021-10-09 20:36:04 | INFO | fairseq_cli.preprocess | [gn] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/train.gn: 26032 sents, 774981 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:04 | INFO | fairseq_cli.preprocess | [gn] Dictionary: 29288 types\n",
      "2021-10-09 20:36:04 | INFO | fairseq_cli.preprocess | [gn] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/dev.gn: 995 sents, 18741 tokens, 0.416% replaced by <unk>\n",
      "2021-10-09 20:36:04 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:05 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/train.es: 26032 sents, 830976 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:05 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:06 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/gn-es/dev.es: 995 sents, 14826 tokens, 0.567% replaced by <unk>\n",
      "2021-10-09 20:36:06 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang gn --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_GN_ES/train --validpref $TOKENIZED_PATH_GN_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:36:07 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='oto', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/dev', workers=20)\n",
      "2021-10-09 20:36:08 | INFO | fairseq_cli.preprocess | [oto] Dictionary: 29288 types\n",
      "2021-10-09 20:36:08 | INFO | fairseq_cli.preprocess | [oto] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/train.oto: 4889 sents, 133718 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:08 | INFO | fairseq_cli.preprocess | [oto] Dictionary: 29288 types\n",
      "2021-10-09 20:36:09 | INFO | fairseq_cli.preprocess | [oto] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/dev.oto: 599 sents, 11994 tokens, 9.05% replaced by <unk>\n",
      "2021-10-09 20:36:09 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:09 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/train.es: 4889 sents, 86293 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:09 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:10 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/oto-es/dev.es: 599 sents, 6962 tokens, 0.402% replaced by <unk>\n",
      "2021-10-09 20:36:10 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang oto --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_OTO_ES/train --validpref $TOKENIZED_PATH_OTO_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:36:12 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='nah', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/dev', workers=20)\n",
      "2021-10-09 20:36:12 | INFO | fairseq_cli.preprocess | [nah] Dictionary: 29288 types\n",
      "2021-10-09 20:36:13 | INFO | fairseq_cli.preprocess | [nah] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/train.nah: 16145 sents, 603223 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:13 | INFO | fairseq_cli.preprocess | [nah] Dictionary: 29288 types\n",
      "2021-10-09 20:36:14 | INFO | fairseq_cli.preprocess | [nah] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/dev.nah: 671 sents, 10153 tokens, 0.256% replaced by <unk>\n",
      "2021-10-09 20:36:14 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:15 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/train.es: 16145 sents, 567769 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:15 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:16 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/nah-es/dev.es: 671 sents, 8538 tokens, 0.363% replaced by <unk>\n",
      "2021-10-09 20:36:16 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang nah --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_NAH_ES/train --validpref $TOKENIZED_PATH_NAH_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:36:17 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='tar', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/dev', workers=20)\n",
      "2021-10-09 20:36:18 | INFO | fairseq_cli.preprocess | [tar] Dictionary: 29288 types\n",
      "2021-10-09 20:36:18 | INFO | fairseq_cli.preprocess | [tar] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/train.tar: 14720 sents, 185128 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:18 | INFO | fairseq_cli.preprocess | [tar] Dictionary: 29288 types\n",
      "2021-10-09 20:36:19 | INFO | fairseq_cli.preprocess | [tar] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/dev.tar: 995 sents, 23243 tokens, 3.48% replaced by <unk>\n",
      "2021-10-09 20:36:19 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:20 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/train.es: 14720 sents, 182795 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:20 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:21 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/tar-es/dev.es: 995 sents, 14826 tokens, 0.567% replaced by <unk>\n",
      "2021-10-09 20:36:21 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang tar --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_TAR_ES/train --validpref $TOKENIZED_PATH_TAR_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:36:22 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='shp', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/dev', workers=20)\n",
      "2021-10-09 20:36:23 | INFO | fairseq_cli.preprocess | [shp] Dictionary: 29288 types\n",
      "2021-10-09 20:36:23 | INFO | fairseq_cli.preprocess | [shp] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/train.shp: 14592 sents, 157244 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:23 | INFO | fairseq_cli.preprocess | [shp] Dictionary: 29288 types\n",
      "2021-10-09 20:36:24 | INFO | fairseq_cli.preprocess | [shp] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/dev.shp: 996 sents, 18230 tokens, 0.477% replaced by <unk>\n",
      "2021-10-09 20:36:24 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:24 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/train.es: 14592 sents, 140972 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:24 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:24 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/shp-es/dev.es: 996 sents, 14827 tokens, 0.567% replaced by <unk>\n",
      "2021-10-09 20:36:24 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang shp --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_SHP_ES/train --validpref $TOKENIZED_PATH_SHP_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-09 20:36:26 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='hch', srcdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', target_lang='es', task='translation_multi_simple_epoch', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/train', user_dir=None, validpref='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/dev', workers=20)\n",
      "2021-10-09 20:36:26 | INFO | fairseq_cli.preprocess | [hch] Dictionary: 29288 types\n",
      "2021-10-09 20:36:27 | INFO | fairseq_cli.preprocess | [hch] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/train.hch: 8966 sents, 146253 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:27 | INFO | fairseq_cli.preprocess | [hch] Dictionary: 29288 types\n",
      "2021-10-09 20:36:27 | INFO | fairseq_cli.preprocess | [hch] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/dev.hch: 994 sents, 24775 tokens, 0.307% replaced by <unk>\n",
      "2021-10-09 20:36:27 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:28 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/train.es: 8966 sents, 99337 tokens, 0.0% replaced by <unk>\n",
      "2021-10-09 20:36:28 | INFO | fairseq_cli.preprocess | [es] Dictionary: 29288 types\n",
      "2021-10-09 20:36:28 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/hch-es/dev.es: 994 sents, 14825 tokens, 0.567% replaced by <unk>\n",
      "2021-10-09 20:36:28 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang hch --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_HCH_ES/train --validpref $TOKENIZED_PATH_HCH_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --task translation_multi_simple_epoch \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-10 23:53:29 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=2, label_smoothing=0.1, lang_dict=None, lang_pairs='quy-es,es-en,cni-es,aym-es,bzd-es,gn-es,oto-es,nah-es,tar-es,shp-es,hch-es', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='/storage/master-thesis/models/10nal-es+es-en/checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='/storage/master-thesis/models/10nal-es+es-en/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')\n",
      "2021-10-10 23:53:29 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['aym', 'bzd', 'cni', 'en', 'es', 'gn', 'hch', 'nah', 'oto', 'quy', 'shp', 'tar']\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [aym] dictionary: 29300 types\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [bzd] dictionary: 29300 types\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [cni] dictionary: 29300 types\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 29300 types\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 29300 types\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [gn] dictionary: 29300 types\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [hch] dictionary: 29300 types\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [nah] dictionary: 29300 types\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [oto] dictionary: 29300 types\n",
      "2021-10-10 23:53:29 | INFO | fairseq.data.multilingual.multilingual_data_manager | [quy] dictionary: 29300 types\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | [shp] dictionary: 29300 types\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | [tar] dictionary: 29300 types\n",
      "2021-10-10 23:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n",
      "2021-10-10 23:53:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:quy-es': 1, 'main:es-en': 1, 'main:cni-es': 1, 'main:aym-es': 1, 'main:bzd-es': 1, 'main:gn-es': 1, 'main:oto-es': 1, 'main:nah-es': 1, 'main:tar-es': 1, 'main:shp-es': 1, 'main:hch-es': 1}\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 29297; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.quy-es.quy\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.quy-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid quy-es 996 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 29292; tgt_langtok: 29291\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.es-en.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.es-en.en\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid es-en 4643 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:cni-es src_langtok: 29290; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 883 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.cni-es.cni\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 883 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.cni-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid cni-es 883 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aym-es src_langtok: 29288; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.aym-es.aym\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.aym-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid aym-es 996 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bzd-es src_langtok: 29289; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.bzd-es.bzd\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.bzd-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid bzd-es 996 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:gn-es src_langtok: 29293; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 995 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.gn-es.gn\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 995 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.gn-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid gn-es 995 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:oto-es src_langtok: 29296; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 599 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.oto-es.oto\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 599 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.oto-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid oto-es 599 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:nah-es src_langtok: 29295; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 671 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.nah-es.nah\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 671 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.nah-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid nah-es 671 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:tar-es src_langtok: 29299; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 995 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.tar-es.tar\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 995 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.tar-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid tar-es 995 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:shp-es src_langtok: 29298; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.shp-es.shp\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.shp-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid shp-es 996 examples\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hch-es src_langtok: 29294; tgt_langtok: 29292\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 994 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.hch-es.hch\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.data_utils | loaded 994 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/valid.hch-es.es\n",
      "2021-10-10 23:53:30 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin valid hch-es 994 examples\n",
      "2021-10-10 23:53:31 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(29300, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(29300, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=29300, bias=False)\n",
      "  )\n",
      ")\n",
      "2021-10-10 23:53:31 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)\n",
      "2021-10-10 23:53:31 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
      "2021-10-10 23:53:31 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
      "2021-10-10 23:53:31 | INFO | fairseq_cli.train | num. model params: 59140096 (num. trained: 59140096)\n",
      "2021-10-10 23:53:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
      "2021-10-10 23:53:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2021-10-10 23:53:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-10-10 23:53:37 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 7.795 GB ; name = Quadro RTX 4000                         \n",
      "2021-10-10 23:53:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-10-10 23:53:37 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2021-10-10 23:53:37 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None\n",
      "2021-10-10 23:53:40 | INFO | fairseq.trainer | loaded checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint_last.pt (epoch 156 @ 0 updates)\n",
      "2021-10-10 23:53:40 | INFO | fairseq.optim.adam | using FusedAdam\n",
      "2021-10-10 23:53:40 | INFO | fairseq.trainer | loading train data for epoch 156\n",
      "2021-10-10 23:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=156/None\n",
      "2021-10-10 23:53:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:quy-es': 1, 'main:es-en': 1, 'main:cni-es': 1, 'main:aym-es': 1, 'main:bzd-es': 1, 'main:gn-es': 1, 'main:oto-es': 1, 'main:nah-es': 1, 'main:tar-es': 1, 'main:shp-es': 1, 'main:hch-es': 1}\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 29297; tgt_langtok: 29292\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 125008 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.quy-es.quy\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 125008 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.quy-es.es\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train quy-es 125008 examples\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 29292; tgt_langtok: 29291\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.es-en.es\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.es-en.en\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train es-en 197297 examples\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:cni-es src_langtok: 29290; tgt_langtok: 29292\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 3883 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.cni-es.cni\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 3883 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.cni-es.es\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train cni-es 3883 examples\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:aym-es src_langtok: 29288; tgt_langtok: 29292\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 6531 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.aym-es.aym\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 6531 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.aym-es.es\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train aym-es 6531 examples\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:bzd-es src_langtok: 29289; tgt_langtok: 29292\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 7508 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.bzd-es.bzd\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 7508 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.bzd-es.es\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train bzd-es 7508 examples\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:gn-es src_langtok: 29293; tgt_langtok: 29292\n",
      "2021-10-10 23:53:40 | INFO | fairseq.data.data_utils | loaded 26032 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.gn-es.gn\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 26032 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.gn-es.es\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train gn-es 26032 examples\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:oto-es src_langtok: 29296; tgt_langtok: 29292\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 4889 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.oto-es.oto\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 4889 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.oto-es.es\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train oto-es 4889 examples\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:nah-es src_langtok: 29295; tgt_langtok: 29292\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 16145 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.nah-es.nah\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 16145 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.nah-es.es\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train nah-es 16145 examples\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:tar-es src_langtok: 29299; tgt_langtok: 29292\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 14720 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.tar-es.tar\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 14720 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.tar-es.es\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train tar-es 14720 examples\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:shp-es src_langtok: 29298; tgt_langtok: 29292\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 14592 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.shp-es.shp\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 14592 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.shp-es.es\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train shp-es 14592 examples\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:hch-es src_langtok: 29294; tgt_langtok: 29292\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 8966 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.hch-es.hch\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.data_utils | loaded 8966 examples from: /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin/train.hch-es.es\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/10nal-es+es-en/tokenizers/10nal-es-en/bin train hch-es 8966 examples\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:quy-es', 125008), ('main:es-en', 197297), ('main:cni-es', 3883), ('main:aym-es', 6531), ('main:bzd-es', 7508), ('main:gn-es', 26032), ('main:oto-es', 4889), ('main:nah-es', 16145), ('main:tar-es', 14720), ('main:shp-es', 14592), ('main:hch-es', 8966)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat\n",
      "2021-10-10 23:53:41 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 425571\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 425571; virtual dataset size 425571\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=156/shard_epoch=1\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:quy-es': 125008, 'main:es-en': 197297, 'main:cni-es': 3883, 'main:aym-es': 6531, 'main:bzd-es': 7508, 'main:gn-es': 26032, 'main:oto-es': 4889, 'main:nah-es': 16145, 'main:tar-es': 14720, 'main:shp-es': 14592, 'main:hch-es': 8966}; raw total size: 425571\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:quy-es': 125008, 'main:es-en': 197297, 'main:cni-es': 3883, 'main:aym-es': 6531, 'main:bzd-es': 7508, 'main:gn-es': 26032, 'main:oto-es': 4889, 'main:nah-es': 16145, 'main:tar-es': 14720, 'main:shp-es': 14592, 'main:hch-es': 8966}; resampled total size: 425571\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.050147\n",
      "2021-10-10 23:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-10 23:53:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008932\n",
      "2021-10-10 23:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.151668\n",
      "2021-10-10 23:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:53:41 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[51562, 326420, 64365, 352237, 361837, 127319, 385559, 335451, 414975, 314861]\n",
      "2021-10-10 23:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012706\n",
      "2021-10-10 23:53:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.213876\n",
      "2021-10-10 23:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.379095\n",
      "2021-10-10 23:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-10 23:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117848\n",
      "2021-10-10 23:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:53:42 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[51562, 326420, 64365, 352237, 361837, 127319, 385559, 335451, 414975, 314861]\n",
      "2021-10-10 23:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012961\n",
      "2021-10-10 23:53:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.155505\n",
      "2021-10-10 23:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.287176\n",
      "2021-10-10 23:53:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 156:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-10 23:53:43 | INFO | fairseq.trainer | begin training epoch 156\n",
      "epoch 156:   0%|                               | 1/2037 [00:00<14:10,  2.39it/s]2021-10-10 23:53:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
      "epoch 156:   1%|2                             | 14/2037 [00:02<04:33,  7.39it/s]2021-10-10 23:53:46 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
      "epoch 156:  21%|2| 431/2037 [01:04<04:00,  6.68it/s, loss=3.674, nll_loss=1.943,2021-10-10 23:54:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 156:  54%|5| 1099/2037 [02:46<02:20,  6.68it/s, loss=3.695, nll_loss=1.9652021-10-10 23:56:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n",
      "epoch 156: 100%|9| 2036/2037 [05:11<00:00,  6.23it/s, loss=3.975, nll_loss=2.2882021-10-10 23:58:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003698\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.162938\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103458\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.270995\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002957\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.159892\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102041\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265770\n",
      "2021-10-10 23:58:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 156 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.02it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.90it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.64it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.26it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.55it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.65it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.48it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 17.35it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.72it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.17it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  27%|#6    | 22/81 [00:01<00:03, 18.58it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.81it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.84it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.28it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.16it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 18.99it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.09it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.27it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.07it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.21it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.31it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.65it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.74it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.66it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.66it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.79it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.62it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.66it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.70it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.64it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.67it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.36it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  91%|#####4| 74/81 [00:03<00:00, 19.48it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  95%|#####7| 77/81 [00:04<00:00, 19.56it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-10 23:59:00 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 7.043 | nll_loss 5.68 | ppl 51.26 | wps 48972.6 | wpb 2494.5 | bsz 169.9 | num_updates 2033\n",
      "2021-10-10 23:59:00 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-10 23:59:04 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint156.pt (epoch 156 @ 2033 updates, score 7.043) (writing took 4.544387959999995 seconds)\n",
      "2021-10-10 23:59:04 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)\n",
      "2021-10-10 23:59:04 | INFO | train | epoch 156 | loss 3.786 | nll_loss 2.071 | ppl 4.2 | wps 22166.5 | ups 6.3 | wpb 3520.6 | bsz 208.9 | num_updates 2033 | lr 0.000254125 | gnorm 2.243 | loss_scale 8 | train_wall 610 | wall 0\n",
      "2021-10-10 23:59:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=157/shard_epoch=1\n",
      "2021-10-10 23:59:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=157/shard_epoch=2\n",
      "2021-10-10 23:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-10 23:59:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006622\n",
      "2021-10-10 23:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.143116\n",
      "2021-10-10 23:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:59:04 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[123607, 259391, 162238, 9869, 193614, 55327, 400403, 362523, 46594, 353825]\n",
      "2021-10-10 23:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.014814\n",
      "2021-10-10 23:59:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.209791\n",
      "2021-10-10 23:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.368623\n",
      "2021-10-10 23:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-10 23:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.121623\n",
      "2021-10-10 23:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:59:06 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[123607, 259391, 162238, 9869, 193614, 55327, 400403, 362523, 46594, 353825]\n",
      "2021-10-10 23:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017515\n",
      "2021-10-10 23:59:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-10 23:59:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.236824\n",
      "2021-10-10 23:59:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.376929\n",
      "2021-10-10 23:59:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 157:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-10 23:59:07 | INFO | fairseq.trainer | begin training epoch 157\n",
      "epoch 157:  35%|3| 706/2037 [01:49<03:22,  6.58it/s, loss=4.005, nll_loss=2.317,2021-10-11 00:00:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0\n",
      "epoch 157: 100%|9| 2036/2037 [05:15<00:00,  6.66it/s, loss=4.462, nll_loss=2.85,2021-10-11 00:04:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003289\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157723\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105112\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.267151\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002884\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156460\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103441\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263734\n",
      "2021-10-11 00:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 157 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:08,  9.05it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.94it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:05, 12.60it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 14.12it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 15.26it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 16.24it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:03, 16.96it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.66it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.20it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.36it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.70it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.95it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.00it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.25it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.15it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.29it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 18.84it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.12it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 18.83it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.02it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.09it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.38it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.46it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.42it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 19.53it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.34it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.44it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.26it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.36it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.34it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.27it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.31it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.35it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.09it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 18.99it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.28it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.75it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.15it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:04:28 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 6.803 | nll_loss 5.435 | ppl 43.25 | wps 48387.7 | wpb 2494.5 | bsz 169.9 | num_updates 4069 | best_loss 6.803\n",
      "2021-10-11 00:04:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:04:32 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint157.pt (epoch 157 @ 4069 updates, score 6.803) (writing took 4.508232876999955 seconds)\n",
      "2021-10-11 00:04:32 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)\n",
      "2021-10-11 00:04:32 | INFO | train | epoch 157 | loss 4.229 | nll_loss 2.575 | ppl 5.96 | wps 21857.5 | ups 6.21 | wpb 3520.6 | bsz 209 | num_updates 4069 | lr 0.000495743 | gnorm 2.62 | loss_scale 4 | train_wall 307 | wall 0\n",
      "2021-10-11 00:04:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=158/shard_epoch=2\n",
      "2021-10-11 00:04:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=158/shard_epoch=3\n",
      "2021-10-11 00:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:04:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006875\n",
      "2021-10-11 00:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.145254\n",
      "2021-10-11 00:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:32 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[351270, 300176, 421491, 284026, 381350, 109404, 65319, 388306, 146004, 71644]\n",
      "2021-10-11 00:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012147\n",
      "2021-10-11 00:04:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.216812\n",
      "2021-10-11 00:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.375214\n",
      "2021-10-11 00:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117209\n",
      "2021-10-11 00:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:34 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[351270, 300176, 421491, 284026, 381350, 109404, 65319, 388306, 146004, 71644]\n",
      "2021-10-11 00:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013362\n",
      "2021-10-11 00:04:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:04:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.226559\n",
      "2021-10-11 00:04:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.358143\n",
      "2021-10-11 00:04:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 158:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:04:35 | INFO | fairseq.trainer | begin training epoch 158\n",
      "epoch 158: 100%|9| 2036/2037 [05:16<00:00,  6.28it/s, loss=4.415, nll_loss=2.7942021-10-11 00:09:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004190\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.168710\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.106666\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.280617\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003171\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.162891\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.108760\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.275846\n",
      "2021-10-11 00:09:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 158 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.63it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.47it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 12.14it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.70it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.87it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.85it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.57it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 17.40it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.80it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.19it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  27%|#6    | 22/81 [00:01<00:03, 18.62it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.77it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.79it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  35%|##    | 28/81 [00:01<00:02, 19.05it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.15it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.04it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.21it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 18.83it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.09it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 18.79it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 18.97it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.06it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.36it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.43it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.37it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 19.45it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.26it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 19.34it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.18it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.25it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.28it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.24it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.34it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.43it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.16it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.00it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 19.29it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  95%|#####7| 77/81 [00:04<00:00, 19.12it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 18.97it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:09:57 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 6.706 | nll_loss 5.338 | ppl 40.46 | wps 48096.4 | wpb 2494.5 | bsz 169.9 | num_updates 6106 | best_loss 6.706\n",
      "2021-10-11 00:09:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:10:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint158.pt (epoch 158 @ 6106 updates, score 6.706) (writing took 5.072175220999952 seconds)\n",
      "2021-10-11 00:10:02 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)\n",
      "2021-10-11 00:10:02 | INFO | train | epoch 158 | loss 4.441 | nll_loss 2.819 | ppl 7.06 | wps 21751.3 | ups 6.18 | wpb 3520.3 | bsz 208.9 | num_updates 6106 | lr 0.000404689 | gnorm 2.488 | loss_scale 4 | train_wall 308 | wall 0\n",
      "2021-10-11 00:10:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=159/shard_epoch=3\n",
      "2021-10-11 00:10:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=159/shard_epoch=4\n",
      "2021-10-11 00:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:10:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006903\n",
      "2021-10-11 00:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.148310\n",
      "2021-10-11 00:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:10:02 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[280288, 396568, 96052, 282957, 287795, 71971, 352765, 113206, 229273, 151033]\n",
      "2021-10-11 00:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013671\n",
      "2021-10-11 00:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.285473\n",
      "2021-10-11 00:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.448363\n",
      "2021-10-11 00:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116157\n",
      "2021-10-11 00:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:10:03 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[280288, 396568, 96052, 282957, 287795, 71971, 352765, 113206, 229273, 151033]\n",
      "2021-10-11 00:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012660\n",
      "2021-10-11 00:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.380099\n",
      "2021-10-11 00:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.509831\n",
      "2021-10-11 00:10:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 159:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:10:05 | INFO | fairseq.trainer | begin training epoch 159\n",
      "epoch 159:  61%|6| 1252/2037 [03:15<02:04,  6.31it/s, loss=4.26, nll_loss=2.612,2021-10-11 00:13:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0\n",
      "epoch 159:  79%|7| 1619/2037 [04:11<01:06,  6.27it/s, loss=4.379, nll_loss=2.7472021-10-11 00:14:17 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 1.0\n",
      "epoch 159:  95%|9| 1926/2037 [04:59<00:18,  6.16it/s, loss=4.348, nll_loss=2.7132021-10-11 00:15:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.5\n",
      "epoch 159: 100%|9| 2036/2037 [05:16<00:00,  6.78it/s, loss=4.378, nll_loss=2.7492021-10-11 00:15:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:15:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:15:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003130\n",
      "2021-10-11 00:15:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157152\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103852\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265121\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003032\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155459\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103307\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262789\n",
      "2021-10-11 00:15:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 159 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.72it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.55it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.26it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 12.91it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.25it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.42it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.31it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 17.26it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.70it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.13it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  27%|#6    | 22/81 [00:01<00:03, 18.62it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.77it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.81it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.23it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.03it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 18.78it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 18.86it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  46%|##7   | 37/81 [00:01<00:02, 19.06it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 18.93it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 18.76it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:02, 18.85it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 18.91it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.37it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.29it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.16it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.16it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  69%|####1 | 56/81 [00:02<00:01, 19.29it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.28it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.05it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.21it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.21it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.26it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.28it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.00it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 18.84it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.14it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.67it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:15:26 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 6.688 | nll_loss 5.302 | ppl 39.45 | wps 48153.6 | wpb 2494.5 | bsz 169.9 | num_updates 8140 | best_loss 6.688\n",
      "2021-10-11 00:15:26 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:15:32 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint159.pt (epoch 159 @ 8140 updates, score 6.688) (writing took 5.572285316000034 seconds)\n",
      "2021-10-11 00:15:32 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)\n",
      "2021-10-11 00:15:32 | INFO | train | epoch 159 | loss 4.294 | nll_loss 2.649 | ppl 6.27 | wps 21697.6 | ups 6.16 | wpb 3520.4 | bsz 208.9 | num_updates 8140 | lr 0.0003505 | gnorm 2.149 | loss_scale 0.5 | train_wall 307 | wall 0\n",
      "2021-10-11 00:15:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=160/shard_epoch=4\n",
      "2021-10-11 00:15:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=160/shard_epoch=5\n",
      "2021-10-11 00:15:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:15:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007140\n",
      "2021-10-11 00:15:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.151554\n",
      "2021-10-11 00:15:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:32 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[129630, 274901, 360607, 360155, 142089, 70105, 104127, 284757, 151093, 397858]\n",
      "2021-10-11 00:15:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.014124\n",
      "2021-10-11 00:15:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.391056\n",
      "2021-10-11 00:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.557767\n",
      "2021-10-11 00:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.122896\n",
      "2021-10-11 00:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:34 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[129630, 274901, 360607, 360155, 142089, 70105, 104127, 284757, 151093, 397858]\n",
      "2021-10-11 00:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013678\n",
      "2021-10-11 00:15:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:15:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.587842\n",
      "2021-10-11 00:15:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.725431\n",
      "2021-10-11 00:15:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 160:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:15:35 | INFO | fairseq.trainer | begin training epoch 160\n",
      "epoch 160:   7%| | 150/2037 [00:23<04:56,  6.36it/s, loss=4.138, nll_loss=2.472,2021-10-11 00:15:59 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.25\n",
      "epoch 160:  16%|1| 316/2037 [00:49<04:33,  6.30it/s, loss=4.124, nll_loss=2.452,2021-10-11 00:16:25 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.125\n",
      "epoch 160:  22%|2| 448/2037 [01:09<04:04,  6.51it/s, loss=4.155, nll_loss=2.485,2021-10-11 00:16:45 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0625\n",
      "epoch 160:  35%|3| 715/2037 [01:51<03:25,  6.43it/s, loss=4.166, nll_loss=2.497,2021-10-11 00:17:26 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.03125\n",
      "epoch 160:  45%|4| 918/2037 [02:22<02:57,  6.29it/s, loss=4.167, nll_loss=2.501,2021-10-11 00:17:58 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.015625\n",
      "epoch 160: 100%|9| 2036/2037 [05:17<00:00,  6.57it/s, loss=4.342, nll_loss=2.6482021-10-11 00:20:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004174\n",
      "2021-10-11 00:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.173512\n",
      "2021-10-11 00:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105001\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.283843\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003469\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.178692\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.128042\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.311351\n",
      "2021-10-11 00:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 160 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.77it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.52it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.23it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 12.82it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 14.09it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.19it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.12it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.99it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 17.48it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.98it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  27%|#6    | 22/81 [00:01<00:03, 18.48it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.67it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.69it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.12it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 18.89it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 18.62it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 18.58it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  46%|##7   | 37/81 [00:02<00:02, 18.82it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 18.63it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 18.55it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:02, 18.48it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 18.46it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.05it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 18.98it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 18.96it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.10it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  69%|####1 | 56/81 [00:03<00:01, 19.17it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.21it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.00it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.10it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 18.99it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.19it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.28it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.07it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 18.93it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.22it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.73it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.12it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:20:58 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 6.92 | nll_loss 5.46 | ppl 44 | wps 47676 | wpb 2494.5 | bsz 169.9 | num_updates 10172 | best_loss 6.688\n",
      "2021-10-11 00:20:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:21:03 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint160.pt (epoch 160 @ 10172 updates, score 6.92) (writing took 5.102139115999762 seconds)\n",
      "2021-10-11 00:21:03 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)\n",
      "2021-10-11 00:21:03 | INFO | train | epoch 160 | loss 4.287 | nll_loss 2.609 | ppl 6.1 | wps 21577.4 | ups 6.13 | wpb 3520.4 | bsz 208.8 | num_updates 10172 | lr 0.000313543 | gnorm 2.645 | loss_scale 0.0156 | train_wall 308 | wall 0\n",
      "2021-10-11 00:21:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=161/shard_epoch=5\n",
      "2021-10-11 00:21:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=161/shard_epoch=6\n",
      "2021-10-11 00:21:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:21:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007620\n",
      "2021-10-11 00:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.176512\n",
      "2021-10-11 00:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:21:04 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[98404, 4554, 189472, 98146, 237067, 308590, 313021, 395398, 260331, 290453]\n",
      "2021-10-11 00:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.016631\n",
      "2021-10-11 00:21:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.370826\n",
      "2021-10-11 00:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.565060\n",
      "2021-10-11 00:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.127686\n",
      "2021-10-11 00:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:21:05 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[98404, 4554, 189472, 98146, 237067, 308590, 313021, 395398, 260331, 290453]\n",
      "2021-10-11 00:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.014578\n",
      "2021-10-11 00:21:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.315649\n",
      "2021-10-11 00:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.458923\n",
      "2021-10-11 00:21:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 161:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:21:06 | INFO | fairseq.trainer | begin training epoch 161\n",
      "epoch 161: 100%|9| 2036/2037 [05:18<00:00,  6.62it/s, loss=4.197, nll_loss=2.5092021-10-11 00:26:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004551\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.167512\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.110730\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.283946\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002980\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.167400\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105488\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.277039\n",
      "2021-10-11 00:26:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 161 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.00it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  8.77it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:07, 10.49it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:06, 12.16it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.45it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.66it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.65it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.67it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 17.16it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.68it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  27%|#6    | 22/81 [00:01<00:03, 18.27it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.48it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.47it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  35%|##    | 28/81 [00:01<00:02, 18.44it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 18.65it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 18.63it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 18.78it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 18.34it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 18.59it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 18.36it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 18.40it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 18.51it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  57%|###4  | 46/81 [00:02<00:01, 18.89it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.16it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.10it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.11it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.18it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  69%|####1 | 56/81 [00:03<00:01, 19.31it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.30it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.04it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.27it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.21it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.30it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.33it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.13it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.09it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.30it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.79it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:26:30 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 6.864 | nll_loss 5.459 | ppl 43.99 | wps 47674 | wpb 2494.5 | bsz 169.9 | num_updates 12209 | best_loss 6.688\n",
      "2021-10-11 00:26:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:26:40 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint161.pt (epoch 161 @ 12209 updates, score 6.864) (writing took 10.537513152999963 seconds)\n",
      "2021-10-11 00:26:41 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)\n",
      "2021-10-11 00:26:41 | INFO | train | epoch 161 | loss 4.251 | nll_loss 2.555 | ppl 5.88 | wps 21227.4 | ups 6.03 | wpb 3520.3 | bsz 208.9 | num_updates 12209 | lr 0.000286194 | gnorm 3.607 | loss_scale 0.0156 | train_wall 308 | wall 0\n",
      "2021-10-11 00:26:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=162/shard_epoch=6\n",
      "2021-10-11 00:26:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=162/shard_epoch=7\n",
      "2021-10-11 00:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:26:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008559\n",
      "2021-10-11 00:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.160024\n",
      "2021-10-11 00:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:41 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[324379, 241164, 386265, 19387, 412015, 63616, 251691, 292713, 258793, 80431]\n",
      "2021-10-11 00:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.014926\n",
      "2021-10-11 00:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.421281\n",
      "2021-10-11 00:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.597181\n",
      "2021-10-11 00:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115350\n",
      "2021-10-11 00:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:43 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[324379, 241164, 386265, 19387, 412015, 63616, 251691, 292713, 258793, 80431]\n",
      "2021-10-11 00:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013604\n",
      "2021-10-11 00:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.318996\n",
      "2021-10-11 00:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.448962\n",
      "2021-10-11 00:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 162:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:26:44 | INFO | fairseq.trainer | begin training epoch 162\n",
      "epoch 162: 100%|9| 2036/2037 [05:19<00:00,  6.24it/s, loss=4.349, nll_loss=2.6842021-10-11 00:32:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003810\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.162205\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.110560\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.277638\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003686\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.161446\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.109543\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.275699\n",
      "2021-10-11 00:32:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 162 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.92it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:09,  8.01it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:07,  9.67it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:06, 11.37it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.66it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.03it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  17%|#     | 14/81 [00:01<00:04, 15.13it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  20%|#1    | 16/81 [00:01<00:03, 16.29it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 16.89it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.44it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  27%|#6    | 22/81 [00:01<00:03, 18.09it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.34it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.48it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  35%|##    | 28/81 [00:01<00:02, 18.87it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.09it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 18.93it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  42%|##5   | 34/81 [00:02<00:02, 19.02it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  44%|##6   | 36/81 [00:02<00:02, 18.70it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.03it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 18.79it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 18.93it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 18.94it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.31it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.45it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.41it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  65%|###9  | 53/81 [00:03<00:01, 19.53it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  68%|####  | 55/81 [00:03<00:01, 19.25it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 19.31it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.16it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.27it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.31it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.31it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.36it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.26it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  88%|#####2| 71/81 [00:04<00:00, 18.95it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  90%|#####4| 73/81 [00:04<00:00, 18.93it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.22it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.68it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:32:09 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 6.844 | nll_loss 5.454 | ppl 43.85 | wps 45462.4 | wpb 2494.5 | bsz 169.9 | num_updates 14246 | best_loss 6.688\n",
      "2021-10-11 00:32:09 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:32:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint162.pt (epoch 162 @ 14246 updates, score 6.844) (writing took 5.078127430000222 seconds)\n",
      "2021-10-11 00:32:14 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)\n",
      "2021-10-11 00:32:14 | INFO | train | epoch 162 | loss 4.137 | nll_loss 2.445 | ppl 5.44 | wps 21547.3 | ups 6.12 | wpb 3520.3 | bsz 208.9 | num_updates 14246 | lr 0.000264944 | gnorm 3.55 | loss_scale 0.0156 | train_wall 309 | wall 0\n",
      "2021-10-11 00:32:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=163/shard_epoch=7\n",
      "2021-10-11 00:32:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=163/shard_epoch=8\n",
      "2021-10-11 00:32:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:32:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007766\n",
      "2021-10-11 00:32:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.169837\n",
      "2021-10-11 00:32:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:14 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[113752, 49745, 248152, 401102, 372461, 23762, 372765, 127746, 120189, 4336]\n",
      "2021-10-11 00:32:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015392\n",
      "2021-10-11 00:32:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.474849\n",
      "2021-10-11 00:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.661094\n",
      "2021-10-11 00:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.123963\n",
      "2021-10-11 00:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:16 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[113752, 49745, 248152, 401102, 372461, 23762, 372765, 127746, 120189, 4336]\n",
      "2021-10-11 00:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.016314\n",
      "2021-10-11 00:32:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:32:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.273667\n",
      "2021-10-11 00:32:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.415016\n",
      "2021-10-11 00:32:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 163:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:32:17 | INFO | fairseq.trainer | begin training epoch 163\n",
      "epoch 163: 100%|9| 2036/2037 [05:18<00:00,  6.94it/s, loss=4.028, nll_loss=2.3352021-10-11 00:37:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004056\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.168690\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.106176\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.280012\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003181\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.166305\n",
      "2021-10-11 00:37:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105678\n",
      "2021-10-11 00:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.276220\n",
      "2021-10-11 00:37:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 163 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.52it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.29it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.96it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.48it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.70it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.75it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.42it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 17.08it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 17.35it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.79it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  27%|#6    | 22/81 [00:01<00:03, 18.23it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.39it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.37it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  35%|##    | 28/81 [00:01<00:02, 18.75it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 18.90it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 18.79it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 18.89it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 18.49it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 18.69it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 18.40it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 18.69it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 18.78it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  57%|###4  | 46/81 [00:02<00:01, 19.05it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.23it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.17it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 19.33it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.20it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 19.17it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 18.97it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 18.97it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 18.92it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 18.96it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.01it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.11it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 18.93it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 18.74it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.11it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.67it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.07it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:37:41 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 6.809 | nll_loss 5.395 | ppl 42.07 | wps 47395.3 | wpb 2494.5 | bsz 169.9 | num_updates 16283 | best_loss 6.688\n",
      "2021-10-11 00:37:41 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:37:47 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint163.pt (epoch 163 @ 16283 updates, score 6.809) (writing took 5.98672709199991 seconds)\n",
      "2021-10-11 00:37:47 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)\n",
      "2021-10-11 00:37:47 | INFO | train | epoch 163 | loss 4.061 | nll_loss 2.367 | ppl 5.16 | wps 21536.8 | ups 6.12 | wpb 3520.3 | bsz 208.9 | num_updates 16283 | lr 0.000247818 | gnorm 3.528 | loss_scale 0.0156 | train_wall 308 | wall 0\n",
      "2021-10-11 00:37:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=164/shard_epoch=8\n",
      "2021-10-11 00:37:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=164/shard_epoch=9\n",
      "2021-10-11 00:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:37:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007624\n",
      "2021-10-11 00:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.166424\n",
      "2021-10-11 00:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:47 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[251760, 139543, 191072, 323767, 130455, 27024, 73757, 277587, 104487, 167689]\n",
      "2021-10-11 00:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.018094\n",
      "2021-10-11 00:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.403821\n",
      "2021-10-11 00:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.589553\n",
      "2021-10-11 00:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.131881\n",
      "2021-10-11 00:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:49 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[251760, 139543, 191072, 323767, 130455, 27024, 73757, 277587, 104487, 167689]\n",
      "2021-10-11 00:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.016685\n",
      "2021-10-11 00:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.506460\n",
      "2021-10-11 00:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.656143\n",
      "2021-10-11 00:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 164:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:37:50 | INFO | fairseq.trainer | begin training epoch 164\n",
      "epoch 164:   6%| | 127/2037 [00:20<05:07,  6.22it/s, loss=3.952, nll_loss=2.242,2021-10-11 00:38:10 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 0.0078125\n",
      "epoch 164: 100%|9| 2036/2037 [05:18<00:00,  6.08it/s, loss=4.497, nll_loss=2.6482021-10-11 00:43:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:43:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:43:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003654\n",
      "2021-10-11 00:43:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.167529\n",
      "2021-10-11 00:43:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.108614\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.280896\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003327\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.171254\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.114280\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.289943\n",
      "2021-10-11 00:43:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 164 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.98it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:   4%|2      | 3/81 [00:00<00:08,  9.64it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:   6%|4      | 5/81 [00:00<00:06, 11.12it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:05, 12.55it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.85it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.90it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.70it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  19%|#1    | 15/81 [00:00<00:04, 16.30it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  21%|#2    | 17/81 [00:01<00:03, 16.81it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.32it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 17.48it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 17.86it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:03, 17.97it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:03, 17.99it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 18.48it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 18.40it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 17.95it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 18.01it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  46%|##7   | 37/81 [00:02<00:02, 18.26it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 18.22it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 18.13it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:02, 18.02it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:02, 17.99it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 18.49it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 18.61it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 18.71it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 18.69it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  68%|####  | 55/81 [00:03<00:01, 18.50it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 18.64it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 18.58it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 18.50it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 18.51it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 18.47it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 18.47it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 18.40it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 18.33it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  90%|#####4| 73/81 [00:04<00:00, 18.15it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  93%|#####5| 75/81 [00:04<00:00, 18.44it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  95%|#####7| 77/81 [00:04<00:00, 18.31it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 18.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:43:14 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 6.836 | nll_loss 5.243 | ppl 37.87 | wps 45668.2 | wpb 2494.5 | bsz 169.9 | num_updates 18319 | best_loss 6.688\n",
      "2021-10-11 00:43:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:43:20 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint164.pt (epoch 164 @ 18319 updates, score 6.836) (writing took 5.183162839000033 seconds)\n",
      "2021-10-11 00:43:20 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)\n",
      "2021-10-11 00:43:20 | INFO | train | epoch 164 | loss 4.686 | nll_loss 2.899 | ppl 7.46 | wps 21540.3 | ups 6.12 | wpb 3520.2 | bsz 208.9 | num_updates 18319 | lr 0.000233641 | gnorm 4.697 | loss_scale 0.0078 | train_wall 308 | wall 0\n",
      "2021-10-11 00:43:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=165/shard_epoch=9\n",
      "2021-10-11 00:43:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=165/shard_epoch=10\n",
      "2021-10-11 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:43:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008009\n",
      "2021-10-11 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.181898\n",
      "2021-10-11 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:20 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[397891, 224522, 290964, 358100, 249971, 240146, 183245, 55947, 217697, 386133]\n",
      "2021-10-11 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.018515\n",
      "2021-10-11 00:43:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.444346\n",
      "2021-10-11 00:43:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.645754\n",
      "2021-10-11 00:43:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.128297\n",
      "2021-10-11 00:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:22 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[397891, 224522, 290964, 358100, 249971, 240146, 183245, 55947, 217697, 386133]\n",
      "2021-10-11 00:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017389\n",
      "2021-10-11 00:43:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:43:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.362822\n",
      "2021-10-11 00:43:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.509594\n",
      "2021-10-11 00:43:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 165:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:43:23 | INFO | fairseq.trainer | begin training epoch 165\n",
      "epoch 165: 100%|9| 2036/2037 [05:19<00:00,  6.90it/s, loss=4.755, nll_loss=2.9112021-10-11 00:48:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003549\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156058\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.111778\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.272379\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003270\n",
      "2021-10-11 00:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.162662\n",
      "2021-10-11 00:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.106398\n",
      "2021-10-11 00:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.273265\n",
      "2021-10-11 00:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 165 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.25it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.05it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.72it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.33it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.61it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.72it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.57it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 17.45it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.81it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.14it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  27%|#6    | 22/81 [00:01<00:03, 18.53it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.68it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.72it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.16it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 18.97it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 18.79it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 18.85it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  46%|##7   | 37/81 [00:01<00:02, 18.97it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 18.84it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 18.72it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:02, 18.79it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 18.92it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.44it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.14it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.20it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.19it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  69%|####1 | 56/81 [00:02<00:01, 19.31it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.28it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 18.97it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.20it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.19it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.29it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.29it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.08it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 18.95it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.23it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.72it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.12it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:48:47 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 6.877 | nll_loss 5.29 | ppl 39.13 | wps 48008.6 | wpb 2494.5 | bsz 169.9 | num_updates 20356 | best_loss 6.688\n",
      "2021-10-11 00:48:47 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:48:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint165.pt (epoch 165 @ 20356 updates, score 6.877) (writing took 3.4445523540002796 seconds)\n",
      "2021-10-11 00:48:51 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)\n",
      "2021-10-11 00:48:51 | INFO | train | epoch 165 | loss 4.609 | nll_loss 2.75 | ppl 6.73 | wps 21677.8 | ups 6.16 | wpb 3520.3 | bsz 208.9 | num_updates 20356 | lr 0.000221643 | gnorm 5.184 | loss_scale 0.0078 | train_wall 308 | wall 0\n",
      "2021-10-11 00:48:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=166/shard_epoch=10\n",
      "2021-10-11 00:48:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=166/shard_epoch=11\n",
      "2021-10-11 00:48:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:48:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007135\n",
      "2021-10-11 00:48:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.163979\n",
      "2021-10-11 00:48:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:51 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[416945, 404167, 316541, 363953, 27115, 372451, 17780, 30473, 176310, 103727]\n",
      "2021-10-11 00:48:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015020\n",
      "2021-10-11 00:48:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.358886\n",
      "2021-10-11 00:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.538999\n",
      "2021-10-11 00:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.125297\n",
      "2021-10-11 00:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:52 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[416945, 404167, 316541, 363953, 27115, 372451, 17780, 30473, 176310, 103727]\n",
      "2021-10-11 00:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.018685\n",
      "2021-10-11 00:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.383667\n",
      "2021-10-11 00:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.528734\n",
      "2021-10-11 00:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 166:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:48:54 | INFO | fairseq.trainer | begin training epoch 166\n",
      "epoch 166: 100%|9| 2036/2037 [05:16<00:00,  6.27it/s, loss=4.66, nll_loss=2.85, 2021-10-11 00:54:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003678\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.184793\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.114035\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.303708\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003295\n",
      "2021-10-11 00:54:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157871\n",
      "2021-10-11 00:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105232\n",
      "2021-10-11 00:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.267507\n",
      "2021-10-11 00:54:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 166 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.42it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.23it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.93it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.54it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.78it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.84it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.66it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.42it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.04it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.27it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.59it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.85it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.92it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.19it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.09it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.22it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 18.77it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.02it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 18.70it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 18.83it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 18.94it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.29it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.34it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.37it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 19.52it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.36it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 19.46it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.20it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.23it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.18it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.09it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.09it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.20it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.04it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 18.99it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.22it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.74it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.13it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:54:15 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 6.891 | nll_loss 5.278 | ppl 38.81 | wps 48117.3 | wpb 2494.5 | bsz 169.9 | num_updates 22393 | best_loss 6.688\n",
      "2021-10-11 00:54:15 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:54:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint166.pt (epoch 166 @ 22393 updates, score 6.891) (writing took 3.5573672430000443 seconds)\n",
      "2021-10-11 00:54:19 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)\n",
      "2021-10-11 00:54:19 | INFO | train | epoch 166 | loss 4.644 | nll_loss 2.807 | ppl 7 | wps 21855.6 | ups 6.21 | wpb 3520.3 | bsz 208.9 | num_updates 22393 | lr 0.000211322 | gnorm 6.08 | loss_scale 0.0078 | train_wall 306 | wall 0\n",
      "2021-10-11 00:54:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=167/shard_epoch=11\n",
      "2021-10-11 00:54:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=167/shard_epoch=12\n",
      "2021-10-11 00:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:54:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.009367\n",
      "2021-10-11 00:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.166290\n",
      "2021-10-11 00:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:19 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[140446, 350572, 260583, 280, 21126, 127558, 141412, 103036, 207206, 284614]\n",
      "2021-10-11 00:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015798\n",
      "2021-10-11 00:54:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.285423\n",
      "2021-10-11 00:54:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.468487\n",
      "2021-10-11 00:54:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:54:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.131519\n",
      "2021-10-11 00:54:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:20 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[140446, 350572, 260583, 280, 21126, 127558, 141412, 103036, 207206, 284614]\n",
      "2021-10-11 00:54:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.016700\n",
      "2021-10-11 00:54:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.291131\n",
      "2021-10-11 00:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.440370\n",
      "2021-10-11 00:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 167:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:54:22 | INFO | fairseq.trainer | begin training epoch 167\n",
      "epoch 167: 100%|9| 2036/2037 [05:16<00:00,  6.57it/s, loss=4.685, nll_loss=2.8882021-10-11 00:59:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 00:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003542\n",
      "2021-10-11 00:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.160366\n",
      "2021-10-11 00:59:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.107116\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.271956\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003136\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157118\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103696\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264815\n",
      "2021-10-11 00:59:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 167 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.47it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.34it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 12.04it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.66it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.92it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 16.01it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:03, 16.80it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.60it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.24it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.44it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.85it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.17it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.26it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.52it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:03, 13.53it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:03, 14.98it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  44%|##6   | 36/81 [00:02<00:02, 15.82it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 16.83it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 17.29it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 17.91it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:02, 18.34it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 18.93it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.19it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.26it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.38it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  69%|####1 | 56/81 [00:03<00:01, 19.54it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.55it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.27it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.52it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.47it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.57it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.64it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.39it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.30it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.54it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.01it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.43it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 00:59:43 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 6.968 | nll_loss 5.328 | ppl 40.16 | wps 47190.2 | wpb 2494.5 | bsz 169.9 | num_updates 24430 | best_loss 6.688\n",
      "2021-10-11 00:59:43 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 00:59:46 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint167.pt (epoch 167 @ 24430 updates, score 6.968) (writing took 3.214657215999978 seconds)\n",
      "2021-10-11 00:59:46 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)\n",
      "2021-10-11 00:59:46 | INFO | train | epoch 167 | loss 4.695 | nll_loss 2.883 | ppl 7.38 | wps 21875.6 | ups 6.21 | wpb 3520.3 | bsz 208.9 | num_updates 24430 | lr 0.00020232 | gnorm 6.845 | loss_scale 0.0078 | train_wall 306 | wall 0\n",
      "2021-10-11 00:59:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=168/shard_epoch=12\n",
      "2021-10-11 00:59:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=168/shard_epoch=13\n",
      "2021-10-11 00:59:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:59:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006800\n",
      "2021-10-11 00:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.143749\n",
      "2021-10-11 00:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:47 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[263396, 290932, 151476, 342766, 301234, 26913, 237001, 420120, 130965, 289587]\n",
      "2021-10-11 00:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011398\n",
      "2021-10-11 00:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.276292\n",
      "2021-10-11 00:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.432308\n",
      "2021-10-11 00:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 00:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116814\n",
      "2021-10-11 00:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:48 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[263396, 290932, 151476, 342766, 301234, 26913, 237001, 420120, 130965, 289587]\n",
      "2021-10-11 00:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011662\n",
      "2021-10-11 00:59:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 00:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.249585\n",
      "2021-10-11 00:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.378954\n",
      "2021-10-11 00:59:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 168:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 00:59:49 | INFO | fairseq.trainer | begin training epoch 168\n",
      "epoch 168: 100%|9| 2036/2037 [05:11<00:00,  6.25it/s, loss=4.735, nll_loss=2.9412021-10-11 01:05:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003762\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.175981\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102266\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.283048\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002997\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.159006\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102695\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265732\n",
      "2021-10-11 01:05:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 168 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.64it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.53it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 12.25it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.84it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 15.08it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 16.13it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:03, 16.93it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.70it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.30it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.49it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.84it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.13it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.19it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.47it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.32it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.44it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.06it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.29it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.04it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.19it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.26it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.58it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.64it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.61it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 19.71it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.52it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.61it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.40it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.53it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.56it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.48it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.56it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.58it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.32it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.23it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.50it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.97it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:05:05 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 6.89 | nll_loss 5.34 | ppl 40.51 | wps 48876.3 | wpb 2494.5 | bsz 169.9 | num_updates 26467 | best_loss 6.688\n",
      "2021-10-11 01:05:05 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:05:09 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint168.pt (epoch 168 @ 26467 updates, score 6.89) (writing took 3.3513547870006732 seconds)\n",
      "2021-10-11 01:05:09 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)\n",
      "2021-10-11 01:05:09 | INFO | train | epoch 168 | loss 4.763 | nll_loss 2.965 | ppl 7.81 | wps 22238.5 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 26467 | lr 0.000194378 | gnorm 7.539 | loss_scale 0.0078 | train_wall 302 | wall 0\n",
      "2021-10-11 01:05:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=169/shard_epoch=13\n",
      "2021-10-11 01:05:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=169/shard_epoch=14\n",
      "2021-10-11 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:05:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006881\n",
      "2021-10-11 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.148654\n",
      "2021-10-11 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:09 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[98445, 180007, 358113, 411728, 42239, 29081, 261974, 315150, 347883, 80185]\n",
      "2021-10-11 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012161\n",
      "2021-10-11 01:05:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.190218\n",
      "2021-10-11 01:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.351981\n",
      "2021-10-11 01:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.123295\n",
      "2021-10-11 01:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:10 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[98445, 180007, 358113, 411728, 42239, 29081, 261974, 315150, 347883, 80185]\n",
      "2021-10-11 01:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012487\n",
      "2021-10-11 01:05:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:05:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.196135\n",
      "2021-10-11 01:05:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.332938\n",
      "2021-10-11 01:05:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 169:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:05:12 | INFO | fairseq.trainer | begin training epoch 169\n",
      "epoch 169: 100%|9| 2036/2037 [05:11<00:00,  6.24it/s, loss=4.967, nll_loss=3.1872021-10-11 01:10:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003847\n",
      "2021-10-11 01:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155582\n",
      "2021-10-11 01:10:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103421\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263771\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002987\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154860\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103323\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262084\n",
      "2021-10-11 01:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 169 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.55it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.43it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 12.14it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.75it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.98it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 16.06it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:03, 16.87it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.65it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.26it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.44it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.82it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.10it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.18it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.46it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.29it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.44it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.04it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.27it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.04it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.19it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.27it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.54it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.60it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.55it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 19.66it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.47it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.56it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.33it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.29it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.38it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.40it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.47it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.56it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.32it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.21it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.49it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.01it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:10:28 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 6.947 | nll_loss 5.375 | ppl 41.5 | wps 48793.7 | wpb 2494.5 | bsz 169.9 | num_updates 28504 | best_loss 6.688\n",
      "2021-10-11 01:10:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:10:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint169.pt (epoch 169 @ 28504 updates, score 6.947) (writing took 3.314574879999782 seconds)\n",
      "2021-10-11 01:10:31 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)\n",
      "2021-10-11 01:10:31 | INFO | train | epoch 169 | loss 4.839 | nll_loss 3.035 | ppl 8.19 | wps 22235.5 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 28504 | lr 0.000187304 | gnorm 7.945 | loss_scale 0.0078 | train_wall 302 | wall 0\n",
      "2021-10-11 01:10:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=170/shard_epoch=14\n",
      "2021-10-11 01:10:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=170/shard_epoch=15\n",
      "2021-10-11 01:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:10:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006853\n",
      "2021-10-11 01:10:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.143816\n",
      "2021-10-11 01:10:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:32 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[21002, 35583, 379889, 309960, 305991, 75113, 155299, 94648, 363868, 314863]\n",
      "2021-10-11 01:10:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013795\n",
      "2021-10-11 01:10:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.215209\n",
      "2021-10-11 01:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.373727\n",
      "2021-10-11 01:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116024\n",
      "2021-10-11 01:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:33 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[21002, 35583, 379889, 309960, 305991, 75113, 155299, 94648, 363868, 314863]\n",
      "2021-10-11 01:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013042\n",
      "2021-10-11 01:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.218272\n",
      "2021-10-11 01:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.348257\n",
      "2021-10-11 01:10:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 170:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:10:34 | INFO | fairseq.trainer | begin training epoch 170\n",
      "epoch 170: 100%|9| 2036/2037 [05:11<00:00,  6.48it/s, loss=4.908, nll_loss=3.1132021-10-11 01:15:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003313\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154465\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105928\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264658\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003027\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.150372\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103861\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.258272\n",
      "2021-10-11 01:15:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 170 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.31it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.14it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.84it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.45it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.75it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.90it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.74it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.58it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.22it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.46it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.98it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 19.07it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.46it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.25it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 18.94it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.03it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  46%|##7   | 37/81 [00:01<00:02, 19.30it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.14it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 18.89it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:02, 18.88it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.02it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.61it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.59it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.58it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.61it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.73it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.55it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.62it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.67it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.62it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.72it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.78it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.50it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.34it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.60it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.06it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:15:51 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 6.936 | nll_loss 5.352 | ppl 40.84 | wps 48863.6 | wpb 2494.5 | bsz 169.9 | num_updates 30541 | best_loss 6.688\n",
      "2021-10-11 01:15:51 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:15:54 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint170.pt (epoch 170 @ 30541 updates, score 6.936) (writing took 3.4782356639998397 seconds)\n",
      "2021-10-11 01:15:54 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)\n",
      "2021-10-11 01:15:54 | INFO | train | epoch 170 | loss 4.902 | nll_loss 3.108 | ppl 8.62 | wps 22226.4 | ups 6.31 | wpb 3520.3 | bsz 208.9 | num_updates 30541 | lr 0.00018095 | gnorm 8.623 | loss_scale 0.0078 | train_wall 302 | wall 0\n",
      "2021-10-11 01:15:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=171/shard_epoch=15\n",
      "2021-10-11 01:15:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=171/shard_epoch=16\n",
      "2021-10-11 01:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:15:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006639\n",
      "2021-10-11 01:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.146750\n",
      "2021-10-11 01:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:54 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[223045, 73258, 207266, 405314, 267136, 217678, 295118, 15008, 10345, 88221]\n",
      "2021-10-11 01:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012898\n",
      "2021-10-11 01:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.251577\n",
      "2021-10-11 01:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.412194\n",
      "2021-10-11 01:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:15:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117908\n",
      "2021-10-11 01:15:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:56 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[223045, 73258, 207266, 405314, 267136, 217678, 295118, 15008, 10345, 88221]\n",
      "2021-10-11 01:15:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013401\n",
      "2021-10-11 01:15:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.256984\n",
      "2021-10-11 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.389285\n",
      "2021-10-11 01:15:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 171:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:15:57 | INFO | fairseq.trainer | begin training epoch 171\n",
      "epoch 171: 100%|9| 2036/2037 [05:11<00:00,  6.53it/s, loss=5.174, nll_loss=3.3222021-10-11 01:21:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003224\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.159725\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103239\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.267076\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003033\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157584\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103675\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265154\n",
      "2021-10-11 01:21:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 171 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.52it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.32it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.01it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 12.68it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 14.03it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.30it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.28it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.16it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.81it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.11it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.56it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.89it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.03it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.34it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.23it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.41it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.07it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.27it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.00it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.20it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.34it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.69it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.65it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.72it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.76it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.62it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.74it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.42it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.28it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.54it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.01it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:21:13 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 7.036 | nll_loss 5.463 | ppl 44.12 | wps 48841.1 | wpb 2494.5 | bsz 169.9 | num_updates 32578 | best_loss 6.688\n",
      "2021-10-11 01:21:13 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:21:17 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint171.pt (epoch 171 @ 32578 updates, score 7.036) (writing took 3.3493720890000986 seconds)\n",
      "2021-10-11 01:21:17 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)\n",
      "2021-10-11 01:21:17 | INFO | train | epoch 171 | loss 4.985 | nll_loss 3.188 | ppl 9.11 | wps 22217.9 | ups 6.31 | wpb 3520.3 | bsz 208.9 | num_updates 32578 | lr 0.000175201 | gnorm 9.152 | loss_scale 0.0078 | train_wall 302 | wall 0\n",
      "2021-10-11 01:21:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=172/shard_epoch=16\n",
      "2021-10-11 01:21:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=172/shard_epoch=17\n",
      "2021-10-11 01:21:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:21:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007328\n",
      "2021-10-11 01:21:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.158219\n",
      "2021-10-11 01:21:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:17 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[131568, 358605, 27760, 231382, 175662, 285430, 114049, 272536, 295068, 98969]\n",
      "2021-10-11 01:21:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013990\n",
      "2021-10-11 01:21:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.221327\n",
      "2021-10-11 01:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.394458\n",
      "2021-10-11 01:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.119842\n",
      "2021-10-11 01:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:18 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[131568, 358605, 27760, 231382, 175662, 285430, 114049, 272536, 295068, 98969]\n",
      "2021-10-11 01:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013049\n",
      "2021-10-11 01:21:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.256115\n",
      "2021-10-11 01:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.389971\n",
      "2021-10-11 01:21:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 172:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:21:20 | INFO | fairseq.trainer | begin training epoch 172\n",
      "epoch 172: 100%|9| 2036/2037 [05:13<00:00,  6.51it/s, loss=7.769, nll_loss=5.4442021-10-11 01:26:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003373\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157318\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.106205\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.267854\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003046\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157430\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102453\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263893\n",
      "2021-10-11 01:26:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 172 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.34it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.16it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 10.93it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.63it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.12it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.27it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  19%|#1    | 15/81 [00:00<00:04, 16.40it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.22it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.95it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.24it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.71it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.04it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.14it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.48it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.37it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.55it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.20it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.39it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.31it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.37it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.40it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.92it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.69it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.68it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  69%|####1 | 56/81 [00:02<00:01, 19.73it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.67it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.32it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.51it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.49it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.47it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.54it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.33it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.23it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.52it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.05it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:26:38 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 9.036 | nll_loss 6.701 | ppl 104.06 | wps 49071.1 | wpb 2494.5 | bsz 169.9 | num_updates 34615 | best_loss 6.688\n",
      "2021-10-11 01:26:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:26:41 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint172.pt (epoch 172 @ 34615 updates, score 9.036) (writing took 3.480811919000189 seconds)\n",
      "2021-10-11 01:26:41 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)\n",
      "2021-10-11 01:26:41 | INFO | train | epoch 172 | loss 8.602 | nll_loss 5.927 | ppl 60.86 | wps 22109.9 | ups 6.28 | wpb 3520.3 | bsz 208.9 | num_updates 34615 | lr 0.000169968 | gnorm 11.717 | loss_scale 0.0156 | train_wall 304 | wall 0\n",
      "2021-10-11 01:26:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=173/shard_epoch=17\n",
      "2021-10-11 01:26:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=173/shard_epoch=18\n",
      "2021-10-11 01:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:26:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006920\n",
      "2021-10-11 01:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.145533\n",
      "2021-10-11 01:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:41 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[86566, 156246, 302320, 399631, 60887, 246470, 112171, 421119, 284133, 210452]\n",
      "2021-10-11 01:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013743\n",
      "2021-10-11 01:26:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.242676\n",
      "2021-10-11 01:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.403003\n",
      "2021-10-11 01:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117530\n",
      "2021-10-11 01:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:43 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[86566, 156246, 302320, 399631, 60887, 246470, 112171, 421119, 284133, 210452]\n",
      "2021-10-11 01:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013754\n",
      "2021-10-11 01:26:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.225089\n",
      "2021-10-11 01:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.357272\n",
      "2021-10-11 01:26:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 173:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:26:44 | INFO | fairseq.trainer | begin training epoch 173\n",
      "epoch 173: 100%|9| 2036/2037 [05:13<00:00,  6.73it/s, loss=5.14, nll_loss=3.35, 2021-10-11 01:31:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004795\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.173551\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.133131\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.312534\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003782\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.173606\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.133859\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.312377\n",
      "2021-10-11 01:31:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 173 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.60it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.41it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.12it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 12.76it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 14.07it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.28it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.23it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 17.19it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 17.72it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.20it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.55it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.83it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.91it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.20it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.12it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.24it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 18.59it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 18.77it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 18.60it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 18.82it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 18.98it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  57%|###4  | 46/81 [00:02<00:01, 19.27it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.41it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.35it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 19.34it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.20it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 19.43it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.31it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.43it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.36it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.29it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.43it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.53it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.29it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.17it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.45it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.98it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:32:03 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 7.293 | nll_loss 5.693 | ppl 51.72 | wps 48290.1 | wpb 2494.5 | bsz 169.9 | num_updates 36652 | best_loss 6.688\n",
      "2021-10-11 01:32:03 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:32:06 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint173.pt (epoch 173 @ 36652 updates, score 7.293) (writing took 3.431652141000086 seconds)\n",
      "2021-10-11 01:32:06 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)\n",
      "2021-10-11 01:32:06 | INFO | train | epoch 173 | loss 5.997 | nll_loss 4.009 | ppl 16.1 | wps 22056.8 | ups 6.27 | wpb 3520.3 | bsz 208.9 | num_updates 36652 | lr 0.000165178 | gnorm 16.447 | loss_scale 0.0156 | train_wall 304 | wall 0\n",
      "2021-10-11 01:32:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=174/shard_epoch=18\n",
      "2021-10-11 01:32:06 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=174/shard_epoch=19\n",
      "2021-10-11 01:32:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:32:06 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006819\n",
      "2021-10-11 01:32:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.152797\n",
      "2021-10-11 01:32:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:32:06 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[224977, 298078, 201224, 397832, 253263, 81605, 202158, 116483, 342633, 250638]\n",
      "2021-10-11 01:32:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.014338\n",
      "2021-10-11 01:32:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.286861\n",
      "2021-10-11 01:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.454947\n",
      "2021-10-11 01:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.121837\n",
      "2021-10-11 01:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:32:08 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[224977, 298078, 201224, 397832, 253263, 81605, 202158, 116483, 342633, 250638]\n",
      "2021-10-11 01:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013598\n",
      "2021-10-11 01:32:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.301436\n",
      "2021-10-11 01:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.437798\n",
      "2021-10-11 01:32:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 174:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:32:09 | INFO | fairseq.trainer | begin training epoch 174\n",
      "epoch 174: 100%|9| 2036/2037 [05:13<00:00,  6.24it/s, loss=4.753, nll_loss=3.0212021-10-11 01:37:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003309\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.160960\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105000\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.270288\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003253\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157702\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105752\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.267809\n",
      "2021-10-11 01:37:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 174 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.59it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:09,  7.92it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:07,  9.62it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:06, 11.38it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.95it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.36it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.54it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  21%|#2    | 17/81 [00:01<00:03, 16.64it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.47it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 17.90it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.45it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.86it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.98it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.34it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.26it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  43%|##5   | 35/81 [00:02<00:02, 19.21it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  46%|##7   | 37/81 [00:02<00:02, 19.42it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.26it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.15it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.19it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.24it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.68it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.65it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  67%|####  | 54/81 [00:03<00:01, 19.63it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  69%|####1 | 56/81 [00:03<00:01, 19.72it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.64it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.17it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.36it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.31it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.42it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.56it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.33it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  90%|#####4| 73/81 [00:04<00:00, 19.22it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.51it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.02it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:37:28 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 6.98 | nll_loss 5.492 | ppl 45.01 | wps 46580 | wpb 2494.5 | bsz 169.9 | num_updates 38689 | best_loss 6.688\n",
      "2021-10-11 01:37:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:37:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint174.pt (epoch 174 @ 38689 updates, score 6.98) (writing took 3.401888895999946 seconds)\n",
      "2021-10-11 01:37:31 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)\n",
      "2021-10-11 01:37:31 | INFO | train | epoch 174 | loss 4.801 | nll_loss 3.057 | ppl 8.32 | wps 22058 | ups 6.27 | wpb 3520.3 | bsz 208.9 | num_updates 38689 | lr 0.00016077 | gnorm 17.683 | loss_scale 0.0156 | train_wall 304 | wall 0\n",
      "2021-10-11 01:37:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=175/shard_epoch=19\n",
      "2021-10-11 01:37:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=175/shard_epoch=20\n",
      "2021-10-11 01:37:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:37:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008013\n",
      "2021-10-11 01:37:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.156462\n",
      "2021-10-11 01:37:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:31 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[100189, 407473, 20466, 353348, 386590, 393020, 192847, 203113, 297801, 209399]\n",
      "2021-10-11 01:37:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015523\n",
      "2021-10-11 01:37:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.589110\n",
      "2021-10-11 01:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.762040\n",
      "2021-10-11 01:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.124630\n",
      "2021-10-11 01:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:33 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[100189, 407473, 20466, 353348, 386590, 393020, 192847, 203113, 297801, 209399]\n",
      "2021-10-11 01:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.014134\n",
      "2021-10-11 01:37:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:37:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.224372\n",
      "2021-10-11 01:37:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.364091\n",
      "2021-10-11 01:37:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 175:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:37:34 | INFO | fairseq.trainer | begin training epoch 175\n",
      "epoch 175: 100%|9| 2036/2037 [05:12<00:00,  6.88it/s, loss=4.405, nll_loss=2.6912021-10-11 01:42:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003844\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157678\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105462\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.267950\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002972\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156297\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104317\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264556\n",
      "2021-10-11 01:42:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 175 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.24it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.08it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.76it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.37it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.52it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.66it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.58it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.41it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.03it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.23it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.59it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.94it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.08it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.41it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.29it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.39it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 18.88it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.18it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 18.98it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.15it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.23it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.52it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.47it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.35it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.42it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  69%|####1 | 56/81 [00:02<00:01, 19.48it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.50it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.21it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.46it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.43it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.52it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.59it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.33it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.18it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.46it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 18.94it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:42:52 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 7.113 | nll_loss 5.528 | ppl 46.15 | wps 48487.1 | wpb 2494.5 | bsz 169.9 | num_updates 40726 | best_loss 6.688\n",
      "2021-10-11 01:42:52 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:42:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint175.pt (epoch 175 @ 40726 updates, score 7.113) (writing took 3.2386600780000663 seconds)\n",
      "2021-10-11 01:42:56 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)\n",
      "2021-10-11 01:42:56 | INFO | train | epoch 175 | loss 4.591 | nll_loss 2.868 | ppl 7.3 | wps 22113.6 | ups 6.28 | wpb 3520.3 | bsz 208.9 | num_updates 40726 | lr 0.000156698 | gnorm 16.803 | loss_scale 0.0156 | train_wall 304 | wall 0\n",
      "2021-10-11 01:42:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=176/shard_epoch=20\n",
      "2021-10-11 01:42:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=176/shard_epoch=21\n",
      "2021-10-11 01:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:42:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006771\n",
      "2021-10-11 01:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.150895\n",
      "2021-10-11 01:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:56 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[423195, 271559, 138572, 368068, 354712, 57683, 37391, 28126, 140588, 175087]\n",
      "2021-10-11 01:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013059\n",
      "2021-10-11 01:42:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.232805\n",
      "2021-10-11 01:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.397766\n",
      "2021-10-11 01:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.122612\n",
      "2021-10-11 01:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:57 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[423195, 271559, 138572, 368068, 354712, 57683, 37391, 28126, 140588, 175087]\n",
      "2021-10-11 01:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012586\n",
      "2021-10-11 01:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.223951\n",
      "2021-10-11 01:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.360130\n",
      "2021-10-11 01:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 176:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:42:58 | INFO | fairseq.trainer | begin training epoch 176\n",
      "epoch 176: 100%|9| 2036/2037 [05:09<00:00,  6.73it/s, loss=4.466, nll_loss=2.7462021-10-11 01:48:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003634\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157131\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103067\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264533\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002706\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153995\n",
      "2021-10-11 01:48:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103198\n",
      "2021-10-11 01:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260497\n",
      "2021-10-11 01:48:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 176 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.39it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.28it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 12.02it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.66it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.06it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.16it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.21it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.90it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.45it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.85it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.33it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.27it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.07it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.19it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  46%|##7   | 37/81 [00:01<00:02, 19.29it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.28it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.28it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.40it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.50it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 20.06it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 20.02it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 20.03it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 20.08it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.81it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.99it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 20.03it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 20.13it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.82it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 20.11it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  96%|#####7| 78/81 [00:03<00:00, 19.73it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 21.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:48:13 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 6.954 | nll_loss 5.503 | ppl 45.36 | wps 49809.4 | wpb 2494.5 | bsz 169.9 | num_updates 42763 | best_loss 6.688\n",
      "2021-10-11 01:48:13 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:48:16 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint176.pt (epoch 176 @ 42763 updates, score 6.954) (writing took 3.254754208999657 seconds)\n",
      "2021-10-11 01:48:16 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)\n",
      "2021-10-11 01:48:16 | INFO | train | epoch 176 | loss 4.498 | nll_loss 2.779 | ppl 6.87 | wps 22379.3 | ups 6.36 | wpb 3520.3 | bsz 208.9 | num_updates 42763 | lr 0.000152921 | gnorm 16.487 | loss_scale 0.0156 | train_wall 301 | wall 0\n",
      "2021-10-11 01:48:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=177/shard_epoch=21\n",
      "2021-10-11 01:48:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=177/shard_epoch=22\n",
      "2021-10-11 01:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:48:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006599\n",
      "2021-10-11 01:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.143422\n",
      "2021-10-11 01:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:16 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[238910, 174467, 413484, 397031, 340687, 387457, 80963, 99172, 100534, 57188]\n",
      "2021-10-11 01:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011966\n",
      "2021-10-11 01:48:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.242533\n",
      "2021-10-11 01:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.398875\n",
      "2021-10-11 01:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:48:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115266\n",
      "2021-10-11 01:48:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:18 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[238910, 174467, 413484, 397031, 340687, 387457, 80963, 99172, 100534, 57188]\n",
      "2021-10-11 01:48:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011311\n",
      "2021-10-11 01:48:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.246435\n",
      "2021-10-11 01:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.373959\n",
      "2021-10-11 01:48:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 177:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:48:19 | INFO | fairseq.trainer | begin training epoch 177\n",
      "epoch 177: 100%|9| 2036/2037 [05:09<00:00,  6.56it/s, loss=4.594, nll_loss=2.8572021-10-11 01:53:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003350\n",
      "2021-10-11 01:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157629\n",
      "2021-10-11 01:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102068\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263730\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002909\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155111\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104280\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262888\n",
      "2021-10-11 01:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 177 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.42it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.34it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 12.09it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.72it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 15.08it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 16.15it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 17.20it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.80it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.34it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.76it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 19.08it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.61it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.48it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.29it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.36it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.56it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.35it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:01, 19.50it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.59it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.90it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.97it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.93it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.93it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 20.01it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.77it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.93it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.88it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.99it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.48it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 19.80it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.60it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.56it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:53:33 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 7.066 | nll_loss 5.588 | ppl 48.1 | wps 49716.1 | wpb 2494.5 | bsz 169.9 | num_updates 44800 | best_loss 6.688\n",
      "2021-10-11 01:53:33 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:53:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint177.pt (epoch 177 @ 44800 updates, score 7.066) (writing took 3.2751489149995905 seconds)\n",
      "2021-10-11 01:53:36 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)\n",
      "2021-10-11 01:53:36 | INFO | train | epoch 177 | loss 4.451 | nll_loss 2.726 | ppl 6.62 | wps 22393.9 | ups 6.36 | wpb 3520.3 | bsz 208.9 | num_updates 44800 | lr 0.000149404 | gnorm 15.997 | loss_scale 0.0156 | train_wall 301 | wall 0\n",
      "2021-10-11 01:53:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=178/shard_epoch=22\n",
      "2021-10-11 01:53:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=178/shard_epoch=23\n",
      "2021-10-11 01:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:53:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006502\n",
      "2021-10-11 01:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139950\n",
      "2021-10-11 01:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:36 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[150243, 266415, 125088, 279387, 220648, 62090, 144587, 219926, 7953, 335783]\n",
      "2021-10-11 01:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012909\n",
      "2021-10-11 01:53:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.651067\n",
      "2021-10-11 01:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.804935\n",
      "2021-10-11 01:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115924\n",
      "2021-10-11 01:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:38 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[150243, 266415, 125088, 279387, 220648, 62090, 144587, 219926, 7953, 335783]\n",
      "2021-10-11 01:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010667\n",
      "2021-10-11 01:53:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:53:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.188090\n",
      "2021-10-11 01:53:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.315548\n",
      "2021-10-11 01:53:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 178:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:53:39 | INFO | fairseq.trainer | begin training epoch 178\n",
      "epoch 178: 100%|9| 2036/2037 [05:08<00:00,  6.30it/s, loss=4.531, nll_loss=2.8212021-10-11 01:58:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003196\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154875\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102590\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261301\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002927\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153210\n",
      "2021-10-11 01:58:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101489\n",
      "2021-10-11 01:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.258192\n",
      "2021-10-11 01:58:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 178 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.93it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.82it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.58it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.22it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.65it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.73it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.84it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.53it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.16it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.66it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 19.06it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.60it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.51it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.32it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.44it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.61it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.25it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.41it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.49it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 20.05it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 20.01it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 20.02it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 20.11it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.92it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 20.04it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 20.00it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 20.07it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 20.05it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  96%|#####7| 78/81 [00:03<00:00, 19.69it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 21.15it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 01:58:53 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 7.08 | nll_loss 5.616 | ppl 49.04 | wps 49856 | wpb 2494.5 | bsz 169.9 | num_updates 46837 | best_loss 6.688\n",
      "2021-10-11 01:58:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 01:58:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint178.pt (epoch 178 @ 46837 updates, score 7.08) (writing took 3.165541817999838 seconds)\n",
      "2021-10-11 01:58:56 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)\n",
      "2021-10-11 01:58:56 | INFO | train | epoch 178 | loss 4.408 | nll_loss 2.682 | ppl 6.42 | wps 22429.9 | ups 6.37 | wpb 3520.3 | bsz 208.9 | num_updates 46837 | lr 0.000146119 | gnorm 15.72 | loss_scale 0.0156 | train_wall 301 | wall 0\n",
      "2021-10-11 01:58:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=179/shard_epoch=23\n",
      "2021-10-11 01:58:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=179/shard_epoch=24\n",
      "2021-10-11 01:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:58:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006589\n",
      "2021-10-11 01:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.136491\n",
      "2021-10-11 01:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:56 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[214070, 128864, 169348, 355667, 321741, 292662, 22902, 322639, 315918, 302588]\n",
      "2021-10-11 01:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010925\n",
      "2021-10-11 01:58:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.194795\n",
      "2021-10-11 01:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.343086\n",
      "2021-10-11 01:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 01:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110953\n",
      "2021-10-11 01:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:57 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[214070, 128864, 169348, 355667, 321741, 292662, 22902, 322639, 315918, 302588]\n",
      "2021-10-11 01:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010872\n",
      "2021-10-11 01:58:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 01:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.189443\n",
      "2021-10-11 01:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.312157\n",
      "2021-10-11 01:58:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 179:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 01:58:59 | INFO | fairseq.trainer | begin training epoch 179\n",
      "epoch 179: 100%|9| 2036/2037 [05:09<00:00,  6.38it/s, loss=4.353, nll_loss=2.6332021-10-11 02:04:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003388\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.158574\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102998\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265821\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002986\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155157\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104045\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262938\n",
      "2021-10-11 02:04:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 179 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.21it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.12it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.88it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.53it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.91it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 16.01it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 17.03it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.60it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.18it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.67it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.99it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.53it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:03, 13.86it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:03, 14.98it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 16.08it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 17.05it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 17.52it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 18.17it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 18.58it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.17it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.43it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.56it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.72it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.89it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.65it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.92it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.90it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 20.09it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.86it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.76it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 20.02it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:04:14 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 7.069 | nll_loss 5.637 | ppl 49.77 | wps 48088.1 | wpb 2494.5 | bsz 169.9 | num_updates 48874 | best_loss 6.688\n",
      "2021-10-11 02:04:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:04:17 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint179.pt (epoch 179 @ 48874 updates, score 7.069) (writing took 3.245877876999657 seconds)\n",
      "2021-10-11 02:04:17 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)\n",
      "2021-10-11 02:04:17 | INFO | train | epoch 179 | loss 4.377 | nll_loss 2.647 | ppl 6.26 | wps 22348.3 | ups 6.35 | wpb 3520.3 | bsz 208.9 | num_updates 48874 | lr 0.000143041 | gnorm 15.53 | loss_scale 0.0156 | train_wall 302 | wall 0\n",
      "2021-10-11 02:04:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=180/shard_epoch=24\n",
      "2021-10-11 02:04:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=180/shard_epoch=25\n",
      "2021-10-11 02:04:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:04:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006668\n",
      "2021-10-11 02:04:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.144352\n",
      "2021-10-11 02:04:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:17 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[376221, 29541, 199760, 371755, 226115, 97604, 376812, 184100, 212953, 99179]\n",
      "2021-10-11 02:04:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011941\n",
      "2021-10-11 02:04:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.234523\n",
      "2021-10-11 02:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.391810\n",
      "2021-10-11 02:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111466\n",
      "2021-10-11 02:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:18 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[376221, 29541, 199760, 371755, 226115, 97604, 376812, 184100, 212953, 99179]\n",
      "2021-10-11 02:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011915\n",
      "2021-10-11 02:04:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.237720\n",
      "2021-10-11 02:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.362050\n",
      "2021-10-11 02:04:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 180:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:04:20 | INFO | fairseq.trainer | begin training epoch 180\n",
      "epoch 180: 100%|9| 2036/2037 [05:09<00:00,  6.56it/s, loss=4.513, nll_loss=2.78,2021-10-11 02:09:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003035\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154590\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101913\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260156\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002717\n",
      "2021-10-11 02:09:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153493\n",
      "2021-10-11 02:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102450\n",
      "2021-10-11 02:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.259192\n",
      "2021-10-11 02:09:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 180 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.58it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.51it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 12.25it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.82it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.12it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.19it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.22it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.88it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.43it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.84it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.40it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.30it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.17it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.28it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.47it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.26it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.42it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.52it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.92it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.92it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.70it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.88it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.80it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.88it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.63it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.54it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  96%|#####7| 78/81 [00:03<00:00, 19.27it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:09:34 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 7.185 | nll_loss 5.746 | ppl 53.65 | wps 49598.5 | wpb 2494.5 | bsz 169.9 | num_updates 50911 | best_loss 6.688\n",
      "2021-10-11 02:09:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:09:37 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint180.pt (epoch 180 @ 50911 updates, score 7.185) (writing took 3.252476657000443 seconds)\n",
      "2021-10-11 02:09:37 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)\n",
      "2021-10-11 02:09:37 | INFO | train | epoch 180 | loss 4.344 | nll_loss 2.597 | ppl 6.05 | wps 22393.1 | ups 6.36 | wpb 3520.3 | bsz 208.9 | num_updates 50911 | lr 0.00014015 | gnorm 14.383 | loss_scale 0.0312 | train_wall 301 | wall 0\n",
      "2021-10-11 02:09:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=181/shard_epoch=25\n",
      "2021-10-11 02:09:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=181/shard_epoch=26\n",
      "2021-10-11 02:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:09:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006825\n",
      "2021-10-11 02:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.140903\n",
      "2021-10-11 02:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:37 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[320089, 155755, 136222, 392567, 314569, 369087, 374284, 403278, 294173, 211875]\n",
      "2021-10-11 02:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012763\n",
      "2021-10-11 02:09:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.151314\n",
      "2021-10-11 02:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.305972\n",
      "2021-10-11 02:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112773\n",
      "2021-10-11 02:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:39 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[320089, 155755, 136222, 392567, 314569, 369087, 374284, 403278, 294173, 211875]\n",
      "2021-10-11 02:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011569\n",
      "2021-10-11 02:09:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.149466\n",
      "2021-10-11 02:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.274731\n",
      "2021-10-11 02:09:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 181:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:09:40 | INFO | fairseq.trainer | begin training epoch 181\n",
      "epoch 181: 100%|9| 2036/2037 [05:08<00:00,  6.87it/s, loss=4.421, nll_loss=2.6882021-10-11 02:14:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:14:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.005335\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155512\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101202\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262688\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002694\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153690\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101191\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.258115\n",
      "2021-10-11 02:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 181 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.15it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.97it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.65it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.26it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.63it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.86it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:03, 16.78it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.69it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.24it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.70it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 19.03it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.51it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.38it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.23it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.35it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.58it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.39it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:01, 19.55it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.62it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.94it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.97it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.96it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.95it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 20.06it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.82it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 20.04it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 20.03it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 20.09it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 20.08it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  96%|#####7| 78/81 [00:03<00:00, 19.71it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 21.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:14:53 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 7.211 | nll_loss 5.728 | ppl 52.99 | wps 49659.8 | wpb 2494.5 | bsz 169.9 | num_updates 52948 | best_loss 6.688\n",
      "2021-10-11 02:14:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:14:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint181.pt (epoch 181 @ 52948 updates, score 7.211) (writing took 3.285696081999049 seconds)\n",
      "2021-10-11 02:14:57 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)\n",
      "2021-10-11 02:14:57 | INFO | train | epoch 181 | loss 4.309 | nll_loss 2.554 | ppl 5.87 | wps 22447.3 | ups 6.38 | wpb 3520.3 | bsz 208.9 | num_updates 52948 | lr 0.000137428 | gnorm 13.796 | loss_scale 0.0312 | train_wall 301 | wall 0\n",
      "2021-10-11 02:14:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=182/shard_epoch=26\n",
      "2021-10-11 02:14:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=182/shard_epoch=27\n",
      "2021-10-11 02:14:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:14:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007737\n",
      "2021-10-11 02:14:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.146138\n",
      "2021-10-11 02:14:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:57 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[332854, 250707, 188852, 270072, 55189, 123836, 329707, 219759, 186267, 154852]\n",
      "2021-10-11 02:14:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011969\n",
      "2021-10-11 02:14:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.196935\n",
      "2021-10-11 02:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.355970\n",
      "2021-10-11 02:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111393\n",
      "2021-10-11 02:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:58 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[332854, 250707, 188852, 270072, 55189, 123836, 329707, 219759, 186267, 154852]\n",
      "2021-10-11 02:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011166\n",
      "2021-10-11 02:14:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:14:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.191083\n",
      "2021-10-11 02:14:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.314511\n",
      "2021-10-11 02:14:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 182:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:14:59 | INFO | fairseq.trainer | begin training epoch 182\n",
      "epoch 182: 100%|9| 2036/2037 [05:08<00:00,  6.51it/s, loss=4.322, nll_loss=2.5792021-10-11 02:20:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003251\n",
      "2021-10-11 02:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155258\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104834\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263943\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002745\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155402\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104078\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262767\n",
      "2021-10-11 02:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 182 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.90it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.80it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 11.57it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.26it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.72it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.91it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.02it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.75it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.38it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.85it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.46it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.42it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.26it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.38it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.33it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.13it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.33it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.43it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.81it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.88it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.69it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.81it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.72it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.95it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.93it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.91it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 20.01it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.75it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 20.01it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  96%|#####7| 78/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 21.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:20:13 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 7.133 | nll_loss 5.679 | ppl 51.23 | wps 49828.3 | wpb 2494.5 | bsz 169.9 | num_updates 54985 | best_loss 6.688\n",
      "2021-10-11 02:20:13 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:20:16 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint182.pt (epoch 182 @ 54985 updates, score 7.133) (writing took 3.25666036399889 seconds)\n",
      "2021-10-11 02:20:16 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)\n",
      "2021-10-11 02:20:16 | INFO | train | epoch 182 | loss 4.276 | nll_loss 2.519 | ppl 5.73 | wps 22421.7 | ups 6.37 | wpb 3520.3 | bsz 208.9 | num_updates 54985 | lr 0.000134858 | gnorm 14.357 | loss_scale 0.0312 | train_wall 301 | wall 0\n",
      "2021-10-11 02:20:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=183/shard_epoch=27\n",
      "2021-10-11 02:20:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=183/shard_epoch=28\n",
      "2021-10-11 02:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:20:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006533\n",
      "2021-10-11 02:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139111\n",
      "2021-10-11 02:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:16 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[112600, 413166, 342646, 250553, 275342, 11966, 89259, 110019, 75473, 208689]\n",
      "2021-10-11 02:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011108\n",
      "2021-10-11 02:20:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.236603\n",
      "2021-10-11 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.387715\n",
      "2021-10-11 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113531\n",
      "2021-10-11 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:18 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[112600, 413166, 342646, 250553, 275342, 11966, 89259, 110019, 75473, 208689]\n",
      "2021-10-11 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010959\n",
      "2021-10-11 02:20:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.234975\n",
      "2021-10-11 02:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.360349\n",
      "2021-10-11 02:20:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 183:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:20:19 | INFO | fairseq.trainer | begin training epoch 183\n",
      "epoch 183: 100%|9| 2036/2037 [05:08<00:00,  6.36it/s, loss=4.343, nll_loss=2.5782021-10-11 02:25:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003252\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.172359\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.132882\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.309287\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003002\n",
      "2021-10-11 02:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.172632\n",
      "2021-10-11 02:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.132035\n",
      "2021-10-11 02:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.308289\n",
      "2021-10-11 02:25:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 183 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.01it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.89it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.66it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.30it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.60it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.73it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.85it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.51it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.16it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.61it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.98it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.47it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.33it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.16it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.28it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.48it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.24it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.41it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.44it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.72it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.81it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.76it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.69it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.77it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.98it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.85it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.53it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.72it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:25:33 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 7.129 | nll_loss 5.677 | ppl 51.17 | wps 49509.7 | wpb 2494.5 | bsz 169.9 | num_updates 57022 | best_loss 6.688\n",
      "2021-10-11 02:25:33 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:25:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint183.pt (epoch 183 @ 57022 updates, score 7.129) (writing took 3.3621584530010296 seconds)\n",
      "2021-10-11 02:25:36 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)\n",
      "2021-10-11 02:25:36 | INFO | train | epoch 183 | loss 4.255 | nll_loss 2.495 | ppl 5.64 | wps 22411.9 | ups 6.37 | wpb 3520.3 | bsz 208.9 | num_updates 57022 | lr 0.000132428 | gnorm 13.817 | loss_scale 0.0312 | train_wall 301 | wall 0\n",
      "2021-10-11 02:25:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=184/shard_epoch=28\n",
      "2021-10-11 02:25:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=184/shard_epoch=29\n",
      "2021-10-11 02:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:25:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006560\n",
      "2021-10-11 02:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.144327\n",
      "2021-10-11 02:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:36 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[359241, 51495, 297758, 10987, 313428, 186022, 249769, 73922, 336062, 79656]\n",
      "2021-10-11 02:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012076\n",
      "2021-10-11 02:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.638508\n",
      "2021-10-11 02:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.795896\n",
      "2021-10-11 02:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116748\n",
      "2021-10-11 02:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:38 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[359241, 51495, 297758, 10987, 313428, 186022, 249769, 73922, 336062, 79656]\n",
      "2021-10-11 02:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012640\n",
      "2021-10-11 02:25:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.648360\n",
      "2021-10-11 02:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.778732\n",
      "2021-10-11 02:25:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 184:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:25:40 | INFO | fairseq.trainer | begin training epoch 184\n",
      "epoch 184: 100%|9| 2036/2037 [05:09<00:00,  7.14it/s, loss=4.235, nll_loss=2.4732021-10-11 02:30:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003338\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157475\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103720\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265233\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002682\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.151928\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103030\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.258194\n",
      "2021-10-11 02:30:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 184 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.56it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.35it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.02it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 12.72it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.97it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.21it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.27it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.26it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.92it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.50it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.79it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.41it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.31it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.17it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.31it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.47it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.23it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.42it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.54it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.86it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.87it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.93it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.77it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.98it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.89it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.84it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.88it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.63it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.53it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.80it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.27it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:30:55 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 7.231 | nll_loss 5.79 | ppl 55.32 | wps 49307.5 | wpb 2494.5 | bsz 169.9 | num_updates 59059 | best_loss 6.688\n",
      "2021-10-11 02:30:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:30:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint184.pt (epoch 184 @ 59059 updates, score 7.231) (writing took 3.3425771520014678 seconds)\n",
      "2021-10-11 02:30:58 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)\n",
      "2021-10-11 02:30:58 | INFO | train | epoch 184 | loss 4.246 | nll_loss 2.481 | ppl 5.58 | wps 22287.4 | ups 6.33 | wpb 3520.3 | bsz 208.9 | num_updates 59059 | lr 0.000130124 | gnorm 12.776 | loss_scale 0.0312 | train_wall 302 | wall 0\n",
      "2021-10-11 02:30:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=185/shard_epoch=29\n",
      "2021-10-11 02:30:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=185/shard_epoch=30\n",
      "2021-10-11 02:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:30:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006606\n",
      "2021-10-11 02:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139094\n",
      "2021-10-11 02:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:30:58 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[370555, 123157, 63467, 202608, 136006, 385622, 5344, 184969, 71177, 308322]\n",
      "2021-10-11 02:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011489\n",
      "2021-10-11 02:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.203263\n",
      "2021-10-11 02:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.354833\n",
      "2021-10-11 02:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112481\n",
      "2021-10-11 02:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:31:00 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[370555, 123157, 63467, 202608, 136006, 385622, 5344, 184969, 71177, 308322]\n",
      "2021-10-11 02:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011254\n",
      "2021-10-11 02:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:31:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.189947\n",
      "2021-10-11 02:31:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.314548\n",
      "2021-10-11 02:31:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 185:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:31:01 | INFO | fairseq.trainer | begin training epoch 185\n",
      "epoch 185: 100%|9| 2036/2037 [05:09<00:00,  6.33it/s, loss=4.18, nll_loss=2.403,2021-10-11 02:36:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:36:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:36:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003628\n",
      "2021-10-11 02:36:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153751\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102067\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260103\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003497\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154351\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101801\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260173\n",
      "2021-10-11 02:36:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 185 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.28it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.10it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 10.87it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.59it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.12it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.38it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.54it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.38it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.05it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.56it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.21it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.22it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 18.97it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.18it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.42it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.25it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.46it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.55it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.90it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.96it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.94it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.89it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  72%|####2 | 58/81 [00:02<00:01, 19.94it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.69it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.91it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 20.00it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.82it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.96it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.34it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.81it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:36:15 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 7.149 | nll_loss 5.704 | ppl 52.14 | wps 49679.9 | wpb 2494.5 | bsz 169.9 | num_updates 61096 | best_loss 6.688\n",
      "2021-10-11 02:36:15 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:36:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint185.pt (epoch 185 @ 61096 updates, score 7.149) (writing took 3.227749532999951 seconds)\n",
      "2021-10-11 02:36:18 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)\n",
      "2021-10-11 02:36:18 | INFO | train | epoch 185 | loss 4.228 | nll_loss 2.459 | ppl 5.5 | wps 22388.9 | ups 6.36 | wpb 3520.3 | bsz 208.9 | num_updates 61096 | lr 0.000127936 | gnorm 12.833 | loss_scale 0.0312 | train_wall 302 | wall 0\n",
      "2021-10-11 02:36:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=186/shard_epoch=30\n",
      "2021-10-11 02:36:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=186/shard_epoch=31\n",
      "2021-10-11 02:36:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:36:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006559\n",
      "2021-10-11 02:36:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.136200\n",
      "2021-10-11 02:36:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:18 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[5767, 346024, 348480, 393517, 296666, 393137, 350257, 186656, 320088, 226727]\n",
      "2021-10-11 02:36:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011048\n",
      "2021-10-11 02:36:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.237871\n",
      "2021-10-11 02:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.385983\n",
      "2021-10-11 02:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111357\n",
      "2021-10-11 02:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:20 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[5767, 346024, 348480, 393517, 296666, 393137, 350257, 186656, 320088, 226727]\n",
      "2021-10-11 02:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011377\n",
      "2021-10-11 02:36:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.217730\n",
      "2021-10-11 02:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.341337\n",
      "2021-10-11 02:36:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 186:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:36:21 | INFO | fairseq.trainer | begin training epoch 186\n",
      "epoch 186: 100%|9| 2036/2037 [05:11<00:00,  6.59it/s, loss=4.316, nll_loss=2.5572021-10-11 02:41:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003802\n",
      "2021-10-11 02:41:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156204\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104304\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265026\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003486\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155898\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102835\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262938\n",
      "2021-10-11 02:41:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 186 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.53it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:09,  8.01it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:07,  9.77it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:06, 11.54it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:05, 13.17it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 14.56it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  20%|#1    | 16/81 [00:01<00:04, 15.89it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 16.76it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.52it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.15it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.60it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.26it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.27it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.17it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  43%|##5   | 35/81 [00:02<00:02, 19.25it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.42it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.22it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.41it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.52it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.87it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.92it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.93it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.91it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 19.97it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.81it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.98it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.92it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.88it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.96it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.43it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.34it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.65it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.20it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:41:37 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 7.207 | nll_loss 5.725 | ppl 52.89 | wps 47434.7 | wpb 2494.5 | bsz 169.9 | num_updates 63133 | best_loss 6.688\n",
      "2021-10-11 02:41:37 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:41:41 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint186.pt (epoch 186 @ 63133 updates, score 7.207) (writing took 3.306576787999802 seconds)\n",
      "2021-10-11 02:41:41 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)\n",
      "2021-10-11 02:41:41 | INFO | train | epoch 186 | loss 4.206 | nll_loss 2.435 | ppl 5.41 | wps 22243.3 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 63133 | lr 0.000125855 | gnorm 13.005 | loss_scale 0.0312 | train_wall 303 | wall 0\n",
      "2021-10-11 02:41:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=187/shard_epoch=31\n",
      "2021-10-11 02:41:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=187/shard_epoch=32\n",
      "2021-10-11 02:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:41:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006535\n",
      "2021-10-11 02:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.138361\n",
      "2021-10-11 02:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:41 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[115214, 227412, 420521, 63989, 398835, 128400, 319276, 145400, 2874, 173918]\n",
      "2021-10-11 02:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011631\n",
      "2021-10-11 02:41:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.317198\n",
      "2021-10-11 02:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.468194\n",
      "2021-10-11 02:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113191\n",
      "2021-10-11 02:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:42 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[115214, 227412, 420521, 63989, 398835, 128400, 319276, 145400, 2874, 173918]\n",
      "2021-10-11 02:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011117\n",
      "2021-10-11 02:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:41:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.154132\n",
      "2021-10-11 02:41:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.279332\n",
      "2021-10-11 02:41:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 187:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:41:44 | INFO | fairseq.trainer | begin training epoch 187\n",
      "epoch 187: 100%|9| 2036/2037 [05:09<00:00,  6.64it/s, loss=4.228, nll_loss=2.47,2021-10-11 02:46:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003823\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155836\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101547\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261864\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002832\n",
      "2021-10-11 02:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:46:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156062\n",
      "2021-10-11 02:46:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:46:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101903\n",
      "2021-10-11 02:46:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261318\n",
      "2021-10-11 02:46:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 187 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.90it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.78it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 11.56it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.25it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.71it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.90it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.01it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.76it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.42it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.89it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.51it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.45it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.30it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.43it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.63it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.42it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.47it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.16it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.87it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 20.04it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.82it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.97it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.92it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 20.12it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.85it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.75it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 20.01it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.81it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:46:58 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 7.132 | nll_loss 5.672 | ppl 50.99 | wps 49934 | wpb 2494.5 | bsz 169.9 | num_updates 65170 | best_loss 6.688\n",
      "2021-10-11 02:46:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:47:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint187.pt (epoch 187 @ 65170 updates, score 7.132) (writing took 3.2463173950000055 seconds)\n",
      "2021-10-11 02:47:01 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)\n",
      "2021-10-11 02:47:01 | INFO | train | epoch 187 | loss 4.188 | nll_loss 2.414 | ppl 5.33 | wps 22381.8 | ups 6.36 | wpb 3520.3 | bsz 208.9 | num_updates 65170 | lr 0.000123873 | gnorm 13.156 | loss_scale 0.0312 | train_wall 302 | wall 0\n",
      "2021-10-11 02:47:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=188/shard_epoch=32\n",
      "2021-10-11 02:47:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=188/shard_epoch=33\n",
      "2021-10-11 02:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:47:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006421\n",
      "2021-10-11 02:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.136811\n",
      "2021-10-11 02:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:47:01 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[395267, 406617, 266607, 332383, 87037, 377684, 71858, 316380, 422212, 32022]\n",
      "2021-10-11 02:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011340\n",
      "2021-10-11 02:47:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.174680\n",
      "2021-10-11 02:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.323781\n",
      "2021-10-11 02:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:47:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110477\n",
      "2021-10-11 02:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:47:03 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[395267, 406617, 266607, 332383, 87037, 377684, 71858, 316380, 422212, 32022]\n",
      "2021-10-11 02:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011065\n",
      "2021-10-11 02:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:47:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.181874\n",
      "2021-10-11 02:47:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.304382\n",
      "2021-10-11 02:47:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 188:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:47:04 | INFO | fairseq.trainer | begin training epoch 188\n",
      "epoch 188: 100%|9| 2036/2037 [05:11<00:00,  6.70it/s, loss=4.184, nll_loss=2.3982021-10-11 02:52:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:52:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:52:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003609\n",
      "2021-10-11 02:52:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156399\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103167\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263786\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002722\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.158470\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105167\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.266977\n",
      "2021-10-11 02:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 188 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.10it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.00it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.76it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 13.41it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:04, 14.73it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.90it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:03, 16.78it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.59it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.22it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.46it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.87it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 19.15it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.55it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.42it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.16it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.25it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.45it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.23it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.41it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.50it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.78it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.86it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.90it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.74it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.93it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.87it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.85it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.94it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.69it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.57it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.82it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.28it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:52:20 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 7.23 | nll_loss 5.792 | ppl 55.42 | wps 49426 | wpb 2494.5 | bsz 169.9 | num_updates 67207 | best_loss 6.688\n",
      "2021-10-11 02:52:20 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:52:23 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint188.pt (epoch 188 @ 67207 updates, score 7.23) (writing took 3.2409616350014403 seconds)\n",
      "2021-10-11 02:52:23 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)\n",
      "2021-10-11 02:52:23 | INFO | train | epoch 188 | loss 4.167 | nll_loss 2.377 | ppl 5.2 | wps 22250.4 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 67207 | lr 0.000121981 | gnorm 12.709 | loss_scale 0.0625 | train_wall 303 | wall 0\n",
      "2021-10-11 02:52:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=189/shard_epoch=33\n",
      "2021-10-11 02:52:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=189/shard_epoch=34\n",
      "2021-10-11 02:52:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:52:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007730\n",
      "2021-10-11 02:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.140200\n",
      "2021-10-11 02:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:24 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[62597, 239586, 64319, 22590, 48005, 269952, 79860, 188457, 168804, 318342]\n",
      "2021-10-11 02:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011568\n",
      "2021-10-11 02:52:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.564295\n",
      "2021-10-11 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.717042\n",
      "2021-10-11 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114203\n",
      "2021-10-11 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:25 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[62597, 239586, 64319, 22590, 48005, 269952, 79860, 188457, 168804, 318342]\n",
      "2021-10-11 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011749\n",
      "2021-10-11 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.495376\n",
      "2021-10-11 02:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.622309\n",
      "2021-10-11 02:52:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 189:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:52:27 | INFO | fairseq.trainer | begin training epoch 189\n",
      "epoch 189: 100%|9| 2036/2037 [05:09<00:00,  6.61it/s, loss=4.177, nll_loss=2.3732021-10-11 02:57:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003236\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.158026\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102186\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264246\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003010\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154421\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102836\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260811\n",
      "2021-10-11 02:57:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 189 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.95it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.80it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 11.55it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.20it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.60it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.73it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.84it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.51it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.12it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.62it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.95it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.49it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.32it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.20it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.25it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.41it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.20it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.37it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.48it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 20.00it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.90it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.88it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.92it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.74it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.93it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.87it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.87it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.97it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.69it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.56it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.28it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 02:57:42 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 7.115 | nll_loss 5.65 | ppl 50.21 | wps 49443 | wpb 2494.5 | bsz 169.9 | num_updates 69244 | best_loss 6.688\n",
      "2021-10-11 02:57:42 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 02:57:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint189.pt (epoch 189 @ 69244 updates, score 7.115) (writing took 3.3023525680000603 seconds)\n",
      "2021-10-11 02:57:45 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)\n",
      "2021-10-11 02:57:45 | INFO | train | epoch 189 | loss 4.14 | nll_loss 2.333 | ppl 5.04 | wps 22302.3 | ups 6.34 | wpb 3520.3 | bsz 208.9 | num_updates 69244 | lr 0.000120174 | gnorm 10.71 | loss_scale 0.0625 | train_wall 302 | wall 0\n",
      "2021-10-11 02:57:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=190/shard_epoch=34\n",
      "2021-10-11 02:57:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=190/shard_epoch=35\n",
      "2021-10-11 02:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:57:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006667\n",
      "2021-10-11 02:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.147369\n",
      "2021-10-11 02:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:45 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[53942, 251064, 10409, 386849, 174929, 31993, 357594, 134664, 317763, 103302]\n",
      "2021-10-11 02:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013057\n",
      "2021-10-11 02:57:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.214780\n",
      "2021-10-11 02:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.379804\n",
      "2021-10-11 02:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 02:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114288\n",
      "2021-10-11 02:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:46 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[53942, 251064, 10409, 386849, 174929, 31993, 357594, 134664, 317763, 103302]\n",
      "2021-10-11 02:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011912\n",
      "2021-10-11 02:57:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 02:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.207343\n",
      "2021-10-11 02:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.334397\n",
      "2021-10-11 02:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 190:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 02:57:48 | INFO | fairseq.trainer | begin training epoch 190\n",
      "epoch 190: 100%|9| 2036/2037 [05:11<00:00,  6.98it/s, loss=4.077, nll_loss=2.2772021-10-11 03:03:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003260\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156578\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104344\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264897\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002759\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157132\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103753\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264185\n",
      "2021-10-11 03:03:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 190 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.22it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.05it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 10.81it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.53it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.04it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.28it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.46it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.38it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 17.87it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.42it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.81it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.98it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.41it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.37it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.51it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.12it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.37it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.17it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.33it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.42it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.78it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.80it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.78it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.69it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.60it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.65it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.70it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.66it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.75it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.80it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.53it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.47it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.74it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.21it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:03:04 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 7.34 | nll_loss 5.875 | ppl 58.7 | wps 49248.8 | wpb 2494.5 | bsz 169.9 | num_updates 71281 | best_loss 6.688\n",
      "2021-10-11 03:03:04 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:03:08 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint190.pt (epoch 190 @ 71281 updates, score 7.34) (writing took 3.243248806000338 seconds)\n",
      "2021-10-11 03:03:08 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)\n",
      "2021-10-11 03:03:08 | INFO | train | epoch 190 | loss 4.113 | nll_loss 2.314 | ppl 4.97 | wps 22212.1 | ups 6.31 | wpb 3520.3 | bsz 208.9 | num_updates 71281 | lr 0.000118444 | gnorm 9.616 | loss_scale 0.0625 | train_wall 304 | wall 0\n",
      "2021-10-11 03:03:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=191/shard_epoch=35\n",
      "2021-10-11 03:03:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=191/shard_epoch=36\n",
      "2021-10-11 03:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:03:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006746\n",
      "2021-10-11 03:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139194\n",
      "2021-10-11 03:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:08 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[259790, 102682, 333977, 337073, 189866, 402465, 257674, 185455, 60513, 321118]\n",
      "2021-10-11 03:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011510\n",
      "2021-10-11 03:03:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.636131\n",
      "2021-10-11 03:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.787808\n",
      "2021-10-11 03:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.118704\n",
      "2021-10-11 03:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:10 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[259790, 102682, 333977, 337073, 189866, 402465, 257674, 185455, 60513, 321118]\n",
      "2021-10-11 03:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011747\n",
      "2021-10-11 03:03:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.231424\n",
      "2021-10-11 03:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.362776\n",
      "2021-10-11 03:03:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 191:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:03:11 | INFO | fairseq.trainer | begin training epoch 191\n",
      "epoch 191: 100%|9| 2036/2037 [05:11<00:00,  6.85it/s, loss=4.08, nll_loss=2.287,2021-10-11 03:08:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003376\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156945\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104416\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265347\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002926\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.159046\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.109275\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.271954\n",
      "2021-10-11 03:08:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 191 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.14it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  8.88it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:07, 10.51it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:06, 12.15it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.60it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.90it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.02it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.03it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.76it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.40it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.82it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.34it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:03, 13.82it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:03, 14.94it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  43%|##5   | 35/81 [00:02<00:02, 16.03it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 16.99it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 17.45it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 18.10it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 18.54it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.09it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.36it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.47it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.59it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  69%|####1 | 56/81 [00:03<00:01, 19.61it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.67it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.40it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.54it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.52it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.66it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.72it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.41it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  91%|#####4| 74/81 [00:03<00:00, 19.58it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  95%|#####7| 77/81 [00:04<00:00, 19.62it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.55it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:08:28 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 7.239 | nll_loss 5.811 | ppl 56.13 | wps 47379.1 | wpb 2494.5 | bsz 169.9 | num_updates 73318 | best_loss 6.688\n",
      "2021-10-11 03:08:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:08:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint191.pt (epoch 191 @ 73318 updates, score 7.239) (writing took 3.257684271000471 seconds)\n",
      "2021-10-11 03:08:31 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)\n",
      "2021-10-11 03:08:31 | INFO | train | epoch 191 | loss 4.1 | nll_loss 2.305 | ppl 4.94 | wps 22180.4 | ups 6.3 | wpb 3520.3 | bsz 208.9 | num_updates 73318 | lr 0.000116787 | gnorm 9.04 | loss_scale 0.0625 | train_wall 303 | wall 0\n",
      "2021-10-11 03:08:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=192/shard_epoch=36\n",
      "2021-10-11 03:08:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=192/shard_epoch=37\n",
      "2021-10-11 03:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:08:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006597\n",
      "2021-10-11 03:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.137638\n",
      "2021-10-11 03:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:31 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[73245, 345562, 54104, 404326, 224345, 399286, 377495, 407574, 362301, 229280]\n",
      "2021-10-11 03:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011083\n",
      "2021-10-11 03:08:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.236064\n",
      "2021-10-11 03:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.385705\n",
      "2021-10-11 03:08:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:08:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112288\n",
      "2021-10-11 03:08:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:33 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[73245, 345562, 54104, 404326, 224345, 399286, 377495, 407574, 362301, 229280]\n",
      "2021-10-11 03:08:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010839\n",
      "2021-10-11 03:08:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:08:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.230507\n",
      "2021-10-11 03:08:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.354491\n",
      "2021-10-11 03:08:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 192:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:08:34 | INFO | fairseq.trainer | begin training epoch 192\n",
      "epoch 192: 100%|9| 2036/2037 [05:09<00:00,  6.57it/s, loss=4.051, nll_loss=2.2642021-10-11 03:13:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003055\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154287\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103396\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261367\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002700\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157898\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101824\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262953\n",
      "2021-10-11 03:13:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 192 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.35it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07, 10.27it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.99it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.61it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.95it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 16.01it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 17.04it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.52it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.15it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.68it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 19.02it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.54it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.45it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.31it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.37it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.52it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.31it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.45it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.48it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.87it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.87it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.96it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.80it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 20.01it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.88it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 20.04it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.92it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  96%|#####7| 78/81 [00:03<00:00, 19.39it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.87it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:13:49 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 7.193 | nll_loss 5.759 | ppl 54.15 | wps 49590.5 | wpb 2494.5 | bsz 169.9 | num_updates 75355 | best_loss 6.688\n",
      "2021-10-11 03:13:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:13:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint192.pt (epoch 192 @ 75355 updates, score 7.193) (writing took 3.3029201959998318 seconds)\n",
      "2021-10-11 03:13:52 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)\n",
      "2021-10-11 03:13:52 | INFO | train | epoch 192 | loss 4.085 | nll_loss 2.294 | ppl 4.9 | wps 22338 | ups 6.35 | wpb 3520.3 | bsz 208.9 | num_updates 75355 | lr 0.000115198 | gnorm 8.879 | loss_scale 0.0625 | train_wall 302 | wall 0\n",
      "2021-10-11 03:13:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=193/shard_epoch=37\n",
      "2021-10-11 03:13:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=193/shard_epoch=38\n",
      "2021-10-11 03:13:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:13:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006698\n",
      "2021-10-11 03:13:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.140966\n",
      "2021-10-11 03:13:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:52 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[17727, 378557, 216717, 166881, 317377, 229765, 92189, 372356, 108687, 234612]\n",
      "2021-10-11 03:13:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011670\n",
      "2021-10-11 03:13:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.147221\n",
      "2021-10-11 03:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.300790\n",
      "2021-10-11 03:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115519\n",
      "2021-10-11 03:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:54 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[17727, 378557, 216717, 166881, 317377, 229765, 92189, 372356, 108687, 234612]\n",
      "2021-10-11 03:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011408\n",
      "2021-10-11 03:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.148000\n",
      "2021-10-11 03:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.275849\n",
      "2021-10-11 03:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 193:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:13:55 | INFO | fairseq.trainer | begin training epoch 193\n",
      "epoch 193: 100%|9| 2036/2037 [05:10<00:00,  6.91it/s, loss=3.902, nll_loss=2.1022021-10-11 03:19:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003911\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154676\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101591\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260847\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002873\n",
      "2021-10-11 03:19:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156028\n",
      "2021-10-11 03:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104124\n",
      "2021-10-11 03:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263588\n",
      "2021-10-11 03:19:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 193 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.74it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.62it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.38it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.05it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.50it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.64it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.77it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.45it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.09it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.57it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.94it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.11it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.45it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.34it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.51it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.15it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.36it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.22it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.30it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.36it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.76it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.72it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.86it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.70it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.88it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.82it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.84it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.55it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.50it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.76it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.26it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.71it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:19:10 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 7.201 | nll_loss 5.735 | ppl 53.25 | wps 49397.8 | wpb 2494.5 | bsz 169.9 | num_updates 77392 | best_loss 6.688\n",
      "2021-10-11 03:19:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:19:13 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint193.pt (epoch 193 @ 77392 updates, score 7.201) (writing took 3.2742214190002414 seconds)\n",
      "2021-10-11 03:19:13 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)\n",
      "2021-10-11 03:19:13 | INFO | train | epoch 193 | loss 4.064 | nll_loss 2.276 | ppl 4.84 | wps 22336 | ups 6.34 | wpb 3520.3 | bsz 208.9 | num_updates 77392 | lr 0.000113672 | gnorm 8.762 | loss_scale 0.0625 | train_wall 303 | wall 0\n",
      "2021-10-11 03:19:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=194/shard_epoch=38\n",
      "2021-10-11 03:19:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=194/shard_epoch=39\n",
      "2021-10-11 03:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:19:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006878\n",
      "2021-10-11 03:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.142170\n",
      "2021-10-11 03:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:13 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[307419, 419737, 19320, 178640, 351524, 331715, 417192, 180095, 119613, 331978]\n",
      "2021-10-11 03:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011457\n",
      "2021-10-11 03:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.179388\n",
      "2021-10-11 03:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.333878\n",
      "2021-10-11 03:19:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113340\n",
      "2021-10-11 03:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:15 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[307419, 419737, 19320, 178640, 351524, 331715, 417192, 180095, 119613, 331978]\n",
      "2021-10-11 03:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011518\n",
      "2021-10-11 03:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.178008\n",
      "2021-10-11 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.303733\n",
      "2021-10-11 03:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 194:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:19:16 | INFO | fairseq.trainer | begin training epoch 194\n",
      "epoch 194: 100%|9| 2036/2037 [05:11<00:00,  6.65it/s, loss=4.071, nll_loss=2.2982021-10-11 03:24:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003244\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.178379\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.134349\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.316829\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002875\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.170116\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.130833\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.304431\n",
      "2021-10-11 03:24:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 194 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.98it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.85it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.62it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.25it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.67it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.78it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.89it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.54it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.13it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.58it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.94it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.06it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.46it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.38it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.52it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.15it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.31it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.19it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.23it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.29it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.80it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.65it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.84it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.76it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.77it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.86it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.58it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.41it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.72it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.24it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.69it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:24:32 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 7.357 | nll_loss 5.888 | ppl 59.21 | wps 49345 | wpb 2494.5 | bsz 169.9 | num_updates 79429 | best_loss 6.688\n",
      "2021-10-11 03:24:32 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:24:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint194.pt (epoch 194 @ 79429 updates, score 7.357) (writing took 3.223727553999197 seconds)\n",
      "2021-10-11 03:24:36 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)\n",
      "2021-10-11 03:24:36 | INFO | train | epoch 194 | loss 4.04 | nll_loss 2.255 | ppl 4.77 | wps 22236.2 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 79429 | lr 0.000112205 | gnorm 8.248 | loss_scale 0.0625 | train_wall 303 | wall 0\n",
      "2021-10-11 03:24:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=195/shard_epoch=39\n",
      "2021-10-11 03:24:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=195/shard_epoch=40\n",
      "2021-10-11 03:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:24:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006396\n",
      "2021-10-11 03:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.141986\n",
      "2021-10-11 03:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:36 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[38979, 78573, 131022, 109821, 391457, 344011, 67599, 392162, 373764, 167704]\n",
      "2021-10-11 03:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012059\n",
      "2021-10-11 03:24:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.255398\n",
      "2021-10-11 03:24:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.410378\n",
      "2021-10-11 03:24:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:24:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115005\n",
      "2021-10-11 03:24:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:37 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[38979, 78573, 131022, 109821, 391457, 344011, 67599, 392162, 373764, 167704]\n",
      "2021-10-11 03:24:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012250\n",
      "2021-10-11 03:24:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.250994\n",
      "2021-10-11 03:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.379180\n",
      "2021-10-11 03:24:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 195:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:24:38 | INFO | fairseq.trainer | begin training epoch 195\n",
      "epoch 195: 100%|9| 2036/2037 [05:11<00:00,  6.38it/s, loss=4.1, nll_loss=2.327, 2021-10-11 03:29:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003341\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156114\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101049\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261171\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002889\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156545\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103851\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263808\n",
      "2021-10-11 03:29:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 195 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.46it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.29it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.05it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.73it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.19it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.37it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.51it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.19it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.86it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.39it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.82it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.40it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.34it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.18it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.29it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.44it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.25it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.43it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.53it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.86it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.84it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.80it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.76it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.61it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.66it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.72it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.76it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.77it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.43it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 19.73it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.57it/s]\u001b[A\n",
      "epoch 195 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:29:55 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 7.166 | nll_loss 5.763 | ppl 54.3 | wps 49253.2 | wpb 2494.5 | bsz 169.9 | num_updates 81466 | best_loss 6.688\n",
      "2021-10-11 03:29:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:29:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint195.pt (epoch 195 @ 81466 updates, score 7.166) (writing took 3.291563202001271 seconds)\n",
      "2021-10-11 03:29:58 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)\n",
      "2021-10-11 03:29:58 | INFO | train | epoch 195 | loss 4.019 | nll_loss 2.235 | ppl 4.71 | wps 22250 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 81466 | lr 0.000110793 | gnorm 8.423 | loss_scale 0.0625 | train_wall 303 | wall 0\n",
      "2021-10-11 03:29:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=196/shard_epoch=40\n",
      "2021-10-11 03:29:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=196/shard_epoch=41\n",
      "2021-10-11 03:29:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:29:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007555\n",
      "2021-10-11 03:29:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.148360\n",
      "2021-10-11 03:29:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:29:58 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[236201, 276872, 101842, 100445, 252424, 404242, 275777, 380580, 321695, 374349]\n",
      "2021-10-11 03:29:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011230\n",
      "2021-10-11 03:29:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:29:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.191695\n",
      "2021-10-11 03:29:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.352161\n",
      "2021-10-11 03:29:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:29:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:29:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114981\n",
      "2021-10-11 03:29:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:29:59 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[236201, 276872, 101842, 100445, 252424, 404242, 275777, 380580, 321695, 374349]\n",
      "2021-10-11 03:29:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011262\n",
      "2021-10-11 03:29:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:30:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.214384\n",
      "2021-10-11 03:30:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.341553\n",
      "2021-10-11 03:30:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 196:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:30:01 | INFO | fairseq.trainer | begin training epoch 196\n",
      "epoch 196: 100%|9| 2036/2037 [05:11<00:00,  6.55it/s, loss=3.891, nll_loss=2.1132021-10-11 03:35:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003320\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153742\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103348\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261035\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002703\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155836\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104340\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263456\n",
      "2021-10-11 03:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 196 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:09,  8.10it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.99it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 11.73it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.37it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.78it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.91it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.96it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.75it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.08it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.59it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.97it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.09it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.53it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.46it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.37it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.53it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.29it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.45it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.50it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.79it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.86it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.83it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.93it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.72it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.75it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.78it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.59it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.64it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.39it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 19.74it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.62it/s]\u001b[A\n",
      "epoch 196 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.56it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:35:17 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 6.999 | nll_loss 5.566 | ppl 47.39 | wps 49417.9 | wpb 2494.5 | bsz 169.9 | num_updates 83503 | best_loss 6.688\n",
      "2021-10-11 03:35:17 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:35:21 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint196.pt (epoch 196 @ 83503 updates, score 6.999) (writing took 3.3520966670002963 seconds)\n",
      "2021-10-11 03:35:21 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)\n",
      "2021-10-11 03:35:21 | INFO | train | epoch 196 | loss 3.924 | nll_loss 2.152 | ppl 4.44 | wps 22203.6 | ups 6.31 | wpb 3520.3 | bsz 208.9 | num_updates 83503 | lr 0.000109433 | gnorm 8.32 | loss_scale 0.125 | train_wall 304 | wall 0\n",
      "2021-10-11 03:35:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=197/shard_epoch=41\n",
      "2021-10-11 03:35:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=197/shard_epoch=42\n",
      "2021-10-11 03:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:35:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006911\n",
      "2021-10-11 03:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.142419\n",
      "2021-10-11 03:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:21 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[61951, 297674, 127853, 364837, 420884, 310884, 340664, 40785, 318001, 69165]\n",
      "2021-10-11 03:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011679\n",
      "2021-10-11 03:35:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.196990\n",
      "2021-10-11 03:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.352013\n",
      "2021-10-11 03:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114763\n",
      "2021-10-11 03:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:22 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[61951, 297674, 127853, 364837, 420884, 310884, 340664, 40785, 318001, 69165]\n",
      "2021-10-11 03:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011906\n",
      "2021-10-11 03:35:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.196375\n",
      "2021-10-11 03:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.323911\n",
      "2021-10-11 03:35:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 197:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:35:24 | INFO | fairseq.trainer | begin training epoch 197\n",
      "epoch 197: 100%|9| 2036/2037 [05:10<00:00,  6.23it/s, loss=3.902, nll_loss=2.1312021-10-11 03:40:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:40:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:40:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003552\n",
      "2021-10-11 03:40:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156919\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103996\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265251\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003467\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156061\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103718\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263832\n",
      "2021-10-11 03:40:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 197 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.35it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.11it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 10.78it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 12.35it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.71it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.07it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.15it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.19it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.95it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.29it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.82it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 19.01it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.53it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.46it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.28it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.32it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.46it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.26it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.44it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.51it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 20.01it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.95it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.95it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 20.03it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.84it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 20.03it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.95it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 20.01it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.71it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 19.98it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 197 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:40:39 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 7.017 | nll_loss 5.565 | ppl 47.34 | wps 49323.6 | wpb 2494.5 | bsz 169.9 | num_updates 85540 | best_loss 6.688\n",
      "2021-10-11 03:40:39 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:40:42 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint197.pt (epoch 197 @ 85540 updates, score 7.017) (writing took 3.265295409000828 seconds)\n",
      "2021-10-11 03:40:42 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)\n",
      "2021-10-11 03:40:42 | INFO | train | epoch 197 | loss 3.879 | nll_loss 2.1 | ppl 4.29 | wps 22300.1 | ups 6.33 | wpb 3520.3 | bsz 208.9 | num_updates 85540 | lr 0.000108122 | gnorm 9.197 | loss_scale 0.125 | train_wall 303 | wall 0\n",
      "2021-10-11 03:40:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=198/shard_epoch=42\n",
      "2021-10-11 03:40:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=198/shard_epoch=43\n",
      "2021-10-11 03:40:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:40:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006575\n",
      "2021-10-11 03:40:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.138396\n",
      "2021-10-11 03:40:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:43 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[392538, 279038, 339473, 239372, 3747, 24749, 396717, 107481, 54334, 156236]\n",
      "2021-10-11 03:40:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011937\n",
      "2021-10-11 03:40:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.245128\n",
      "2021-10-11 03:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.396378\n",
      "2021-10-11 03:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115748\n",
      "2021-10-11 03:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:44 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[392538, 279038, 339473, 239372, 3747, 24749, 396717, 107481, 54334, 156236]\n",
      "2021-10-11 03:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012418\n",
      "2021-10-11 03:40:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:40:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.253451\n",
      "2021-10-11 03:40:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.382533\n",
      "2021-10-11 03:40:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 198:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:40:45 | INFO | fairseq.trainer | begin training epoch 198\n",
      "epoch 198: 100%|9| 2036/2037 [05:10<00:00,  6.34it/s, loss=3.833, nll_loss=2.0552021-10-11 03:45:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:45:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:45:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003954\n",
      "2021-10-11 03:45:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153525\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103854\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262111\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003242\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155924\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102996\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262980\n",
      "2021-10-11 03:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 198 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.84it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:09,  8.15it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:07,  9.90it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:06, 11.66it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:05, 13.30it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 14.69it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  20%|#1    | 16/81 [00:01<00:04, 16.04it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 16.93it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.72it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.31it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.79it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.40it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.36it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.21it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  43%|##5   | 35/81 [00:02<00:02, 19.30it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.49it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.32it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:01, 19.51it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.61it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.96it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 20.03it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 20.08it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  69%|####1 | 56/81 [00:03<00:01, 20.02it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.89it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 20.06it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.95it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 20.09it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.88it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.75it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 20.01it/s]\u001b[A\n",
      "epoch 198 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.81it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:46:01 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 7.107 | nll_loss 5.701 | ppl 52.03 | wps 47466.6 | wpb 2494.5 | bsz 169.9 | num_updates 87577 | best_loss 6.688\n",
      "2021-10-11 03:46:01 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:46:05 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint198.pt (epoch 198 @ 87577 updates, score 7.107) (writing took 3.244108775999848 seconds)\n",
      "2021-10-11 03:46:05 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)\n",
      "2021-10-11 03:46:05 | INFO | train | epoch 198 | loss 3.853 | nll_loss 2.075 | ppl 4.21 | wps 22263.3 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 87577 | lr 0.000106857 | gnorm 9.119 | loss_scale 0.125 | train_wall 303 | wall 0\n",
      "2021-10-11 03:46:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=199/shard_epoch=43\n",
      "2021-10-11 03:46:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=199/shard_epoch=44\n",
      "2021-10-11 03:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:46:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006586\n",
      "2021-10-11 03:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139104\n",
      "2021-10-11 03:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:46:05 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[117668, 384563, 137576, 375922, 305281, 116907, 177897, 41704, 233388, 424870]\n",
      "2021-10-11 03:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011631\n",
      "2021-10-11 03:46:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.319811\n",
      "2021-10-11 03:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.471406\n",
      "2021-10-11 03:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115462\n",
      "2021-10-11 03:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:46:06 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[117668, 384563, 137576, 375922, 305281, 116907, 177897, 41704, 233388, 424870]\n",
      "2021-10-11 03:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011611\n",
      "2021-10-11 03:46:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:46:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.219402\n",
      "2021-10-11 03:46:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.347389\n",
      "2021-10-11 03:46:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 199:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:46:07 | INFO | fairseq.trainer | begin training epoch 199\n",
      "epoch 199: 100%|9| 2036/2037 [05:11<00:00,  6.59it/s, loss=3.838, nll_loss=2.0612021-10-11 03:51:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003831\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156891\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102193\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263646\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002730\n",
      "2021-10-11 03:51:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154811\n",
      "2021-10-11 03:51:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.100917\n",
      "2021-10-11 03:51:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.259014\n",
      "2021-10-11 03:51:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 199 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.89it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:07,  9.77it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.53it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 13.19it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.60it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.73it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.85it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.52it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 18.14it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.59it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.97it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.13it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.50it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.40it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.54it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.08it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.29it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.19it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.27it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.31it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.73it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.78it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.84it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.69it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.90it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.78it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.76it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.55it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.49it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.77it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.23it/s]\u001b[A\n",
      "epoch 199 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.67it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:51:24 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 7.054 | nll_loss 5.641 | ppl 49.89 | wps 49304.4 | wpb 2494.5 | bsz 169.9 | num_updates 89614 | best_loss 6.688\n",
      "2021-10-11 03:51:24 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:51:27 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint199.pt (epoch 199 @ 89614 updates, score 7.054) (writing took 3.2981648050008516 seconds)\n",
      "2021-10-11 03:51:27 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)\n",
      "2021-10-11 03:51:27 | INFO | train | epoch 199 | loss 3.833 | nll_loss 2.055 | ppl 4.16 | wps 22220.7 | ups 6.31 | wpb 3520.3 | bsz 208.9 | num_updates 89614 | lr 0.000105636 | gnorm 8.769 | loss_scale 0.125 | train_wall 304 | wall 0\n",
      "2021-10-11 03:51:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=200/shard_epoch=44\n",
      "2021-10-11 03:51:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=200/shard_epoch=45\n",
      "2021-10-11 03:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:51:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006625\n",
      "2021-10-11 03:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.142540\n",
      "2021-10-11 03:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:27 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[145454, 103082, 367997, 105320, 229748, 394570, 302757, 61551, 15992, 294453]\n",
      "2021-10-11 03:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011493\n",
      "2021-10-11 03:51:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.169863\n",
      "2021-10-11 03:51:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.324780\n",
      "2021-10-11 03:51:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:51:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115488\n",
      "2021-10-11 03:51:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:29 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[145454, 103082, 367997, 105320, 229748, 394570, 302757, 61551, 15992, 294453]\n",
      "2021-10-11 03:51:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011784\n",
      "2021-10-11 03:51:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.171731\n",
      "2021-10-11 03:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.299872\n",
      "2021-10-11 03:51:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 200:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:51:30 | INFO | fairseq.trainer | begin training epoch 200\n",
      "epoch 200: 100%|9| 2036/2037 [05:10<00:00,  6.86it/s, loss=3.812, nll_loss=2.0382021-10-11 03:56:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003702\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154208\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102275\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260760\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002779\n",
      "2021-10-11 03:56:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156943\n",
      "2021-10-11 03:56:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101968\n",
      "2021-10-11 03:56:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262432\n",
      "2021-10-11 03:56:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 200 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.55it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.41it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.18it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.88it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.35it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.50it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.64it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.14it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.84it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.34it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.78it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.30it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.22it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.00it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.12it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.35it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.12it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.26it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.36it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.69it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.73it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.55it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.74it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.76it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.87it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.66it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.53it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.82it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.28it/s]\u001b[A\n",
      "epoch 200 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 03:56:45 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 7.059 | nll_loss 5.684 | ppl 51.42 | wps 49277.5 | wpb 2494.5 | bsz 169.9 | num_updates 91651 | best_loss 6.688\n",
      "2021-10-11 03:56:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 03:56:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint200.pt (epoch 200 @ 91651 updates, score 7.059) (writing took 3.2648908920000395 seconds)\n",
      "2021-10-11 03:56:48 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)\n",
      "2021-10-11 03:56:48 | INFO | train | epoch 200 | loss 3.817 | nll_loss 2.039 | ppl 4.11 | wps 22340.4 | ups 6.35 | wpb 3520.3 | bsz 208.9 | num_updates 91651 | lr 0.000104456 | gnorm 8.625 | loss_scale 0.125 | train_wall 302 | wall 0\n",
      "2021-10-11 03:56:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=201/shard_epoch=45\n",
      "2021-10-11 03:56:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=201/shard_epoch=46\n",
      "2021-10-11 03:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:56:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006715\n",
      "2021-10-11 03:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.142386\n",
      "2021-10-11 03:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:48 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[144048, 230721, 4359, 371182, 292098, 172953, 188020, 101313, 390816, 73279]\n",
      "2021-10-11 03:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012023\n",
      "2021-10-11 03:56:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.251166\n",
      "2021-10-11 03:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.406438\n",
      "2021-10-11 03:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 03:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113513\n",
      "2021-10-11 03:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:50 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[144048, 230721, 4359, 371182, 292098, 172953, 188020, 101313, 390816, 73279]\n",
      "2021-10-11 03:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012120\n",
      "2021-10-11 03:56:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 03:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.239930\n",
      "2021-10-11 03:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.366420\n",
      "2021-10-11 03:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 201:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 03:56:51 | INFO | fairseq.trainer | begin training epoch 201\n",
      "epoch 201: 100%|9| 2036/2037 [05:10<00:00,  6.61it/s, loss=3.714, nll_loss=1.9262021-10-11 04:02:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003403\n",
      "2021-10-11 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154285\n",
      "2021-10-11 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102858\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261194\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002821\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154443\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103042\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260867\n",
      "2021-10-11 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 201 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.16it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  8.98it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 10.74it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.46it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 13.98it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.26it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.42it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.28it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 17.98it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.51it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.09it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.11it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.03it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.16it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.32it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.00it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.29it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.44it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.76it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.80it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.90it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.73it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.79it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.69it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.78it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.82it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.60it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.52it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.80it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.27it/s]\u001b[A\n",
      "epoch 201 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:02:06 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 7.005 | nll_loss 5.621 | ppl 49.21 | wps 49391.3 | wpb 2494.5 | bsz 169.9 | num_updates 93688 | best_loss 6.688\n",
      "2021-10-11 04:02:06 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:02:09 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint201.pt (epoch 201 @ 93688 updates, score 7.005) (writing took 3.20946616800029 seconds)\n",
      "2021-10-11 04:02:09 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)\n",
      "2021-10-11 04:02:09 | INFO | train | epoch 201 | loss 3.801 | nll_loss 2.023 | ppl 4.06 | wps 22335.5 | ups 6.34 | wpb 3520.3 | bsz 208.9 | num_updates 93688 | lr 0.000103314 | gnorm 8.493 | loss_scale 0.125 | train_wall 302 | wall 0\n",
      "2021-10-11 04:02:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=202/shard_epoch=46\n",
      "2021-10-11 04:02:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=202/shard_epoch=47\n",
      "2021-10-11 04:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:02:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006548\n",
      "2021-10-11 04:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139297\n",
      "2021-10-11 04:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:09 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[7668, 132818, 1177, 182684, 191367, 355853, 279039, 151027, 419950, 169105]\n",
      "2021-10-11 04:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011312\n",
      "2021-10-11 04:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.204825\n",
      "2021-10-11 04:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.356349\n",
      "2021-10-11 04:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111033\n",
      "2021-10-11 04:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:11 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[7668, 132818, 1177, 182684, 191367, 355853, 279039, 151027, 419950, 169105]\n",
      "2021-10-11 04:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011216\n",
      "2021-10-11 04:02:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.201115\n",
      "2021-10-11 04:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.324267\n",
      "2021-10-11 04:02:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 202:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:02:12 | INFO | fairseq.trainer | begin training epoch 202\n",
      "epoch 202: 100%|9| 2036/2037 [05:12<00:00,  6.72it/s, loss=3.898, nll_loss=2.1422021-10-11 04:07:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003428\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157235\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103642\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265020\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002816\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154389\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103421\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261171\n",
      "2021-10-11 04:07:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 202 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.00it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  8.75it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:07, 10.45it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.10it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:05, 13.63it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 14.92it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:04, 16.17it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 16.97it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.70it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.32it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.79it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.86it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.27it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.17it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.15it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.33it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.16it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.37it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.49it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.80it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.87it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.84it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.94it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.79it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.95it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.88it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.86it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.93it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.60it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.55it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.85it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.33it/s]\u001b[A\n",
      "epoch 202 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.83it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:07:29 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 7.033 | nll_loss 5.621 | ppl 49.21 | wps 49200.2 | wpb 2494.5 | bsz 169.9 | num_updates 95725 | best_loss 6.688\n",
      "2021-10-11 04:07:29 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:07:33 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint202.pt (epoch 202 @ 95725 updates, score 7.033) (writing took 3.260598197000945 seconds)\n",
      "2021-10-11 04:07:33 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)\n",
      "2021-10-11 04:07:33 | INFO | train | epoch 202 | loss 3.789 | nll_loss 2.011 | ppl 4.03 | wps 22172.8 | ups 6.3 | wpb 3520.3 | bsz 208.9 | num_updates 95725 | lr 0.000102209 | gnorm 8.415 | loss_scale 0.125 | train_wall 304 | wall 0\n",
      "2021-10-11 04:07:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=203/shard_epoch=47\n",
      "2021-10-11 04:07:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=203/shard_epoch=48\n",
      "2021-10-11 04:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:07:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007635\n",
      "2021-10-11 04:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.143519\n",
      "2021-10-11 04:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:33 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[240099, 379166, 146942, 394615, 85713, 155850, 336117, 314593, 293007, 297296]\n",
      "2021-10-11 04:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011823\n",
      "2021-10-11 04:07:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.191610\n",
      "2021-10-11 04:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.347817\n",
      "2021-10-11 04:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116734\n",
      "2021-10-11 04:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:34 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[240099, 379166, 146942, 394615, 85713, 155850, 336117, 314593, 293007, 297296]\n",
      "2021-10-11 04:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011808\n",
      "2021-10-11 04:07:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.191136\n",
      "2021-10-11 04:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.320579\n",
      "2021-10-11 04:07:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 203:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:07:35 | INFO | fairseq.trainer | begin training epoch 203\n",
      "epoch 203: 100%|9| 2036/2037 [05:11<00:00,  6.90it/s, loss=3.779, nll_loss=2.0012021-10-11 04:12:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003175\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156954\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103314\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264275\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003009\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156774\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104051\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264440\n",
      "2021-10-11 04:12:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 203 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.32it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.16it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 10.93it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.62it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.12it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.35it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.53it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.34it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.02it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.54it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.17it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:03, 13.54it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:03, 14.62it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 15.78it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  46%|##7   | 37/81 [00:02<00:02, 16.84it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 17.41it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 17.77it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:02, 18.20it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 18.53it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.16it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.29it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.26it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.41it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 19.55it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.45it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.69it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.58it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.59it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.47it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.42it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.67it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.09it/s]\u001b[A\n",
      "epoch 203 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:12:52 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 7.187 | nll_loss 5.799 | ppl 55.67 | wps 47394.3 | wpb 2494.5 | bsz 169.9 | num_updates 97762 | best_loss 6.688\n",
      "2021-10-11 04:12:52 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:12:55 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint203.pt (epoch 203 @ 97762 updates, score 7.187) (writing took 3.2672217350009305 seconds)\n",
      "2021-10-11 04:12:55 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)\n",
      "2021-10-11 04:12:55 | INFO | train | epoch 203 | loss 3.777 | nll_loss 1.999 | ppl 4 | wps 22254.5 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 97762 | lr 0.000101138 | gnorm 8.117 | loss_scale 0.125 | train_wall 303 | wall 0\n",
      "2021-10-11 04:12:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=204/shard_epoch=48\n",
      "2021-10-11 04:12:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=204/shard_epoch=49\n",
      "2021-10-11 04:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:12:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006672\n",
      "2021-10-11 04:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.141309\n",
      "2021-10-11 04:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:55 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[62361, 236611, 344722, 416373, 168701, 135859, 419349, 18255, 387218, 162719]\n",
      "2021-10-11 04:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012229\n",
      "2021-10-11 04:12:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.245312\n",
      "2021-10-11 04:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.399784\n",
      "2021-10-11 04:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115460\n",
      "2021-10-11 04:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:57 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[62361, 236611, 344722, 416373, 168701, 135859, 419349, 18255, 387218, 162719]\n",
      "2021-10-11 04:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011656\n",
      "2021-10-11 04:12:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.246432\n",
      "2021-10-11 04:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.374488\n",
      "2021-10-11 04:12:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 204:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:12:58 | INFO | fairseq.trainer | begin training epoch 204\n",
      "epoch 204: 100%|9| 2036/2037 [05:11<00:00,  6.38it/s, loss=3.703, nll_loss=1.9532021-10-11 04:18:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003077\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155993\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103465\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263158\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002740\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154282\n",
      "2021-10-11 04:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102456\n",
      "2021-10-11 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260036\n",
      "2021-10-11 04:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 204 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.60it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.46it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.15it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.84it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.29it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.44it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.59it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.28it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.93it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.35it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.79it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.82it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.24it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.16it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.34it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 18.98it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.25it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.04it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.20it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.24it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.55it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.67it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.65it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.66it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.76it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.62it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.67it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.71it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.65it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.35it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  91%|#####4| 74/81 [00:03<00:00, 19.51it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.53it/s]\u001b[A\n",
      "epoch 204 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.45it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:18:14 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 7.023 | nll_loss 5.649 | ppl 50.17 | wps 48948.5 | wpb 2494.5 | bsz 169.9 | num_updates 99799 | best_loss 6.688\n",
      "2021-10-11 04:18:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:18:17 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint204.pt (epoch 204 @ 99799 updates, score 7.023) (writing took 3.4506820930000686 seconds)\n",
      "2021-10-11 04:18:17 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)\n",
      "2021-10-11 04:18:17 | INFO | train | epoch 204 | loss 3.745 | nll_loss 1.984 | ppl 3.95 | wps 22237.1 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 99799 | lr 0.000100101 | gnorm 6.221 | loss_scale 0.25 | train_wall 303 | wall 0\n",
      "2021-10-11 04:18:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=205/shard_epoch=49\n",
      "2021-10-11 04:18:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=205/shard_epoch=50\n",
      "2021-10-11 04:18:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:18:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006566\n",
      "2021-10-11 04:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139233\n",
      "2021-10-11 04:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:18 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[329612, 74236, 180021, 423801, 263323, 367747, 153696, 161591, 188974, 153969]\n",
      "2021-10-11 04:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012305\n",
      "2021-10-11 04:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.163558\n",
      "2021-10-11 04:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.316102\n",
      "2021-10-11 04:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116082\n",
      "2021-10-11 04:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:19 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[329612, 74236, 180021, 423801, 263323, 367747, 153696, 161591, 188974, 153969]\n",
      "2021-10-11 04:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011133\n",
      "2021-10-11 04:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.163653\n",
      "2021-10-11 04:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.291754\n",
      "2021-10-11 04:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 205:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:18:20 | INFO | fairseq.trainer | begin training epoch 205\n",
      "epoch 205: 100%|9| 2036/2037 [05:10<00:00,  6.74it/s, loss=3.789, nll_loss=2.0482021-10-11 04:23:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003735\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154446\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101833\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260656\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002682\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154368\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.100979\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.258566\n",
      "2021-10-11 04:23:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 205 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.61it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.45it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.21it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.88it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.31it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.46it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  19%|#1    | 15/81 [00:00<00:03, 16.59it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.37it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.01it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.27it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.75it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.08it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.16it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.49it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.39it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.52it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.10it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.26it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.16it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.16it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.26it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.69it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.63it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.68it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.74it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.60it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.68it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.69it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.62it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.72it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.78it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.47it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.34it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.60it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.06it/s]\u001b[A\n",
      "epoch 205 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.51it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:23:35 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 7.042 | nll_loss 5.653 | ppl 50.3 | wps 48964 | wpb 2494.5 | bsz 169.9 | num_updates 101836 | best_loss 6.688\n",
      "2021-10-11 04:23:35 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:23:39 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint205.pt (epoch 205 @ 101836 updates, score 7.042) (writing took 3.1764477729993814 seconds)\n",
      "2021-10-11 04:23:39 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)\n",
      "2021-10-11 04:23:39 | INFO | train | epoch 205 | loss 3.722 | nll_loss 1.969 | ppl 3.91 | wps 22326.6 | ups 6.34 | wpb 3520.3 | bsz 208.9 | num_updates 101836 | lr 9.90945e-05 | gnorm 5.412 | loss_scale 0.25 | train_wall 303 | wall 0\n",
      "2021-10-11 04:23:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=206/shard_epoch=50\n",
      "2021-10-11 04:23:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=206/shard_epoch=51\n",
      "2021-10-11 04:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:23:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006798\n",
      "2021-10-11 04:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.143489\n",
      "2021-10-11 04:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:39 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[62655, 3701, 264972, 303348, 407949, 137434, 315490, 142192, 317175, 203786]\n",
      "2021-10-11 04:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011186\n",
      "2021-10-11 04:23:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.459675\n",
      "2021-10-11 04:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.615255\n",
      "2021-10-11 04:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112872\n",
      "2021-10-11 04:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:40 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[62655, 3701, 264972, 303348, 407949, 137434, 315490, 142192, 317175, 203786]\n",
      "2021-10-11 04:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012091\n",
      "2021-10-11 04:23:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:23:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.170958\n",
      "2021-10-11 04:23:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.296880\n",
      "2021-10-11 04:23:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 206:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:23:42 | INFO | fairseq.trainer | begin training epoch 206\n",
      "epoch 206: 100%|9| 2036/2037 [05:13<00:00,  6.44it/s, loss=3.768, nll_loss=2.0252021-10-11 04:28:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003250\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157264\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102415\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263546\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002871\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157904\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.107840\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.269251\n",
      "2021-10-11 04:28:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 206 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.51it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.35it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.12it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.81it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.27it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.44it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  19%|#1    | 15/81 [00:00<00:03, 16.56it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.29it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.91it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.19it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.64it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.93it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.01it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.40it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.33it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.50it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.07it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.25it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.10it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.13it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.23it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.73it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.65it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.65it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.66it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  69%|####1 | 56/81 [00:02<00:01, 19.76it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.73it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.43it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.64it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.53it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.61it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.64it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.42it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.34it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.61it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.10it/s]\u001b[A\n",
      "epoch 206 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.55it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:29:00 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 7.025 | nll_loss 5.642 | ppl 49.95 | wps 48919.9 | wpb 2494.5 | bsz 169.9 | num_updates 103873 | best_loss 6.688\n",
      "2021-10-11 04:29:00 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:29:03 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint206.pt (epoch 206 @ 103873 updates, score 7.025) (writing took 3.178299932998925 seconds)\n",
      "2021-10-11 04:29:03 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)\n",
      "2021-10-11 04:29:03 | INFO | train | epoch 206 | loss 3.713 | nll_loss 1.96 | ppl 3.89 | wps 22116.4 | ups 6.28 | wpb 3520.3 | bsz 208.9 | num_updates 103873 | lr 9.8118e-05 | gnorm 5.464 | loss_scale 0.25 | train_wall 305 | wall 0\n",
      "2021-10-11 04:29:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=207/shard_epoch=51\n",
      "2021-10-11 04:29:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=207/shard_epoch=52\n",
      "2021-10-11 04:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:29:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006539\n",
      "2021-10-11 04:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.142983\n",
      "2021-10-11 04:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:29:03 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[418354, 281669, 230684, 309654, 367782, 324337, 339453, 238645, 194577, 36024]\n",
      "2021-10-11 04:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011225\n",
      "2021-10-11 04:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.235272\n",
      "2021-10-11 04:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.390372\n",
      "2021-10-11 04:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114090\n",
      "2021-10-11 04:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:29:04 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[418354, 281669, 230684, 309654, 367782, 324337, 339453, 238645, 194577, 36024]\n",
      "2021-10-11 04:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011107\n",
      "2021-10-11 04:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:29:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.238256\n",
      "2021-10-11 04:29:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.364313\n",
      "2021-10-11 04:29:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 207:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:29:06 | INFO | fairseq.trainer | begin training epoch 207\n",
      "epoch 207: 100%|9| 2036/2037 [05:12<00:00,  6.36it/s, loss=3.731, nll_loss=1.9832021-10-11 04:34:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003274\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156944\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103557\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.264530\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002908\n",
      "2021-10-11 04:34:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155503\n",
      "2021-10-11 04:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104805\n",
      "2021-10-11 04:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263814\n",
      "2021-10-11 04:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 207 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.73it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.61it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 11.38it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.03it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.42it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.60it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.67it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.48it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.15it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.62it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.25it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 18.87it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 18.81it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 18.99it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.23it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.05it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.29it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.40it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.70it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.79it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.78it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.79it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.90it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.72it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.77it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.77it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.85it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.52it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.43it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.17it/s]\u001b[A\n",
      "epoch 207 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:34:23 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 7.068 | nll_loss 5.697 | ppl 51.87 | wps 49239.3 | wpb 2494.5 | bsz 169.9 | num_updates 105910 | best_loss 6.688\n",
      "2021-10-11 04:34:23 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:34:26 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint207.pt (epoch 207 @ 105910 updates, score 7.068) (writing took 3.20691422100208 seconds)\n",
      "2021-10-11 04:34:26 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)\n",
      "2021-10-11 04:34:26 | INFO | train | epoch 207 | loss 3.705 | nll_loss 1.95 | ppl 3.86 | wps 22179.4 | ups 6.3 | wpb 3520.3 | bsz 208.9 | num_updates 105910 | lr 9.71698e-05 | gnorm 5.295 | loss_scale 0.25 | train_wall 304 | wall 0\n",
      "2021-10-11 04:34:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=208/shard_epoch=52\n",
      "2021-10-11 04:34:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=208/shard_epoch=53\n",
      "2021-10-11 04:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:34:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006618\n",
      "2021-10-11 04:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.140222\n",
      "2021-10-11 04:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:26 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[191953, 375929, 311658, 283140, 97431, 182870, 199828, 95778, 41179, 215086]\n",
      "2021-10-11 04:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011301\n",
      "2021-10-11 04:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.205443\n",
      "2021-10-11 04:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.357886\n",
      "2021-10-11 04:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111496\n",
      "2021-10-11 04:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:28 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[191953, 375929, 311658, 283140, 97431, 182870, 199828, 95778, 41179, 215086]\n",
      "2021-10-11 04:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011267\n",
      "2021-10-11 04:34:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.205505\n",
      "2021-10-11 04:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.329171\n",
      "2021-10-11 04:34:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 208:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:34:29 | INFO | fairseq.trainer | begin training epoch 208\n",
      "epoch 208: 100%|9| 2036/2037 [05:12<00:00,  6.68it/s, loss=3.723, nll_loss=1.9652021-10-11 04:39:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003405\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156779\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102748\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263557\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002740\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157410\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102905\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263571\n",
      "2021-10-11 04:39:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 208 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.24it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.06it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 10.83it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 12.55it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 13.98it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.25it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.25it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.20it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.94it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.28it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.74it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.09it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.16it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.50it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.37it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.36it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.06it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.31it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.28it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.39it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.46it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.98it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.88it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.86it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.92it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.77it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.95it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.87it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.87it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.96it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.69it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.60it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.87it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.31it/s]\u001b[A\n",
      "epoch 208 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:39:47 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 7.097 | nll_loss 5.744 | ppl 53.6 | wps 49335.4 | wpb 2494.5 | bsz 169.9 | num_updates 107947 | best_loss 6.688\n",
      "2021-10-11 04:39:47 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:39:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint208.pt (epoch 208 @ 107947 updates, score 7.097) (writing took 3.2086973100012983 seconds)\n",
      "2021-10-11 04:39:50 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)\n",
      "2021-10-11 04:39:50 | INFO | train | epoch 208 | loss 3.698 | nll_loss 1.942 | ppl 3.84 | wps 22150.7 | ups 6.29 | wpb 3520.3 | bsz 208.9 | num_updates 107947 | lr 9.62487e-05 | gnorm 5.137 | loss_scale 0.25 | train_wall 305 | wall 0\n",
      "2021-10-11 04:39:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=209/shard_epoch=53\n",
      "2021-10-11 04:39:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=209/shard_epoch=54\n",
      "2021-10-11 04:39:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:39:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007727\n",
      "2021-10-11 04:39:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.142675\n",
      "2021-10-11 04:39:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:50 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[70812, 149180, 424747, 410227, 54632, 272024, 425038, 305124, 352380, 64166]\n",
      "2021-10-11 04:39:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012280\n",
      "2021-10-11 04:39:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.201864\n",
      "2021-10-11 04:39:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.357703\n",
      "2021-10-11 04:39:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:39:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116762\n",
      "2021-10-11 04:39:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:51 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[70812, 149180, 424747, 410227, 54632, 272024, 425038, 305124, 352380, 64166]\n",
      "2021-10-11 04:39:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011824\n",
      "2021-10-11 04:39:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.193561\n",
      "2021-10-11 04:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.323062\n",
      "2021-10-11 04:39:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 209:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:39:53 | INFO | fairseq.trainer | begin training epoch 209\n",
      "epoch 209: 100%|9| 2036/2037 [05:13<00:00,  6.43it/s, loss=3.796, nll_loss=2.0512021-10-11 04:45:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003670\n",
      "2021-10-11 04:45:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156054\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102790\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263195\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003310\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.161635\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102805\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.268597\n",
      "2021-10-11 04:45:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 209 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.13it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  8.93it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:07, 10.69it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.40it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:05, 13.90it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.13it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.32it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.10it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.78it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.35it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.80it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.96it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.33it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.29it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.24it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  46%|##7   | 37/81 [00:01<00:02, 19.43it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.31it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.25it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.30it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.30it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.68it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.66it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.69it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.53it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.63it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.64it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.36it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  91%|#####4| 74/81 [00:03<00:00, 19.52it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.57it/s]\u001b[A\n",
      "epoch 209 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:45:11 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 7.102 | nll_loss 5.711 | ppl 52.4 | wps 49057.1 | wpb 2494.5 | bsz 169.9 | num_updates 109984 | best_loss 6.688\n",
      "2021-10-11 04:45:11 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:45:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint209.pt (epoch 209 @ 109984 updates, score 7.102) (writing took 3.239672833999066 seconds)\n",
      "2021-10-11 04:45:14 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)\n",
      "2021-10-11 04:45:14 | INFO | train | epoch 209 | loss 3.694 | nll_loss 1.937 | ppl 3.83 | wps 22095.2 | ups 6.28 | wpb 3520.3 | bsz 208.9 | num_updates 109984 | lr 9.53532e-05 | gnorm 5.095 | loss_scale 0.25 | train_wall 305 | wall 0\n",
      "2021-10-11 04:45:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=210/shard_epoch=54\n",
      "2021-10-11 04:45:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=210/shard_epoch=55\n",
      "2021-10-11 04:45:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:45:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006556\n",
      "2021-10-11 04:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139212\n",
      "2021-10-11 04:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:15 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[147255, 93826, 244682, 415466, 239941, 121475, 233244, 288540, 43465, 299385]\n",
      "2021-10-11 04:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012588\n",
      "2021-10-11 04:45:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.229841\n",
      "2021-10-11 04:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.382497\n",
      "2021-10-11 04:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113521\n",
      "2021-10-11 04:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:16 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[147255, 93826, 244682, 415466, 239941, 121475, 233244, 288540, 43465, 299385]\n",
      "2021-10-11 04:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011839\n",
      "2021-10-11 04:45:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.236913\n",
      "2021-10-11 04:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.363136\n",
      "2021-10-11 04:45:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 210:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:45:17 | INFO | fairseq.trainer | begin training epoch 210\n",
      "epoch 210: 100%|9| 2036/2037 [05:11<00:00,  6.46it/s, loss=3.7, nll_loss=1.948, 2021-10-11 04:50:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003702\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.161868\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103257\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.269502\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002928\n",
      "2021-10-11 04:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156233\n",
      "2021-10-11 04:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101647\n",
      "2021-10-11 04:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261361\n",
      "2021-10-11 04:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 210 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  6.74it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:10,  7.36it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:08,  9.07it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:06, 10.84it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:05, 12.52it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 13.97it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  20%|#1    | 16/81 [00:01<00:04, 15.39it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 16.35it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.23it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 17.97it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:03, 18.52it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.82it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.29it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.27it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  43%|##5   | 35/81 [00:02<00:02, 19.24it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.41it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.22it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.35it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.45it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.55it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.58it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  67%|####  | 54/81 [00:03<00:01, 19.58it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 19.72it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.65it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.86it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.81it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.84it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.59it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.52it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.77it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.22it/s]\u001b[A\n",
      "epoch 210 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:50:34 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 7.128 | nll_loss 5.76 | ppl 54.18 | wps 47155.3 | wpb 2494.5 | bsz 169.9 | num_updates 112021 | best_loss 6.688\n",
      "2021-10-11 04:50:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:50:37 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint210.pt (epoch 210 @ 112021 updates, score 7.128) (writing took 3.293147346998012 seconds)\n",
      "2021-10-11 04:50:37 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)\n",
      "2021-10-11 04:50:37 | INFO | train | epoch 210 | loss 3.692 | nll_loss 1.934 | ppl 3.82 | wps 22200.7 | ups 6.31 | wpb 3520.3 | bsz 208.9 | num_updates 112021 | lr 9.44823e-05 | gnorm 5.205 | loss_scale 0.25 | train_wall 304 | wall 0\n",
      "2021-10-11 04:50:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=211/shard_epoch=55\n",
      "2021-10-11 04:50:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=211/shard_epoch=56\n",
      "2021-10-11 04:50:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:50:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007814\n",
      "2021-10-11 04:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139387\n",
      "2021-10-11 04:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:38 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[95495, 238303, 119191, 407339, 299343, 78140, 422588, 52220, 184860, 258053]\n",
      "2021-10-11 04:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012471\n",
      "2021-10-11 04:50:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.347365\n",
      "2021-10-11 04:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.500097\n",
      "2021-10-11 04:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113536\n",
      "2021-10-11 04:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:39 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[95495, 238303, 119191, 407339, 299343, 78140, 422588, 52220, 184860, 258053]\n",
      "2021-10-11 04:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012135\n",
      "2021-10-11 04:50:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.164864\n",
      "2021-10-11 04:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.291397\n",
      "2021-10-11 04:50:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 211:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:50:40 | INFO | fairseq.trainer | begin training epoch 211\n",
      "epoch 211: 100%|9| 2036/2037 [05:13<00:00,  6.36it/s, loss=3.672, nll_loss=1.91,2021-10-11 04:55:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003739\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154739\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102218\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261337\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002814\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154425\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101715\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.259508\n",
      "2021-10-11 04:55:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 211 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.22it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  8.99it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:07, 10.70it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.31it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:05, 13.85it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.12it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.35it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  22%|#3    | 18/81 [00:01<00:03, 17.11it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.83it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.37it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 18.79it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.01it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.39it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.32it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.25it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 18.89it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  48%|##8   | 39/81 [00:02<00:02, 19.18it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  51%|###   | 41/81 [00:02<00:02, 19.10it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  53%|###1  | 43/81 [00:02<00:01, 19.22it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  56%|###3  | 45/81 [00:02<00:01, 19.27it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  59%|###5  | 48/81 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.71it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.67it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.61it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.61it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.72it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.76it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.94it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.76it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.43it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 19.72it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.63it/s]\u001b[A\n",
      "epoch 211 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 04:55:58 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 7.114 | nll_loss 5.741 | ppl 53.49 | wps 49013.3 | wpb 2494.5 | bsz 169.9 | num_updates 114058 | best_loss 6.688\n",
      "2021-10-11 04:55:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 04:56:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint211.pt (epoch 211 @ 114058 updates, score 7.114) (writing took 3.213412638000591 seconds)\n",
      "2021-10-11 04:56:02 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)\n",
      "2021-10-11 04:56:02 | INFO | train | epoch 211 | loss 3.688 | nll_loss 1.929 | ppl 3.81 | wps 22123.5 | ups 6.28 | wpb 3520.3 | bsz 208.9 | num_updates 114058 | lr 9.36348e-05 | gnorm 5.142 | loss_scale 0.25 | train_wall 305 | wall 0\n",
      "2021-10-11 04:56:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=212/shard_epoch=56\n",
      "2021-10-11 04:56:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=212/shard_epoch=57\n",
      "2021-10-11 04:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:56:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006770\n",
      "2021-10-11 04:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.145956\n",
      "2021-10-11 04:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:56:02 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[260468, 110200, 317178, 405835, 359621, 185200, 333777, 291242, 201278, 316372]\n",
      "2021-10-11 04:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012513\n",
      "2021-10-11 04:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.590988\n",
      "2021-10-11 04:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.750447\n",
      "2021-10-11 04:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 04:56:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116236\n",
      "2021-10-11 04:56:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:56:04 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[260468, 110200, 317178, 405835, 359621, 185200, 333777, 291242, 201278, 316372]\n",
      "2021-10-11 04:56:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012180\n",
      "2021-10-11 04:56:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 04:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.393511\n",
      "2021-10-11 04:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.522884\n",
      "2021-10-11 04:56:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 212:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 04:56:05 | INFO | fairseq.trainer | begin training epoch 212\n",
      "epoch 212: 100%|9| 2036/2037 [05:11<00:00,  6.66it/s, loss=3.681, nll_loss=1.9172021-10-11 05:01:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003625\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154798\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103719\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262901\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002971\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.156204\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103176\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263037\n",
      "2021-10-11 05:01:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 212 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.25it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.07it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 10.84it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.52it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.03it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.23it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  19%|#1    | 15/81 [00:00<00:04, 16.40it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.17it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.87it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.17it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.66it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.02it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.13it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.39it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.22it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.21it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.39it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.12it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.29it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.36it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.67it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.76it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.73it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.71it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.79it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.64it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.69it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.73it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.67it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.37it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  91%|#####4| 74/81 [00:03<00:00, 19.54it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.56it/s]\u001b[A\n",
      "epoch 212 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 05:01:21 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 7.148 | nll_loss 5.77 | ppl 54.58 | wps 49027.5 | wpb 2494.5 | bsz 169.9 | num_updates 116095 | best_loss 6.688\n",
      "2021-10-11 05:01:21 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 05:01:25 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint212.pt (epoch 212 @ 116095 updates, score 7.148) (writing took 3.206733509003243 seconds)\n",
      "2021-10-11 05:01:25 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)\n",
      "2021-10-11 05:01:25 | INFO | train | epoch 212 | loss 3.685 | nll_loss 1.924 | ppl 3.8 | wps 22198.2 | ups 6.31 | wpb 3520.3 | bsz 208.9 | num_updates 116095 | lr 9.28097e-05 | gnorm 5.178 | loss_scale 0.5 | train_wall 304 | wall 0\n",
      "2021-10-11 05:01:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=213/shard_epoch=57\n",
      "2021-10-11 05:01:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=213/shard_epoch=58\n",
      "2021-10-11 05:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:01:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006636\n",
      "2021-10-11 05:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.144922\n",
      "2021-10-11 05:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:25 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[77752, 229594, 348584, 255375, 82304, 2363, 291906, 339384, 364543, 351941]\n",
      "2021-10-11 05:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012227\n",
      "2021-10-11 05:01:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.239575\n",
      "2021-10-11 05:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.397640\n",
      "2021-10-11 05:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115316\n",
      "2021-10-11 05:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:26 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[77752, 229594, 348584, 255375, 82304, 2363, 291906, 339384, 364543, 351941]\n",
      "2021-10-11 05:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011960\n",
      "2021-10-11 05:01:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.241383\n",
      "2021-10-11 05:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.369593\n",
      "2021-10-11 05:01:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 213:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 05:01:27 | INFO | fairseq.trainer | begin training epoch 213\n",
      "epoch 213: 100%|9| 2036/2037 [05:11<00:00,  6.46it/s, loss=3.772, nll_loss=2.0152021-10-11 05:06:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 05:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.006263\n",
      "2021-10-11 05:06:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155413\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103509\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.265915\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002919\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153828\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103556\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260858\n",
      "2021-10-11 05:06:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 213 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.57it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.43it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 11.19it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.88it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.34it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.54it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.68it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.48it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.11it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:03, 18.64it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.87it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.33it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.30it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.40it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.00it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.27it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.09it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.31it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.43it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.76it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.82it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.80it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.86it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.68it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.64it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.64it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.62it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.70it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.38it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 19.66it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.48it/s]\u001b[A\n",
      "epoch 213 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 05:06:44 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 7.158 | nll_loss 5.773 | ppl 54.69 | wps 49233.8 | wpb 2494.5 | bsz 169.9 | num_updates 118132 | best_loss 6.688\n",
      "2021-10-11 05:06:44 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 05:06:47 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint213.pt (epoch 213 @ 118132 updates, score 7.158) (writing took 3.20432383500156 seconds)\n",
      "2021-10-11 05:06:47 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)\n",
      "2021-10-11 05:06:47 | INFO | train | epoch 213 | loss 3.681 | nll_loss 1.92 | ppl 3.78 | wps 22219.3 | ups 6.31 | wpb 3520.3 | bsz 208.9 | num_updates 118132 | lr 9.2006e-05 | gnorm 4.991 | loss_scale 0.5 | train_wall 304 | wall 0\n",
      "2021-10-11 05:06:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=214/shard_epoch=58\n",
      "2021-10-11 05:06:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=214/shard_epoch=59\n",
      "2021-10-11 05:06:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:06:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006617\n",
      "2021-10-11 05:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139915\n",
      "2021-10-11 05:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:48 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[362929, 338897, 195667, 131583, 322184, 152836, 131000, 128289, 240830, 383225]\n",
      "2021-10-11 05:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011315\n",
      "2021-10-11 05:06:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.184381\n",
      "2021-10-11 05:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.336512\n",
      "2021-10-11 05:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112165\n",
      "2021-10-11 05:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:49 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[362929, 338897, 195667, 131583, 322184, 152836, 131000, 128289, 240830, 383225]\n",
      "2021-10-11 05:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011104\n",
      "2021-10-11 05:06:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.184703\n",
      "2021-10-11 05:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.308838\n",
      "2021-10-11 05:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 214:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 05:06:50 | INFO | fairseq.trainer | begin training epoch 214\n",
      "epoch 214: 100%|9| 2036/2037 [05:12<00:00,  6.74it/s, loss=3.721, nll_loss=1.9672021-10-11 05:12:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003274\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.171633\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.131711\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.307403\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002968\n",
      "2021-10-11 05:12:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.170916\n",
      "2021-10-11 05:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.123145\n",
      "2021-10-11 05:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.297709\n",
      "2021-10-11 05:12:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 214 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.16it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  8.99it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 10.76it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.49it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.02it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.26it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  20%|#1    | 16/81 [00:00<00:03, 16.48it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  22%|#3    | 18/81 [00:00<00:03, 17.25it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.90it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.44it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.80it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.36it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.26it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.10it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.19it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.36it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.12it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.28it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.32it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.66it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.73it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.72it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.74it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.67it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.72it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.77it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.73it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.86it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.36it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  91%|#####4| 74/81 [00:03<00:00, 19.51it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.61it/s]\u001b[A\n",
      "epoch 214 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 05:12:08 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 7.171 | nll_loss 5.797 | ppl 55.61 | wps 49250.3 | wpb 2494.5 | bsz 169.9 | num_updates 120169 | best_loss 6.688\n",
      "2021-10-11 05:12:08 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 05:12:11 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint214.pt (epoch 214 @ 120169 updates, score 7.171) (writing took 3.208912424997834 seconds)\n",
      "2021-10-11 05:12:11 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)\n",
      "2021-10-11 05:12:11 | INFO | train | epoch 214 | loss 3.679 | nll_loss 1.917 | ppl 3.78 | wps 22139.9 | ups 6.29 | wpb 3520.3 | bsz 208.9 | num_updates 120169 | lr 9.12229e-05 | gnorm 5.101 | loss_scale 0.5 | train_wall 305 | wall 0\n",
      "2021-10-11 05:12:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=215/shard_epoch=59\n",
      "2021-10-11 05:12:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=215/shard_epoch=60\n",
      "2021-10-11 05:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:12:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007608\n",
      "2021-10-11 05:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.139702\n",
      "2021-10-11 05:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:11 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[243025, 1124, 168130, 43575, 358197, 335716, 217502, 99551, 253894, 318492]\n",
      "2021-10-11 05:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011473\n",
      "2021-10-11 05:12:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.206278\n",
      "2021-10-11 05:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.358323\n",
      "2021-10-11 05:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115560\n",
      "2021-10-11 05:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:13 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[243025, 1124, 168130, 43575, 358197, 335716, 217502, 99551, 253894, 318492]\n",
      "2021-10-11 05:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012695\n",
      "2021-10-11 05:12:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.206907\n",
      "2021-10-11 05:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.336046\n",
      "2021-10-11 05:12:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 215:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 05:12:14 | INFO | fairseq.trainer | begin training epoch 215\n",
      "epoch 215: 100%|9| 2036/2037 [05:14<00:00,  6.38it/s, loss=3.717, nll_loss=1.9622021-10-11 05:17:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 05:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003487\n",
      "2021-10-11 05:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155293\n",
      "2021-10-11 05:17:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102630\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262011\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002802\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153176\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104956\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261459\n",
      "2021-10-11 05:17:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 215 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.04it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  8.86it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 10.61it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.34it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 13.88it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.13it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.36it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.27it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 17.96it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.50it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.14it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:03, 13.72it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:03, 14.87it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 16.03it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 17.03it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 17.50it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 18.16it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 18.61it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.16it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.40it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.51it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.63it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  70%|####2 | 57/81 [00:03<00:01, 19.80it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.66it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.85it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.80it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.90it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.69it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.57it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  94%|#####6| 76/81 [00:04<00:00, 19.85it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.32it/s]\u001b[A\n",
      "epoch 215 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 05:17:33 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 7.198 | nll_loss 5.809 | ppl 56.08 | wps 47860.2 | wpb 2494.5 | bsz 169.9 | num_updates 122206 | best_loss 6.688\n",
      "2021-10-11 05:17:33 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 05:17:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint215.pt (epoch 215 @ 122206 updates, score 7.198) (writing took 3.2966063280000526 seconds)\n",
      "2021-10-11 05:17:36 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)\n",
      "2021-10-11 05:17:36 | INFO | train | epoch 215 | loss 3.676 | nll_loss 1.914 | ppl 3.77 | wps 22048.6 | ups 6.26 | wpb 3520.3 | bsz 208.9 | num_updates 122206 | lr 9.04594e-05 | gnorm 4.964 | loss_scale 0.5 | train_wall 306 | wall 0\n",
      "2021-10-11 05:17:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=216/shard_epoch=60\n",
      "2021-10-11 05:17:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=216/shard_epoch=61\n",
      "2021-10-11 05:17:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:17:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006486\n",
      "2021-10-11 05:17:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.141302\n",
      "2021-10-11 05:17:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:37 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[419436, 363035, 361061, 365257, 355821, 375667, 151709, 27010, 280865, 96703]\n",
      "2021-10-11 05:17:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011601\n",
      "2021-10-11 05:17:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.242157\n",
      "2021-10-11 05:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.395964\n",
      "2021-10-11 05:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113137\n",
      "2021-10-11 05:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:38 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[419436, 363035, 361061, 365257, 355821, 375667, 151709, 27010, 280865, 96703]\n",
      "2021-10-11 05:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011349\n",
      "2021-10-11 05:17:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.232277\n",
      "2021-10-11 05:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.357635\n",
      "2021-10-11 05:17:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 216:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 05:17:39 | INFO | fairseq.trainer | begin training epoch 216\n",
      "epoch 216: 100%|9| 2036/2037 [05:10<00:00,  6.25it/s, loss=3.689, nll_loss=1.9282021-10-11 05:22:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003118\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155285\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.101006\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260068\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002774\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155524\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102397\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261380\n",
      "2021-10-11 05:22:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 216 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.54it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.39it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 11.15it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.81it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.28it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.48it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.61it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.49it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 17.95it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  30%|#7    | 24/81 [00:01<00:03, 18.57it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.79it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.35it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.27it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 19.13it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.24it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.39it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.20it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.38it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.46it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.81it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.79it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.79it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.90it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.74it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  77%|####5 | 62/81 [00:03<00:00, 19.93it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  79%|####7 | 64/81 [00:03<00:00, 19.87it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  81%|####8 | 66/81 [00:03<00:00, 19.83it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.90it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.67it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.53it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.84it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.33it/s]\u001b[A\n",
      "epoch 216 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 05:22:55 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 7.191 | nll_loss 5.825 | ppl 56.67 | wps 49374.2 | wpb 2494.5 | bsz 169.9 | num_updates 124243 | best_loss 6.688\n",
      "2021-10-11 05:22:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 05:22:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint216.pt (epoch 216 @ 124243 updates, score 7.191) (writing took 3.225030143999902 seconds)\n",
      "2021-10-11 05:22:58 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)\n",
      "2021-10-11 05:22:58 | INFO | train | epoch 216 | loss 3.674 | nll_loss 1.911 | ppl 3.76 | wps 22319.6 | ups 6.34 | wpb 3520.3 | bsz 208.9 | num_updates 124243 | lr 8.97148e-05 | gnorm 5.032 | loss_scale 0.5 | train_wall 303 | wall 0\n",
      "2021-10-11 05:22:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=217/shard_epoch=61\n",
      "2021-10-11 05:22:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=217/shard_epoch=62\n",
      "2021-10-11 05:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:22:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006812\n",
      "2021-10-11 05:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.144313\n",
      "2021-10-11 05:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:22:58 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[6280, 172706, 347000, 257544, 275703, 273317, 44264, 234858, 154342, 209296]\n",
      "2021-10-11 05:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011670\n",
      "2021-10-11 05:22:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.155313\n",
      "2021-10-11 05:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.312233\n",
      "2021-10-11 05:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114106\n",
      "2021-10-11 05:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:22:59 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[6280, 172706, 347000, 257544, 275703, 273317, 44264, 234858, 154342, 209296]\n",
      "2021-10-11 05:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011295\n",
      "2021-10-11 05:22:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:23:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.158530\n",
      "2021-10-11 05:23:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.284876\n",
      "2021-10-11 05:23:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 217:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 05:23:00 | INFO | fairseq.trainer | begin training epoch 217\n",
      "epoch 217: 100%|9| 2036/2037 [05:13<00:00,  6.45it/s, loss=3.719, nll_loss=1.9592021-10-11 05:28:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003778\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155112\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.120969\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.280581\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002907\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.172648\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.115466\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.291691\n",
      "2021-10-11 05:28:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 217 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.44it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.25it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 11.00it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 12.70it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 14.09it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.34it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.31it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.22it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.92it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.24it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.67it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.00it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.12it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.38it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.27it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.46it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.07it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.22it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 18.93it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.14it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.24it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.56it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.67it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.55it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  65%|###9  | 53/81 [00:02<00:01, 19.65it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  68%|####  | 55/81 [00:02<00:01, 19.50it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  72%|####2 | 58/81 [00:03<00:01, 19.65it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  74%|####4 | 60/81 [00:03<00:01, 19.40it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.62it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.61it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.71it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.52it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.43it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.68it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.15it/s]\u001b[A\n",
      "epoch 217 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 05:28:18 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 7.277 | nll_loss 5.892 | ppl 59.4 | wps 48791.7 | wpb 2494.5 | bsz 169.9 | num_updates 126280 | best_loss 6.688\n",
      "2021-10-11 05:28:18 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 05:28:22 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint217.pt (epoch 217 @ 126280 updates, score 7.277) (writing took 3.2307621200016 seconds)\n",
      "2021-10-11 05:28:22 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)\n",
      "2021-10-11 05:28:22 | INFO | train | epoch 217 | loss 3.673 | nll_loss 1.91 | ppl 3.76 | wps 22140.3 | ups 6.29 | wpb 3520.3 | bsz 208.9 | num_updates 126280 | lr 8.89883e-05 | gnorm 4.9 | loss_scale 0.5 | train_wall 305 | wall 0\n",
      "2021-10-11 05:28:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=218/shard_epoch=62\n",
      "2021-10-11 05:28:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=218/shard_epoch=63\n",
      "2021-10-11 05:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:28:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006789\n",
      "2021-10-11 05:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.142882\n",
      "2021-10-11 05:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:22 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[92576, 314800, 118784, 70592, 375280, 117605, 312363, 98956, 401864, 71185]\n",
      "2021-10-11 05:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011565\n",
      "2021-10-11 05:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.147183\n",
      "2021-10-11 05:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.302569\n",
      "2021-10-11 05:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113885\n",
      "2021-10-11 05:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:23 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[92576, 314800, 118784, 70592, 375280, 117605, 312363, 98956, 401864, 71185]\n",
      "2021-10-11 05:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011424\n",
      "2021-10-11 05:28:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.142440\n",
      "2021-10-11 05:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.268672\n",
      "2021-10-11 05:28:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 218:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 05:28:24 | INFO | fairseq.trainer | begin training epoch 218\n",
      "epoch 218: 100%|9| 2036/2037 [05:12<00:00,  6.61it/s, loss=3.814, nll_loss=2.0612021-10-11 05:33:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 05:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003156\n",
      "2021-10-11 05:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.157195\n",
      "2021-10-11 05:33:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.105149\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.266205\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002744\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155993\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104298\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.263605\n",
      "2021-10-11 05:33:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 218 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:11,  7.27it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.09it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 10.85it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  10%|6      | 8/81 [00:00<00:05, 12.57it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 14.00it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 15.29it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 16.29it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.28it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 18.00it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.31it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.77it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.11it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 19.20it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.53it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.43it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.35it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.47it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.27it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.42it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.43it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.74it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.77it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.75it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.74it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.85it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.71it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.78it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.78it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.74it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  84%|##### | 68/81 [00:03<00:00, 19.90it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  86%|#####1| 70/81 [00:03<00:00, 19.79it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  89%|#####3| 72/81 [00:03<00:00, 19.42it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  91%|#####4| 74/81 [00:03<00:00, 19.58it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.66it/s]\u001b[A\n",
      "epoch 218 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 05:33:42 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 7.204 | nll_loss 5.816 | ppl 56.35 | wps 49199.1 | wpb 2494.5 | bsz 169.9 | num_updates 128317 | best_loss 6.688\n",
      "2021-10-11 05:33:42 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 05:33:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint218.pt (epoch 218 @ 128317 updates, score 7.204) (writing took 3.2345738699987123 seconds)\n",
      "2021-10-11 05:33:45 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)\n",
      "2021-10-11 05:33:45 | INFO | train | epoch 218 | loss 3.671 | nll_loss 1.907 | ppl 3.75 | wps 22159.5 | ups 6.29 | wpb 3520.3 | bsz 208.9 | num_updates 128317 | lr 8.82791e-05 | gnorm 5.004 | loss_scale 0.5 | train_wall 305 | wall 0\n",
      "2021-10-11 05:33:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=219/shard_epoch=63\n",
      "2021-10-11 05:33:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=219/shard_epoch=64\n",
      "2021-10-11 05:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:33:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006553\n",
      "2021-10-11 05:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.137308\n",
      "2021-10-11 05:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:45 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[62707, 207910, 385023, 70549, 409522, 403120, 418077, 199069, 209552, 56466]\n",
      "2021-10-11 05:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011325\n",
      "2021-10-11 05:33:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.233829\n",
      "2021-10-11 05:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.383374\n",
      "2021-10-11 05:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110998\n",
      "2021-10-11 05:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:47 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[62707, 207910, 385023, 70549, 409522, 403120, 418077, 199069, 209552, 56466]\n",
      "2021-10-11 05:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012516\n",
      "2021-10-11 05:33:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.238353\n",
      "2021-10-11 05:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.362814\n",
      "2021-10-11 05:33:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 219:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 05:33:48 | INFO | fairseq.trainer | begin training epoch 219\n",
      "epoch 219: 100%|9| 2036/2037 [05:11<00:00,  6.45it/s, loss=3.624, nll_loss=1.8562021-10-11 05:39:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003213\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.172448\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.106317\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.282780\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002982\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.155325\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.102715\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.261764\n",
      "2021-10-11 05:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 219 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.50it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.36it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:   9%|6      | 7/81 [00:00<00:06, 11.12it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  12%|7     | 10/81 [00:00<00:05, 12.81it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  15%|8     | 12/81 [00:00<00:04, 14.25it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  17%|#     | 14/81 [00:00<00:04, 15.48it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 16.60it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  25%|#4    | 20/81 [00:01<00:03, 17.44it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.07it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  32%|#9    | 26/81 [00:01<00:02, 18.58it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  36%|##1   | 29/81 [00:01<00:02, 19.20it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  38%|##2   | 31/81 [00:01<00:02, 19.16it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  41%|##4   | 33/81 [00:01<00:02, 18.99it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  43%|##5   | 35/81 [00:01<00:02, 19.14it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  47%|##8   | 38/81 [00:01<00:02, 19.40it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.19it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.37it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.45it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.80it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  62%|###7  | 50/81 [00:02<00:01, 19.89it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  64%|###8  | 52/81 [00:02<00:01, 19.45it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.54it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.65it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.51it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.63it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.67it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.62it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.72it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.80it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.53it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.45it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  93%|#####5| 75/81 [00:03<00:00, 19.47it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  95%|#####7| 77/81 [00:03<00:00, 19.37it/s]\u001b[A\n",
      "epoch 219 | valid on 'valid' subset:  98%|#####8| 79/81 [00:04<00:00, 19.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 05:39:04 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 7.194 | nll_loss 5.825 | ppl 56.67 | wps 49116.4 | wpb 2494.5 | bsz 169.9 | num_updates 130354 | best_loss 6.688\n",
      "2021-10-11 05:39:04 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 05:39:08 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint219.pt (epoch 219 @ 130354 updates, score 7.194) (writing took 3.2731439660019532 seconds)\n",
      "2021-10-11 05:39:08 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)\n",
      "2021-10-11 05:39:08 | INFO | train | epoch 219 | loss 3.668 | nll_loss 1.902 | ppl 3.74 | wps 22246.5 | ups 6.32 | wpb 3520.3 | bsz 208.9 | num_updates 130354 | lr 8.75866e-05 | gnorm 5.069 | loss_scale 0.5 | train_wall 304 | wall 0\n",
      "2021-10-11 05:39:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=220/shard_epoch=64\n",
      "2021-10-11 05:39:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=220/shard_epoch=65\n",
      "2021-10-11 05:39:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:39:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006928\n",
      "2021-10-11 05:39:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.142857\n",
      "2021-10-11 05:39:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:08 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[233650, 274359, 156237, 230054, 332166, 243912, 229864, 326339, 104005, 285166]\n",
      "2021-10-11 05:39:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011405\n",
      "2021-10-11 05:39:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.196699\n",
      "2021-10-11 05:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.351861\n",
      "2021-10-11 05:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112215\n",
      "2021-10-11 05:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:09 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[233650, 274359, 156237, 230054, 332166, 243912, 229864, 326339, 104005, 285166]\n",
      "2021-10-11 05:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012290\n",
      "2021-10-11 05:39:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.199614\n",
      "2021-10-11 05:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.325074\n",
      "2021-10-11 05:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 220:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 05:39:10 | INFO | fairseq.trainer | begin training epoch 220\n",
      "epoch 220: 100%|9| 2036/2037 [05:12<00:00,  6.55it/s, loss=3.756, nll_loss=2.0022021-10-11 05:44:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003400\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.154946\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.104000\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.262919\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002812\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.153996\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.103407\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.260741\n",
      "2021-10-11 05:44:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 220 | valid on 'valid' subset:   0%|               | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:   1%|       | 1/81 [00:00<00:10,  7.36it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:   5%|3      | 4/81 [00:00<00:08,  9.19it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:   7%|5      | 6/81 [00:00<00:06, 10.96it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  11%|7      | 9/81 [00:00<00:05, 12.66it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  14%|8     | 11/81 [00:00<00:04, 14.13it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  16%|9     | 13/81 [00:00<00:04, 15.26it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  19%|#1    | 15/81 [00:00<00:04, 16.41it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  21%|#2    | 17/81 [00:00<00:03, 17.22it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  23%|#4    | 19/81 [00:01<00:03, 17.94it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  26%|#5    | 21/81 [00:01<00:03, 18.20it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  28%|#7    | 23/81 [00:01<00:03, 18.68it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  31%|#8    | 25/81 [00:01<00:02, 19.02it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  33%|##    | 27/81 [00:01<00:02, 18.94it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  37%|##2   | 30/81 [00:01<00:02, 19.36it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  40%|##3   | 32/81 [00:01<00:02, 19.31it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  42%|##5   | 34/81 [00:01<00:02, 19.41it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  44%|##6   | 36/81 [00:01<00:02, 19.06it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  47%|##8   | 38/81 [00:02<00:02, 19.29it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  49%|##9   | 40/81 [00:02<00:02, 19.10it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  52%|###1  | 42/81 [00:02<00:02, 19.30it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  54%|###2  | 44/81 [00:02<00:01, 19.37it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  58%|###4  | 47/81 [00:02<00:01, 19.69it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  60%|###6  | 49/81 [00:02<00:01, 19.74it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  63%|###7  | 51/81 [00:02<00:01, 19.69it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  67%|####  | 54/81 [00:02<00:01, 19.65it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  70%|####2 | 57/81 [00:02<00:01, 19.79it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  73%|####3 | 59/81 [00:03<00:01, 19.63it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  75%|####5 | 61/81 [00:03<00:01, 19.71it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  78%|####6 | 63/81 [00:03<00:00, 19.74it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  80%|####8 | 65/81 [00:03<00:00, 19.62it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  83%|####9 | 67/81 [00:03<00:00, 19.50it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  85%|#####1| 69/81 [00:03<00:00, 19.60it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  88%|#####2| 71/81 [00:03<00:00, 19.36it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  90%|#####4| 73/81 [00:03<00:00, 19.29it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  94%|#####6| 76/81 [00:03<00:00, 19.62it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset:  96%|#####7| 78/81 [00:04<00:00, 19.18it/s]\u001b[A\n",
      "epoch 220 | valid on 'valid' subset: 100%|######| 81/81 [00:04<00:00, 20.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-11 05:44:27 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 7.156 | nll_loss 5.791 | ppl 55.37 | wps 48956 | wpb 2494.5 | bsz 169.9 | num_updates 132391 | best_loss 6.688\n",
      "2021-10-11 05:44:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-11 05:44:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/10nal-es+es-en/checkpoint220.pt (epoch 220 @ 132391 updates, score 7.156) (writing took 3.2309661119979864 seconds)\n",
      "2021-10-11 05:44:31 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)\n",
      "2021-10-11 05:44:31 | INFO | train | epoch 220 | loss 3.666 | nll_loss 1.9 | ppl 3.73 | wps 22191.5 | ups 6.3 | wpb 3520.3 | bsz 208.9 | num_updates 132391 | lr 8.69102e-05 | gnorm 4.986 | loss_scale 1 | train_wall 304 | wall 0\n",
      "2021-10-11 05:44:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=221/shard_epoch=65\n",
      "2021-10-11 05:44:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=221/shard_epoch=66\n",
      "2021-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:44:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006522\n",
      "2021-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.140594\n",
      "2021-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:31 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[249781, 62186, 314335, 223549, 109896, 62285, 295838, 398456, 333770, 335694]\n",
      "2021-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011593\n",
      "2021-10-11 05:44:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.187200\n",
      "2021-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.340280\n",
      "2021-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113282\n",
      "2021-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:32 | WARNING | fairseq.tasks.fairseq_task | 17 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[249781, 62186, 314335, 223549, 109896, 62285, 295838, 398456, 333770, 335694]\n",
      "2021-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011266\n",
      "2021-10-11 05:44:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-11 05:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.185381\n",
      "2021-10-11 05:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.310813\n",
      "2021-10-11 05:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 221:   0%|                                       | 0/2037 [00:00<?, ?it/s]2021-10-11 05:44:33 | INFO | fairseq.trainer | begin training epoch 221\n",
      "epoch 221:  51%|5| 1049/2037 [02:41<02:26,  6.75it/s, loss=3.702, nll_loss=1.94,"
     ]
    }
   ],
   "source": [
    "! fairseq-train $BIN_DIR \\\n",
    "    --arch=transformer --share-all-embeddings \\\n",
    "    --task translation_multi_simple_epoch --lang-pairs quy-es,es-en,cni-es,aym-es,bzd-es,gn-es,oto-es,nah-es,tar-es,shp-es,hch-es \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --restore-file $MODEL_DIR/checkpoint_last.pt \\\n",
    "    --save-dir $MODEL_DIR/ \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --reset-optimizer \\\n",
    "    --encoder-langtok \"src\" \\\n",
    "    --decoder-langtok \\\n",
    "    --fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "! echo $MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "cd /storage/master-thesis/models/quy-es+es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm checkpoint108.pt\n",
    "! rm checkpoint109.pt\n",
    "! rm checkpoint_best.pt\n",
    "! rm checkpoint_last.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm dict.en.txt\n",
    "! rm dict.es.txt\n",
    "! rm dict.quy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5K\t/notebooks/CITATION.cff\n",
      "5.5K\t/notebooks/CODE_OF_CONDUCT.md\n",
      "15K\t/notebooks/CONTRIBUTING.md\n",
      "19K\t/notebooks/ISSUES.md\n",
      "12K\t/notebooks/LICENSE\n",
      "512\t/notebooks/MANIFEST.in\n",
      "3.5K\t/notebooks/Makefile\n",
      "41K\t/notebooks/README.md\n",
      "41K\t/notebooks/README_zh-hans.md\n",
      "42K\t/notebooks/README_zh-hant.md\n",
      "12K\t/notebooks/docker\n",
      "4.6M\t/notebooks/docs\n",
      "5.0M\t/notebooks/examples\n",
      "8.5K\t/notebooks/hubconf.py\n",
      "7.8G\t/notebooks/master-thesis\n",
      "1.5K\t/notebooks/model_cards\n",
      "9.5K\t/notebooks/notebooks\n",
      "512\t/notebooks/pyproject.toml\n",
      "64K\t/notebooks/scripts\n",
      "1.0K\t/notebooks/setup.cfg\n",
      "13K\t/notebooks/setup.py\n",
      "13M\t/notebooks/src\n",
      "731K\t/notebooks/templates\n",
      "4.5K\t/notebooks/test quy-es -> es-en model.ipynb\n",
      "6.8M\t/notebooks/tests\n",
      "147K\t/notebooks/train es-en model.ipynb\n",
      "158K\t/notebooks/train quy-es + es-en model.ipynb\n",
      "160K\t/notebooks/utils\n",
      "3.5K\t/notebooks/valohai.yaml\n",
      "7.9G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\t/notebooks/master-thesis/Untitled.ipynb\n",
      "362M\t/notebooks/master-thesis/corpora\n",
      "7.5G\t/notebooks/master-thesis/models\n",
      "7.8G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6G\t/notebooks/master-thesis/models/es-en\n",
      "802M\t/notebooks/master-thesis/models/quy-es\n",
      "5.2G\t/notebooks/master-thesis/models/quy-es+es-en\n",
      "7.5G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19M\t/apex\n",
      "5.0M\t/bin\n",
      "4.0K\t/boot\n",
      "24K\t/content\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! du -shc /*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CITATION.cff         \u001b[0m\u001b[01;34mdocker\u001b[0m/          setup.py\n",
      " CODE_OF_CONDUCT.md   \u001b[01;34mdocs\u001b[0m/            \u001b[01;34msrc\u001b[0m/\n",
      " CONTRIBUTING.md      \u001b[01;34mexamples\u001b[0m/        \u001b[01;34mtemplates\u001b[0m/\n",
      " ISSUES.md            hubconf.py      'test quy-es -> es-en model.ipynb'\n",
      " LICENSE              \u001b[01;34mmaster-thesis\u001b[0m/   \u001b[01;34mtests\u001b[0m/\n",
      " MANIFEST.in          \u001b[01;34mmodel_cards\u001b[0m/    'train es-en model.ipynb'\n",
      " Makefile             \u001b[01;34mnotebooks\u001b[0m/      'train quy-es + es-en model.ipynb'\n",
      " README.md            pyproject.toml   \u001b[01;34mutils\u001b[0m/\n",
      " README_zh-hans.md    \u001b[01;34mscripts\u001b[0m/         valohai.yaml\n",
      " README_zh-hant.md    setup.cfg\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
