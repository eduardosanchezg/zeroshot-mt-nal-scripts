{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUY_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZER_PATH=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZER_PATH = /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_path_es_en = \"/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/\"\n",
    "tokenized_path_quy_es = \"/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZED_PATH_ES_EN=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en\n",
      "env: TOKENIZED_PATH_QUY_ES=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZED_PATH_ES_EN = /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en\n",
    "%env TOKENIZED_PATH_QUY_ES = /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BIN_DIR=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "%env BIN_DIR = /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_DIR=/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_DIR = /storage/master-thesis/models/quy-es+es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locale: Cannot set LC_CTYPE to default locale: No such file or directory\n",
      "locale: Cannot set LC_MESSAGES to default locale: No such file or directory\n",
      "locale: Cannot set LC_ALL to default locale: No such file or directory\n",
      "LANG=en_US.UTF-8\n",
      "LANGUAGE=\n",
      "LC_CTYPE=\"en_US.UTF-8\"\n",
      "LC_NUMERIC=\"en_US.UTF-8\"\n",
      "LC_TIME=\"en_US.UTF-8\"\n",
      "LC_COLLATE=\"en_US.UTF-8\"\n",
      "LC_MONETARY=\"en_US.UTF-8\"\n",
      "LC_MESSAGES=\"en_US.UTF-8\"\n",
      "LC_PAPER=\"en_US.UTF-8\"\n",
      "LC_NAME=\"en_US.UTF-8\"\n",
      "LC_ADDRESS=\"en_US.UTF-8\"\n",
      "LC_TELEPHONE=\"en_US.UTF-8\"\n",
      "LC_MEASUREMENT=\"en_US.UTF-8\"\n",
      "LC_IDENTIFICATION=\"en_US.UTF-8\"\n",
      "LC_ALL=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "! locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: update-locale: not found\n",
      "env: LANG=en_US.UTF-8\n",
      "env: LC_CTYPE=en_US.UTF-8\n",
      "env: LC_ALL=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "! update-locale LANG=en_US.UTF-8 LANGUAGE=en.UTF-8\n",
    "\n",
    "%env LANG=en_US.UTF-8\n",
    "%env LC_CTYPE=en_US.UTF-8\n",
    "%env LC_ALL=en_US.UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /etc/rc.conf: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! cat /etc/rc.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.10.3 in /usr/local/lib/python3.6/dist-packages (0.10.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers==0.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (0.10.2)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (2.0.0)\n",
      "Requirement already satisfied: hydra-core in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.1.1)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.24)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.51.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.3.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.4)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (4.8)\n",
      "Requirement already satisfied: omegaconf==2.1.* in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (2.1.1)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (5.2.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.18.2)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq) (5.4.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tatoeba format to standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(EN_ES_CORPUS_DIR + \"original/valid.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"dev.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"dev.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "    en.close()\n",
    "    es.close()\n",
    "    \n",
    "with open(EN_ES_CORPUS_DIR + \"original/dev.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"train.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"train.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        if \"\\n\" in line[2] or \"\\n\" in line[3]:\n",
    "            continue\n",
    "        if len(line) != 4:\n",
    "            continue\n",
    "            \n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "        \n",
    "    en.close()\n",
    "    es.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "en_es_quy_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "en_es_quy_trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=30000, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "en_es_quy_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "en_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.en\" for split in [\"dev\", \"train\"]]\n",
    "es_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "quy_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/{split}.quy\" for split in [\"dev\", \"dict\", \"train\"]]\n",
    "es2_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/{split}.es\" for split in [\"dev\", \"dict\", \"train\"]]\n",
    "\n",
    "files = en_files + es_files + es2_files + quy_files\n",
    "en_es_quy_tokenizer.train(files= files, trainer=en_es_quy_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_files(tokenizer, files, extension, output_path):\n",
    "    for file in files:\n",
    "        print(f\"Reading file {file}\")\n",
    "        with open(file, encoding='utf8') as f:\n",
    "          lines = f.readlines()\n",
    "          tokenized_lines = tokenizer.encode_batch(lines)\n",
    "          tokenized_name = Path(file).stem\n",
    "          tokenized_name = output_path + tokenized_name + \".\" + extension\n",
    "          print(tokenized_name)\n",
    "          with open(tokenized_name, 'w', encoding='utf8') as wf:\n",
    "\n",
    "            wf.writelines([\" \".join(t.tokens) + \"\\n\" for t in tokenized_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_quy_tokenizer.save(tokenizer_path + \"quy-es-en-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.Tokenizer object at 0x28a09a0>\n"
     ]
    }
   ],
   "source": [
    "print(en_es_quy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'hold', 'this', 'truth', 'to', 'be', 'self', 'evid', '##ent', 'that', 'everyone', 'is', 'created', 'equal', '?']\n"
     ]
    }
   ],
   "source": [
    "tok = en_es_quy_tokenizer.encode(\"we hold this truth to be self evident that everyone is created equal?\")\n",
    "print(tok.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.en\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.en\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.quy\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dict.quy\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dict.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.quy\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dict.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dict.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.es\n"
     ]
    }
   ],
   "source": [
    "tokenize_files(en_es_quy_tokenizer, en_files, \"en\", tokenized_path_es_en)\n",
    "tokenize_files(en_es_quy_tokenizer, es_files, \"es\", tokenized_path_es_en)\n",
    "\n",
    "tokenize_files(en_es_quy_tokenizer, quy_files, \"quy\", tokenized_path_quy_es)\n",
    "tokenize_files(en_es_quy_tokenizer, es2_files, \"es\", tokenized_path_quy_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.en\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.quy.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.es.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.en.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "### Removing previous dict files\n",
    "! rm $BIN_DIR/dict.quy.txt\n",
    "! rm $BIN_DIR/dict.es.txt\n",
    "! rm $BIN_DIR/dict.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenating all training data\n",
    "! cat $TOKENIZED_PATH_QUY_ES/train.quy $TOKENIZED_PATH_QUY_ES/train.es $TOKENIZED_PATH_ES_EN/train.es $TOKENIZED_PATH_ES_EN/train.en > $BIN_DIR/train.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict.all.txt         test.quy-es.quy.idx     train.quy-es.quy.bin\n",
      "dict.en.txt          train.all               train.quy-es.quy.idx\n",
      "dict.es.txt          train.all-None.all.bin  valid.es-en.en.bin\n",
      "dict.quy.txt         train.all-None.all.idx  valid.es-en.en.idx\n",
      "preprocess.log       train.es-en.en.bin      valid.es-en.es.bin\n",
      "test.es-en.es.bin    train.es-en.en.idx      valid.es-en.es.idx\n",
      "test.es-en.es.idx    train.es-en.es.bin      valid.quy-es.es.bin\n",
      "test.quy-en.quy.bin  train.es-en.es.idx      valid.quy-es.es.idx\n",
      "test.quy-en.quy.idx  train.quy-es.es.bin     valid.quy-es.quy.bin\n",
      "test.quy-es.quy.bin  train.quy-es.es.idx     valid.quy-es.quy.idx\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:44:27 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='all', srcdict=None, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train', user_dir=None, validpref=None, workers=20)\n",
      "2021-10-07 17:44:34 | INFO | fairseq_cli.preprocess | [all] Dictionary: 28976 types\n",
      "2021-10-07 17:44:48 | INFO | fairseq_cli.preprocess | [all] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.all: 644610 sents, 8263690 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:44:48 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang all \\\n",
    "    --trainpref $BIN_DIR/train \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --workers 20 \\\n",
    "    --only-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:46:10 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='quy', srcdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', target_lang='es', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train', user_dir=None, validpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev', workers=20)\n",
      "2021-10-07 17:46:10 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 28976 types\n",
      "2021-10-07 17:46:13 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.quy: 125008 sents, 1922581 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:13 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 28976 types\n",
      "2021-10-07 17:46:14 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.quy: 996 sents, 14488 tokens, 0.456% replaced by <unk>\n",
      "2021-10-07 17:46:14 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:18 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.es: 125008 sents, 2452298 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:18 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:19 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.es: 996 sents, 15303 tokens, 0.242% replaced by <unk>\n",
      "2021-10-07 17:46:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang quy --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_QUY_ES/train --validpref $TOKENIZED_PATH_QUY_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing es dict from quy-es preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:46:28 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='es', srcdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train', user_dir=None, validpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev', workers=20)\n",
      "2021-10-07 17:46:28 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:33 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.es: 197297 sents, 1894674 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:33 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:34 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.es: 4643 sents, 51839 tokens, 0.0405% replaced by <unk>\n",
      "2021-10-07 17:46:34 | INFO | fairseq_cli.preprocess | [en] Dictionary: 28976 types\n",
      "2021-10-07 17:46:38 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.en: 197297 sents, 1994137 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:38 | INFO | fairseq_cli.preprocess | [en] Dictionary: 28976 types\n",
      "2021-10-07 17:46:39 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.en: 4643 sents, 54416 tokens, 0.0404% replaced by <unk>\n",
      "2021-10-07 17:46:39 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang es --target-lang en \\\n",
    "    --trainpref $TOKENIZED_PATH_ES_EN/train --validpref $TOKENIZED_PATH_ES_EN/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-08 00:07:27 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=2, label_smoothing=0.1, lang_dict=None, lang_pairs='quy-es,es-en', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='/storage/master-thesis/models/quy-es+es-en/checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='/storage/master-thesis/models/quy-es+es-en/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')\n",
      "2021-10-08 00:07:27 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['en', 'es', 'quy']\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 28979 types\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 28979 types\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [quy] dictionary: 28979 types\n",
      "2021-10-08 00:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n",
      "2021-10-08 00:07:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:quy-es': 1, 'main:es-en': 1}\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 28978; tgt_langtok: 28977\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.quy-es.quy\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.quy-es.es\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin valid quy-es 996 examples\n",
      "2021-10-08 00:07:27 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 28977; tgt_langtok: 28976\n",
      "2021-10-08 00:07:28 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.es-en.es\n",
      "2021-10-08 00:07:28 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.es-en.en\n",
      "2021-10-08 00:07:28 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin valid es-en 4643 examples\n",
      "2021-10-08 00:07:29 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(28979, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(28979, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=28979, bias=False)\n",
      "  )\n",
      ")\n",
      "2021-10-08 00:07:29 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)\n",
      "2021-10-08 00:07:29 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
      "2021-10-08 00:07:29 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
      "2021-10-08 00:07:29 | INFO | fairseq_cli.train | num. model params: 58975744 (num. trained: 58975744)\n",
      "2021-10-08 00:07:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
      "2021-10-08 00:07:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2021-10-08 00:07:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-10-08 00:07:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 7.795 GB ; name = Quadro RTX 4000                         \n",
      "2021-10-08 00:07:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-10-08 00:07:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2021-10-08 00:07:36 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None\n",
      "2021-10-08 00:08:12 | INFO | fairseq.trainer | loaded checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint_last.pt (epoch 93 @ 0 updates)\n",
      "2021-10-08 00:08:12 | INFO | fairseq.optim.adam | using FusedAdam\n",
      "2021-10-08 00:08:12 | INFO | fairseq.trainer | loading train data for epoch 93\n",
      "2021-10-08 00:08:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=93/None\n",
      "2021-10-08 00:08:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-10-08 00:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:quy-es': 1, 'main:es-en': 1}\n",
      "2021-10-08 00:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 28978; tgt_langtok: 28977\n",
      "2021-10-08 00:08:12 | INFO | fairseq.data.data_utils | loaded 125008 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.quy-es.quy\n",
      "2021-10-08 00:08:12 | INFO | fairseq.data.data_utils | loaded 125008 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.quy-es.es\n",
      "2021-10-08 00:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin train quy-es 125008 examples\n",
      "2021-10-08 00:08:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 28977; tgt_langtok: 28976\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.es-en.es\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.es-en.en\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin train es-en 197297 examples\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:quy-es', 125008), ('main:es-en', 197297)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat\n",
      "2021-10-08 00:08:13 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 322305\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 322305; virtual dataset size 322305\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=93/shard_epoch=1\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:quy-es': 125008, 'main:es-en': 197297}; raw total size: 322305\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:quy-es': 125008, 'main:es-en': 197297}; resampled total size: 322305\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.038486\n",
      "2021-10-08 00:08:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:08:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004569\n",
      "2021-10-08 00:08:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106689\n",
      "2021-10-08 00:08:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:08:13 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[59369]\n",
      "2021-10-08 00:08:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008269\n",
      "2021-10-08 00:08:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:08:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.969045\n",
      "2021-10-08 00:08:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.085476\n",
      "2021-10-08 00:08:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:08:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:08:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088399\n",
      "2021-10-08 00:08:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:08:14 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[59369]\n",
      "2021-10-08 00:08:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009330\n",
      "2021-10-08 00:08:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.904957\n",
      "2021-10-08 00:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.004125\n",
      "2021-10-08 00:08:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 093:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:08:15 | INFO | fairseq.trainer | begin training epoch 93\n",
      "epoch 093:   0%|                               | 4/1290 [00:00<05:44,  3.74it/s]2021-10-08 00:08:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
      "epoch 093:  14%|1| 186/1290 [00:28<02:46,  6.62it/s, loss=2.909, nll_loss=1.096,2021-10-08 00:08:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
      "epoch 093:  33%|3| 429/1290 [01:05<02:10,  6.60it/s, loss=2.977, nll_loss=1.169,2021-10-08 00:09:21 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 093: 100%|9| 1289/1290 [03:18<00:00,  6.42it/s, loss=2.983, nll_loss=1.1822021-10-08 00:11:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001664\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063552\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041403\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107597\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001689\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064679\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042998\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110301\n",
      "2021-10-08 00:11:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 093 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.39it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  11%|7      | 3/28 [00:00<00:02,  9.03it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  18%|#2     | 5/28 [00:00<00:02, 10.80it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 12.49it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 13.94it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 15.09it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.79it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.41it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.86it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:11:36 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 4.681 | nll_loss 3.031 | ppl 8.17 | wps 51387.1 | wpb 2691.4 | bsz 201.4 | num_updates 1287\n",
      "2021-10-08 00:11:36 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:11:41 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint93.pt (epoch 93 @ 1287 updates, score 4.681) (writing took 4.960057757038157 seconds)\n",
      "2021-10-08 00:11:41 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n",
      "2021-10-08 00:11:41 | INFO | train | epoch 093 | loss 2.989 | nll_loss 1.186 | ppl 2.28 | wps 23271 | ups 6.29 | wpb 3696.8 | bsz 249.8 | num_updates 1287 | lr 0.000160875 | gnorm 1.363 | loss_scale 16 | train_wall 383 | wall 0\n",
      "2021-10-08 00:11:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=94/shard_epoch=1\n",
      "2021-10-08 00:11:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=94/shard_epoch=2\n",
      "2021-10-08 00:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:11:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005572\n",
      "2021-10-08 00:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.122463\n",
      "2021-10-08 00:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:41 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[136131]\n",
      "2021-10-08 00:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009658\n",
      "2021-10-08 00:11:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.960784\n",
      "2021-10-08 00:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.093902\n",
      "2021-10-08 00:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091272\n",
      "2021-10-08 00:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:42 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[136131]\n",
      "2021-10-08 00:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011166\n",
      "2021-10-08 00:11:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.965048\n",
      "2021-10-08 00:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.068462\n",
      "2021-10-08 00:11:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 094:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:11:43 | INFO | fairseq.trainer | begin training epoch 94\n",
      "epoch 094: 100%|9| 1289/1290 [03:20<00:00,  6.51it/s, loss=3.3, nll_loss=1.538, 2021-10-08 00:15:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001763\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064577\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042542\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109871\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001807\n",
      "2021-10-08 00:15:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062616\n",
      "2021-10-08 00:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042430\n",
      "2021-10-08 00:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107717\n",
      "2021-10-08 00:15:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 094 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.02it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.77it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.51it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.19it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.74it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 15.01it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.82it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.52it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.69it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.80it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.29it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:15:05 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 4.767 | nll_loss 3.111 | ppl 8.64 | wps 52128.5 | wpb 2691.4 | bsz 201.4 | num_updates 2577 | best_loss 4.681\n",
      "2021-10-08 00:15:05 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:15:09 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint94.pt (epoch 94 @ 2577 updates, score 4.767) (writing took 3.4649377259775065 seconds)\n",
      "2021-10-08 00:15:09 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n",
      "2021-10-08 00:15:09 | INFO | train | epoch 094 | loss 3.149 | nll_loss 1.365 | ppl 2.58 | wps 22916.2 | ups 6.2 | wpb 3695.9 | bsz 249.8 | num_updates 2577 | lr 0.000322125 | gnorm 1.54 | loss_scale 16 | train_wall 194 | wall 0\n",
      "2021-10-08 00:15:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=95/shard_epoch=2\n",
      "2021-10-08 00:15:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=95/shard_epoch=3\n",
      "2021-10-08 00:15:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:15:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004582\n",
      "2021-10-08 00:15:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111263\n",
      "2021-10-08 00:15:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:09 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[243514]\n",
      "2021-10-08 00:15:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010899\n",
      "2021-10-08 00:15:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.947121\n",
      "2021-10-08 00:15:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.070281\n",
      "2021-10-08 00:15:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:15:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089842\n",
      "2021-10-08 00:15:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:10 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[243514]\n",
      "2021-10-08 00:15:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008538\n",
      "2021-10-08 00:15:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.948365\n",
      "2021-10-08 00:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.048316\n",
      "2021-10-08 00:15:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 095:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:15:11 | INFO | fairseq.trainer | begin training epoch 95\n",
      "epoch 095:  93%|9| 1204/1290 [03:07<00:13,  6.29it/s, loss=3.594, nll_loss=1.8672021-10-08 00:18:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n",
      "epoch 095: 100%|9| 1289/1290 [03:20<00:00,  6.34it/s, loss=3.709, nll_loss=2, pp2021-10-08 00:18:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001795\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064637\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041084\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108466\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002043\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063458\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041388\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107864\n",
      "2021-10-08 00:18:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 095 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.79it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.48it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.23it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.92it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.45it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.73it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.26it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.39it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.57it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.77it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.22it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:18:33 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 4.702 | nll_loss 3.041 | ppl 8.23 | wps 51893.1 | wpb 2691.4 | bsz 201.4 | num_updates 3866 | best_loss 4.681\n",
      "2021-10-08 00:18:33 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:18:37 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint95.pt (epoch 95 @ 3866 updates, score 4.702) (writing took 3.4987988279899582 seconds)\n",
      "2021-10-08 00:18:37 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n",
      "2021-10-08 00:18:37 | INFO | train | epoch 095 | loss 3.44 | nll_loss 1.691 | ppl 3.23 | wps 22859.7 | ups 6.18 | wpb 3696.1 | bsz 250 | num_updates 3866 | lr 0.00048325 | gnorm 1.709 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 00:18:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=96/shard_epoch=3\n",
      "2021-10-08 00:18:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=96/shard_epoch=4\n",
      "2021-10-08 00:18:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:18:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004758\n",
      "2021-10-08 00:18:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105812\n",
      "2021-10-08 00:18:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:37 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[163914]\n",
      "2021-10-08 00:18:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010206\n",
      "2021-10-08 00:18:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.929231\n",
      "2021-10-08 00:18:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.046235\n",
      "2021-10-08 00:18:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:18:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091528\n",
      "2021-10-08 00:18:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:38 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[163914]\n",
      "2021-10-08 00:18:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010890\n",
      "2021-10-08 00:18:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:18:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.947183\n",
      "2021-10-08 00:18:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.050668\n",
      "2021-10-08 00:18:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 096:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:18:39 | INFO | fairseq.trainer | begin training epoch 96\n",
      "epoch 096:   2%|5                             | 23/1290 [00:03<03:21,  6.28it/s]2021-10-08 00:18:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0\n",
      "epoch 096: 100%|9| 1289/1290 [03:20<00:00,  6.34it/s, loss=3.712, nll_loss=2.0042021-10-08 00:22:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001864\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063380\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044578\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110793\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001708\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063665\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041465\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107843\n",
      "2021-10-08 00:22:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 096 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.92it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.55it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.27it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.00it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.56it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.90it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.07it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.84it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.10it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.44it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.65it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:22:02 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 4.712 | nll_loss 3.043 | ppl 8.24 | wps 51965.1 | wpb 2691.4 | bsz 201.4 | num_updates 5155 | best_loss 4.681\n",
      "2021-10-08 00:22:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:22:05 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint96.pt (epoch 96 @ 5155 updates, score 4.712) (writing took 3.5131728609558195 seconds)\n",
      "2021-10-08 00:22:05 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n",
      "2021-10-08 00:22:05 | INFO | train | epoch 096 | loss 3.634 | nll_loss 1.91 | ppl 3.76 | wps 22887.4 | ups 6.19 | wpb 3696.4 | bsz 249.8 | num_updates 5155 | lr 0.000440439 | gnorm 1.749 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 00:22:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=97/shard_epoch=4\n",
      "2021-10-08 00:22:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=97/shard_epoch=5\n",
      "2021-10-08 00:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:22:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004677\n",
      "2021-10-08 00:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.118352\n",
      "2021-10-08 00:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:05 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[204181]\n",
      "2021-10-08 00:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011392\n",
      "2021-10-08 00:22:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.986733\n",
      "2021-10-08 00:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.117470\n",
      "2021-10-08 00:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090210\n",
      "2021-10-08 00:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:06 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[204181]\n",
      "2021-10-08 00:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009081\n",
      "2021-10-08 00:22:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.941686\n",
      "2021-10-08 00:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.041978\n",
      "2021-10-08 00:22:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 097:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:22:07 | INFO | fairseq.trainer | begin training epoch 97\n",
      "epoch 097: 100%|9| 1289/1290 [03:20<00:00,  6.47it/s, loss=3.605, nll_loss=1.8822021-10-08 00:25:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002461\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066996\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042263\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112835\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001863\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062915\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044666\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110445\n",
      "2021-10-08 00:25:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 097 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.39it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.08it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.83it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.52it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.10it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.48it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.58it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.45it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.24it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.42it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.59it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.77it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:25:30 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 4.627 | nll_loss 2.969 | ppl 7.83 | wps 52320.8 | wpb 2691.4 | bsz 201.4 | num_updates 6445 | best_loss 4.627\n",
      "2021-10-08 00:25:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:25:34 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint97.pt (epoch 97 @ 6445 updates, score 4.627) (writing took 4.632756820006762 seconds)\n",
      "2021-10-08 00:25:34 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n",
      "2021-10-08 00:25:34 | INFO | train | epoch 097 | loss 3.579 | nll_loss 1.847 | ppl 3.6 | wps 22802.7 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 6445 | lr 0.000393902 | gnorm 1.564 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 00:25:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=98/shard_epoch=5\n",
      "2021-10-08 00:25:34 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=98/shard_epoch=6\n",
      "2021-10-08 00:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:25:34 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007389\n",
      "2021-10-08 00:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117617\n",
      "2021-10-08 00:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:34 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286476]\n",
      "2021-10-08 00:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010967\n",
      "2021-10-08 00:25:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.956453\n",
      "2021-10-08 00:25:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.086078\n",
      "2021-10-08 00:25:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090326\n",
      "2021-10-08 00:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:36 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[286476]\n",
      "2021-10-08 00:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008936\n",
      "2021-10-08 00:25:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.994574\n",
      "2021-10-08 00:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.094801\n",
      "2021-10-08 00:25:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 098:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:25:37 | INFO | fairseq.trainer | begin training epoch 98\n",
      "epoch 098: 100%|9| 1289/1290 [03:20<00:00,  6.33it/s, loss=3.595, nll_loss=1.87,2021-10-08 00:28:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001749\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066084\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044161\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112967\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002155\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063997\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045228\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112349\n",
      "2021-10-08 00:28:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 098 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.84it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.56it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.29it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.94it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.72it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.71it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.30it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.47it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.69it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.85it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:28:59 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 4.631 | nll_loss 2.955 | ppl 7.76 | wps 52054.8 | wpb 2691.4 | bsz 201.4 | num_updates 7735 | best_loss 4.627\n",
      "2021-10-08 00:28:59 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:29:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint98.pt (epoch 98 @ 7735 updates, score 4.631) (writing took 3.543411318969447 seconds)\n",
      "2021-10-08 00:29:02 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n",
      "2021-10-08 00:29:02 | INFO | train | epoch 098 | loss 3.509 | nll_loss 1.768 | ppl 3.41 | wps 22919 | ups 6.2 | wpb 3695.9 | bsz 249.8 | num_updates 7735 | lr 0.000359559 | gnorm 1.504 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 00:29:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=99/shard_epoch=6\n",
      "2021-10-08 00:29:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=99/shard_epoch=7\n",
      "2021-10-08 00:29:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:29:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005879\n",
      "2021-10-08 00:29:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110549\n",
      "2021-10-08 00:29:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:29:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[217058]\n",
      "2021-10-08 00:29:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008801\n",
      "2021-10-08 00:29:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.968426\n",
      "2021-10-08 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.088941\n",
      "2021-10-08 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:29:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092235\n",
      "2021-10-08 00:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:29:04 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[217058]\n",
      "2021-10-08 00:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009252\n",
      "2021-10-08 00:29:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.989036\n",
      "2021-10-08 00:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.091678\n",
      "2021-10-08 00:29:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 099:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:29:05 | INFO | fairseq.trainer | begin training epoch 99\n",
      "epoch 099: 100%|9| 1289/1290 [03:20<00:00,  6.25it/s, loss=3.443, nll_loss=1.6992021-10-08 00:32:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001776\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065515\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043645\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111907\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001847\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064327\n",
      "2021-10-08 00:32:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044677\n",
      "2021-10-08 00:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111910\n",
      "2021-10-08 00:32:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 099 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.76it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.47it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.20it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.87it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.43it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.78it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.82it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.29it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.28it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.43it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.70it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.09it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:32:27 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 4.638 | nll_loss 2.958 | ppl 7.77 | wps 51658 | wpb 2691.4 | bsz 201.4 | num_updates 9025 | best_loss 4.627\n",
      "2021-10-08 00:32:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:32:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint99.pt (epoch 99 @ 9025 updates, score 4.638) (writing took 3.4765338400029577 seconds)\n",
      "2021-10-08 00:32:31 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n",
      "2021-10-08 00:32:31 | INFO | train | epoch 099 | loss 3.448 | nll_loss 1.698 | ppl 3.24 | wps 22895.4 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 9025 | lr 0.000332871 | gnorm 1.456 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 00:32:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=100/shard_epoch=7\n",
      "2021-10-08 00:32:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=100/shard_epoch=8\n",
      "2021-10-08 00:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:32:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005126\n",
      "2021-10-08 00:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109270\n",
      "2021-10-08 00:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[105210]\n",
      "2021-10-08 00:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009885\n",
      "2021-10-08 00:32:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.014508\n",
      "2021-10-08 00:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.134629\n",
      "2021-10-08 00:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087928\n",
      "2021-10-08 00:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[105210]\n",
      "2021-10-08 00:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008855\n",
      "2021-10-08 00:32:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.937791\n",
      "2021-10-08 00:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.035545\n",
      "2021-10-08 00:32:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 100:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:32:33 | INFO | fairseq.trainer | begin training epoch 100\n",
      "epoch 100: 100%|9| 1289/1290 [03:20<00:00,  6.36it/s, loss=3.416, nll_loss=1.6682021-10-08 00:35:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001735\n",
      "2021-10-08 00:35:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063257\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043897\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109818\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001720\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064217\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042701\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109490\n",
      "2021-10-08 00:35:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 100 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.40it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.07it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.82it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.56it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01,  9.98it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 11.62it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 13.14it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 14.48it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 15.65it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.13it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 17.64it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.08it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.77it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:35:55 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 4.628 | nll_loss 2.956 | ppl 7.76 | wps 46860.6 | wpb 2691.4 | bsz 201.4 | num_updates 10315 | best_loss 4.627\n",
      "2021-10-08 00:35:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:35:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint100.pt (epoch 100 @ 10315 updates, score 4.628) (writing took 3.476931626035366 seconds)\n",
      "2021-10-08 00:35:59 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n",
      "2021-10-08 00:35:59 | INFO | train | epoch 100 | loss 3.401 | nll_loss 1.645 | ppl 3.13 | wps 22880.4 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 10315 | lr 0.000311362 | gnorm 1.437 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 00:35:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=101/shard_epoch=8\n",
      "2021-10-08 00:35:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=101/shard_epoch=9\n",
      "2021-10-08 00:35:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:35:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005144\n",
      "2021-10-08 00:35:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116901\n",
      "2021-10-08 00:35:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:35:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[1221]\n",
      "2021-10-08 00:35:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008737\n",
      "2021-10-08 00:35:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.947667\n",
      "2021-10-08 00:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.074287\n",
      "2021-10-08 00:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092380\n",
      "2021-10-08 00:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:36:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[1221]\n",
      "2021-10-08 00:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010861\n",
      "2021-10-08 00:36:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:36:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.940326\n",
      "2021-10-08 00:36:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.044757\n",
      "2021-10-08 00:36:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 101:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:36:01 | INFO | fairseq.trainer | begin training epoch 101\n",
      "epoch 101: 100%|9| 1289/1290 [03:20<00:00,  6.30it/s, loss=3.498, nll_loss=1.7552021-10-08 00:39:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001666\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068101\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046317\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117013\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001689\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064224\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043483\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110327\n",
      "2021-10-08 00:39:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 101 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.51it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.18it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.91it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.60it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.17it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.55it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.40it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.78it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "epoch 101 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:39:23 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 4.595 | nll_loss 2.93 | ppl 7.62 | wps 52238.3 | wpb 2691.4 | bsz 201.4 | num_updates 11605 | best_loss 4.595\n",
      "2021-10-08 00:39:23 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:39:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint101.pt (epoch 101 @ 11605 updates, score 4.595) (writing took 4.802817502000835 seconds)\n",
      "2021-10-08 00:39:28 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)\n",
      "2021-10-08 00:39:28 | INFO | train | epoch 101 | loss 3.357 | nll_loss 1.596 | ppl 3.02 | wps 22779 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 11605 | lr 0.000293547 | gnorm 1.406 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 00:39:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=102/shard_epoch=9\n",
      "2021-10-08 00:39:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=102/shard_epoch=10\n",
      "2021-10-08 00:39:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:39:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004824\n",
      "2021-10-08 00:39:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116577\n",
      "2021-10-08 00:39:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:28 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[307367]\n",
      "2021-10-08 00:39:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010568\n",
      "2021-10-08 00:39:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.944959\n",
      "2021-10-08 00:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.073433\n",
      "2021-10-08 00:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.094676\n",
      "2021-10-08 00:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[307367]\n",
      "2021-10-08 00:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009356\n",
      "2021-10-08 00:39:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:39:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.981093\n",
      "2021-10-08 00:39:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.086177\n",
      "2021-10-08 00:39:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 102:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:39:30 | INFO | fairseq.trainer | begin training epoch 102\n",
      "epoch 102: 100%|9| 1289/1290 [03:20<00:00,  6.61it/s, loss=3.432, nll_loss=1.6782021-10-08 00:42:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001992\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068833\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048352\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120172\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001862\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064686\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043474\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111104\n",
      "2021-10-08 00:42:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 102 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.34it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.10it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.81it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.39it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.82it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 15.08it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.29it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.44it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "epoch 102 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.14it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:42:53 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 4.6 | nll_loss 2.923 | ppl 7.58 | wps 51349.1 | wpb 2691.4 | bsz 201.4 | num_updates 12895 | best_loss 4.595\n",
      "2021-10-08 00:42:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:42:57 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint102.pt (epoch 102 @ 12895 updates, score 4.6) (writing took 3.5303810039767995 seconds)\n",
      "2021-10-08 00:42:57 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)\n",
      "2021-10-08 00:42:57 | INFO | train | epoch 102 | loss 3.322 | nll_loss 1.556 | ppl 2.94 | wps 22884.8 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 12895 | lr 0.000278477 | gnorm 1.418 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 00:42:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=103/shard_epoch=10\n",
      "2021-10-08 00:42:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=103/shard_epoch=11\n",
      "2021-10-08 00:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:42:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004698\n",
      "2021-10-08 00:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106706\n",
      "2021-10-08 00:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:57 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[10852]\n",
      "2021-10-08 00:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008528\n",
      "2021-10-08 00:42:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.943716\n",
      "2021-10-08 00:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.059904\n",
      "2021-10-08 00:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088641\n",
      "2021-10-08 00:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[10852]\n",
      "2021-10-08 00:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008674\n",
      "2021-10-08 00:42:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:42:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.941199\n",
      "2021-10-08 00:42:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.039471\n",
      "2021-10-08 00:42:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 103:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:42:59 | INFO | fairseq.trainer | begin training epoch 103\n",
      "epoch 103: 100%|9| 1289/1290 [03:22<00:00,  6.63it/s, loss=3.296, nll_loss=1.5322021-10-08 00:46:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001990\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068402\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042764\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114398\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001715\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070695\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041578\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114956\n",
      "2021-10-08 00:46:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 103 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.53it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.23it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.89it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.58it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.15it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.49it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.12it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.97it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.23it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.52it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 103 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.30it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:46:23 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 4.6 | nll_loss 2.934 | ppl 7.64 | wps 51756.3 | wpb 2691.4 | bsz 201.4 | num_updates 14185 | best_loss 4.595\n",
      "2021-10-08 00:46:23 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:46:26 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint103.pt (epoch 103 @ 14185 updates, score 4.6) (writing took 3.51068063598359 seconds)\n",
      "2021-10-08 00:46:26 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)\n",
      "2021-10-08 00:46:26 | INFO | train | epoch 103 | loss 3.286 | nll_loss 1.514 | ppl 2.86 | wps 22736.5 | ups 6.15 | wpb 3695.9 | bsz 249.8 | num_updates 14185 | lr 0.000265513 | gnorm 1.399 | loss_scale 4 | train_wall 196 | wall 0\n",
      "2021-10-08 00:46:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=104/shard_epoch=11\n",
      "2021-10-08 00:46:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=104/shard_epoch=12\n",
      "2021-10-08 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:46:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004690\n",
      "2021-10-08 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107304\n",
      "2021-10-08 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:26 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[161346]\n",
      "2021-10-08 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008129\n",
      "2021-10-08 00:46:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.947994\n",
      "2021-10-08 00:46:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.064394\n",
      "2021-10-08 00:46:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:46:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090486\n",
      "2021-10-08 00:46:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:27 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[161346]\n",
      "2021-10-08 00:46:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010305\n",
      "2021-10-08 00:46:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.946632\n",
      "2021-10-08 00:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.048564\n",
      "2021-10-08 00:46:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 104:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:46:28 | INFO | fairseq.trainer | begin training epoch 104\n",
      "epoch 104: 100%|9| 1289/1290 [03:22<00:00,  6.40it/s, loss=3.229, nll_loss=1.4582021-10-08 00:49:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001649\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.078226\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046974\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127886\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001735\n",
      "2021-10-08 00:49:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062573\n",
      "2021-10-08 00:49:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041002\n",
      "2021-10-08 00:49:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106115\n",
      "2021-10-08 00:49:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 104 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.85it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.56it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.29it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.91it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.82it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.69it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.50it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.86it/s]\u001b[A\n",
      "epoch 104 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:49:53 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 4.605 | nll_loss 2.94 | ppl 7.67 | wps 51889.1 | wpb 2691.4 | bsz 201.4 | num_updates 15475 | best_loss 4.595\n",
      "2021-10-08 00:49:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:49:57 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint104.pt (epoch 104 @ 15475 updates, score 4.605) (writing took 3.5362465769867413 seconds)\n",
      "2021-10-08 00:49:57 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)\n",
      "2021-10-08 00:49:57 | INFO | train | epoch 104 | loss 3.257 | nll_loss 1.482 | ppl 2.79 | wps 22662.3 | ups 6.13 | wpb 3695.9 | bsz 249.8 | num_updates 15475 | lr 0.000254205 | gnorm 1.392 | loss_scale 4 | train_wall 196 | wall 0\n",
      "2021-10-08 00:49:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=105/shard_epoch=12\n",
      "2021-10-08 00:49:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=105/shard_epoch=13\n",
      "2021-10-08 00:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:49:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004952\n",
      "2021-10-08 00:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114701\n",
      "2021-10-08 00:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:57 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[317900]\n",
      "2021-10-08 00:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010017\n",
      "2021-10-08 00:49:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.965303\n",
      "2021-10-08 00:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.091066\n",
      "2021-10-08 00:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092080\n",
      "2021-10-08 00:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[317900]\n",
      "2021-10-08 00:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012190\n",
      "2021-10-08 00:49:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:49:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.960821\n",
      "2021-10-08 00:49:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.066274\n",
      "2021-10-08 00:49:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 105:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:49:59 | INFO | fairseq.trainer | begin training epoch 105\n",
      "epoch 105: 100%|9| 1289/1290 [03:22<00:00,  6.41it/s, loss=3.227, nll_loss=1.4512021-10-08 00:53:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001806\n",
      "2021-10-08 00:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066691\n",
      "2021-10-08 00:53:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044237\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.113685\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001756\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064309\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047180\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114212\n",
      "2021-10-08 00:53:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 105 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.65it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.08it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.78it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.33it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.68it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.71it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.41it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.53it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.67it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.83it/s]\u001b[A\n",
      "epoch 105 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:53:23 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 4.617 | nll_loss 2.956 | ppl 7.76 | wps 52296.6 | wpb 2691.4 | bsz 201.4 | num_updates 16765 | best_loss 4.595\n",
      "2021-10-08 00:53:23 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:53:27 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint105.pt (epoch 105 @ 16765 updates, score 4.617) (writing took 3.4975273990421556 seconds)\n",
      "2021-10-08 00:53:27 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)\n",
      "2021-10-08 00:53:27 | INFO | train | epoch 105 | loss 3.228 | nll_loss 1.45 | ppl 2.73 | wps 22696.9 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 16765 | lr 0.00024423 | gnorm 1.392 | loss_scale 4 | train_wall 196 | wall 0\n",
      "2021-10-08 00:53:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=106/shard_epoch=13\n",
      "2021-10-08 00:53:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=106/shard_epoch=14\n",
      "2021-10-08 00:53:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:53:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005846\n",
      "2021-10-08 00:53:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114428\n",
      "2021-10-08 00:53:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:27 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[103361]\n",
      "2021-10-08 00:53:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012758\n",
      "2021-10-08 00:53:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.948963\n",
      "2021-10-08 00:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.077373\n",
      "2021-10-08 00:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090031\n",
      "2021-10-08 00:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:28 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[103361]\n",
      "2021-10-08 00:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009939\n",
      "2021-10-08 00:53:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.943239\n",
      "2021-10-08 00:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.044207\n",
      "2021-10-08 00:53:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 106:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:53:29 | INFO | fairseq.trainer | begin training epoch 106\n",
      "epoch 106: 100%|9| 1289/1290 [03:21<00:00,  6.51it/s, loss=3.341, nll_loss=1.5782021-10-08 00:56:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001743\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066807\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041239\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110737\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001867\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064337\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043064\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110206\n",
      "2021-10-08 00:56:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 106 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.74it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.22it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.92it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.79it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.80it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.64it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.41it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.58it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.91it/s]\u001b[A\n",
      "epoch 106 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 00:56:53 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 4.616 | nll_loss 2.952 | ppl 7.74 | wps 52401.6 | wpb 2691.4 | bsz 201.4 | num_updates 18055 | best_loss 4.595\n",
      "2021-10-08 00:56:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 00:56:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint106.pt (epoch 106 @ 18055 updates, score 4.616) (writing took 3.5564150249701925 seconds)\n",
      "2021-10-08 00:56:56 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)\n",
      "2021-10-08 00:56:56 | INFO | train | epoch 106 | loss 3.208 | nll_loss 1.428 | ppl 2.69 | wps 22747.9 | ups 6.15 | wpb 3695.9 | bsz 249.8 | num_updates 18055 | lr 0.000235343 | gnorm 1.386 | loss_scale 4 | train_wall 195 | wall 0\n",
      "2021-10-08 00:56:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=107/shard_epoch=14\n",
      "2021-10-08 00:56:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=107/shard_epoch=15\n",
      "2021-10-08 00:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:56:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004691\n",
      "2021-10-08 00:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112204\n",
      "2021-10-08 00:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:56 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[129989]\n",
      "2021-10-08 00:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008543\n",
      "2021-10-08 00:56:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.931518\n",
      "2021-10-08 00:56:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.053324\n",
      "2021-10-08 00:56:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 00:56:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084976\n",
      "2021-10-08 00:56:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[129989]\n",
      "2021-10-08 00:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009286\n",
      "2021-10-08 00:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 00:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.924889\n",
      "2021-10-08 00:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.020123\n",
      "2021-10-08 00:56:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 107:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 00:56:58 | INFO | fairseq.trainer | begin training epoch 107\n",
      "epoch 107: 100%|9| 1289/1290 [03:22<00:00,  6.48it/s, loss=3.289, nll_loss=1.5172021-10-08 01:00:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001716\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063985\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042471\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109107\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001956\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065880\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041953\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110846\n",
      "2021-10-08 01:00:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 107 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.15it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.76it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.49it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.22it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.75it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.20it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.36it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.13it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.32it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.57it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 107 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.22it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:00:22 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 4.632 | nll_loss 2.972 | ppl 7.85 | wps 51839.3 | wpb 2691.4 | bsz 201.4 | num_updates 19345 | best_loss 4.595\n",
      "2021-10-08 01:00:22 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:00:26 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint107.pt (epoch 107 @ 19345 updates, score 4.632) (writing took 3.5299884459818713 seconds)\n",
      "2021-10-08 01:00:26 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)\n",
      "2021-10-08 01:00:26 | INFO | train | epoch 107 | loss 3.182 | nll_loss 1.398 | ppl 2.64 | wps 22736.6 | ups 6.15 | wpb 3695.9 | bsz 249.8 | num_updates 19345 | lr 0.000227361 | gnorm 1.377 | loss_scale 4 | train_wall 195 | wall 0\n",
      "2021-10-08 01:00:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=108/shard_epoch=15\n",
      "2021-10-08 01:00:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=108/shard_epoch=16\n",
      "2021-10-08 01:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:00:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004651\n",
      "2021-10-08 01:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105386\n",
      "2021-10-08 01:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:26 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[56238]\n",
      "2021-10-08 01:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012444\n",
      "2021-10-08 01:00:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.953739\n",
      "2021-10-08 01:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.072758\n",
      "2021-10-08 01:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088554\n",
      "2021-10-08 01:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:27 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[56238]\n",
      "2021-10-08 01:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009354\n",
      "2021-10-08 01:00:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:00:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.001818\n",
      "2021-10-08 01:00:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.100699\n",
      "2021-10-08 01:00:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 108:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:00:28 | INFO | fairseq.trainer | begin training epoch 108\n",
      "epoch 108: 100%|9| 1289/1290 [03:24<00:00,  6.69it/s, loss=3.113, nll_loss=1.3332021-10-08 01:03:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001806\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065672\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043029\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111482\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001847\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071130\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042815\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116724\n",
      "2021-10-08 01:03:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 108 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.44it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.10it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.80it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:02,  9.04it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 10.73it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 12.29it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 13.67it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 14.94it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.07it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.42it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 17.91it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.33it/s]\u001b[A\n",
      "epoch 108 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:03:55 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 4.631 | nll_loss 2.973 | ppl 7.85 | wps 46399 | wpb 2691.4 | bsz 201.4 | num_updates 20635 | best_loss 4.595\n",
      "2021-10-08 01:03:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:03:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint108.pt (epoch 108 @ 20635 updates, score 4.631) (writing took 3.4999144190223888 seconds)\n",
      "2021-10-08 01:03:58 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)\n",
      "2021-10-08 01:03:58 | INFO | train | epoch 108 | loss 3.163 | nll_loss 1.377 | ppl 2.6 | wps 22482.7 | ups 6.08 | wpb 3695.9 | bsz 249.8 | num_updates 20635 | lr 0.000220139 | gnorm 1.385 | loss_scale 8 | train_wall 197 | wall 0\n",
      "2021-10-08 01:03:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=109/shard_epoch=16\n",
      "2021-10-08 01:03:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=109/shard_epoch=17\n",
      "2021-10-08 01:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:03:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004915\n",
      "2021-10-08 01:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109630\n",
      "2021-10-08 01:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:03:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[149251]\n",
      "2021-10-08 01:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008257\n",
      "2021-10-08 01:03:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:03:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.968057\n",
      "2021-10-08 01:03:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.087018\n",
      "2021-10-08 01:03:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:03:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:03:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091277\n",
      "2021-10-08 01:03:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:03:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[149251]\n",
      "2021-10-08 01:03:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008519\n",
      "2021-10-08 01:03:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.964029\n",
      "2021-10-08 01:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.064775\n",
      "2021-10-08 01:04:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 109:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:04:00 | INFO | fairseq.trainer | begin training epoch 109\n",
      "epoch 109: 100%|9| 1289/1290 [03:22<00:00,  6.25it/s, loss=3.249, nll_loss=1.4752021-10-08 01:07:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001685\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070110\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041984\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114774\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001566\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064945\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044919\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112413\n",
      "2021-10-08 01:07:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 109 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.87it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.57it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.28it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.96it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.44it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.77it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.79it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.24it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.38it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.58it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "epoch 109 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:07:25 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 4.641 | nll_loss 2.979 | ppl 7.88 | wps 51501.1 | wpb 2691.4 | bsz 201.4 | num_updates 21925 | best_loss 4.595\n",
      "2021-10-08 01:07:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:07:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint109.pt (epoch 109 @ 21925 updates, score 4.641) (writing took 3.5626812989939936 seconds)\n",
      "2021-10-08 01:07:28 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)\n",
      "2021-10-08 01:07:28 | INFO | train | epoch 109 | loss 3.145 | nll_loss 1.358 | ppl 2.56 | wps 22683.7 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 21925 | lr 0.000213565 | gnorm 1.379 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 01:07:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=110/shard_epoch=17\n",
      "2021-10-08 01:07:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=110/shard_epoch=18\n",
      "2021-10-08 01:07:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:07:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004677\n",
      "2021-10-08 01:07:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106229\n",
      "2021-10-08 01:07:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:28 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[285728]\n",
      "2021-10-08 01:07:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009798\n",
      "2021-10-08 01:07:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.910081\n",
      "2021-10-08 01:07:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.027104\n",
      "2021-10-08 01:07:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:07:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088372\n",
      "2021-10-08 01:07:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[285728]\n",
      "2021-10-08 01:07:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011508\n",
      "2021-10-08 01:07:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.903562\n",
      "2021-10-08 01:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.004455\n",
      "2021-10-08 01:07:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 110:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:07:30 | INFO | fairseq.trainer | begin training epoch 110\n",
      "epoch 110: 100%|9| 1289/1290 [03:25<00:00,  6.44it/s, loss=3.149, nll_loss=1.3682021-10-08 01:10:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001961\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065701\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044034\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112713\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001617\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069311\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043736\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115671\n",
      "2021-10-08 01:10:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 110 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.63it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.31it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.04it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.76it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.33it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.66it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.53it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.30it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.48it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.58it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.78it/s]\u001b[A\n",
      "epoch 110 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:10:58 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 4.646 | nll_loss 2.988 | ppl 7.94 | wps 51687.5 | wpb 2691.4 | bsz 201.4 | num_updates 23215 | best_loss 4.595\n",
      "2021-10-08 01:10:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:11:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint110.pt (epoch 110 @ 23215 updates, score 4.646) (writing took 3.5448013179702684 seconds)\n",
      "2021-10-08 01:11:01 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)\n",
      "2021-10-08 01:11:01 | INFO | train | epoch 110 | loss 3.126 | nll_loss 1.336 | ppl 2.53 | wps 22395 | ups 6.06 | wpb 3695.9 | bsz 249.8 | num_updates 23215 | lr 0.000207547 | gnorm 1.387 | loss_scale 8 | train_wall 198 | wall 0\n",
      "2021-10-08 01:11:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=111/shard_epoch=18\n",
      "2021-10-08 01:11:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=111/shard_epoch=19\n",
      "2021-10-08 01:11:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:11:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006060\n",
      "2021-10-08 01:11:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.122027\n",
      "2021-10-08 01:11:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:11:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[184661]\n",
      "2021-10-08 01:11:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013052\n",
      "2021-10-08 01:11:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.962479\n",
      "2021-10-08 01:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.098621\n",
      "2021-10-08 01:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092901\n",
      "2021-10-08 01:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:11:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[184661]\n",
      "2021-10-08 01:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009902\n",
      "2021-10-08 01:11:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.952114\n",
      "2021-10-08 01:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.055971\n",
      "2021-10-08 01:11:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 111:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:11:03 | INFO | fairseq.trainer | begin training epoch 111\n",
      "epoch 111: 100%|9| 1289/1290 [03:22<00:00,  6.26it/s, loss=3.114, nll_loss=1.3282021-10-08 01:14:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001816\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064155\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041794\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108712\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001982\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065682\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040913\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109561\n",
      "2021-10-08 01:14:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 111 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.08it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.85it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.56it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.15it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.54it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.87it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.90it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.72it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.41it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.74it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.92it/s]\u001b[A\n",
      "epoch 111 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:14:28 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 4.64 | nll_loss 2.985 | ppl 7.92 | wps 51946.2 | wpb 2691.4 | bsz 201.4 | num_updates 24505 | best_loss 4.595\n",
      "2021-10-08 01:14:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:14:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint111.pt (epoch 111 @ 24505 updates, score 4.64) (writing took 3.454715939005837 seconds)\n",
      "2021-10-08 01:14:32 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)\n",
      "2021-10-08 01:14:32 | INFO | train | epoch 111 | loss 3.109 | nll_loss 1.317 | ppl 2.49 | wps 22663 | ups 6.13 | wpb 3695.9 | bsz 249.8 | num_updates 24505 | lr 0.00020201 | gnorm 1.379 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 01:14:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=112/shard_epoch=19\n",
      "2021-10-08 01:14:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=112/shard_epoch=20\n",
      "2021-10-08 01:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:14:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004695\n",
      "2021-10-08 01:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112158\n",
      "2021-10-08 01:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[257667]\n",
      "2021-10-08 01:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008798\n",
      "2021-10-08 01:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.935785\n",
      "2021-10-08 01:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.057798\n",
      "2021-10-08 01:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086238\n",
      "2021-10-08 01:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:33 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[257667]\n",
      "2021-10-08 01:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009622\n",
      "2021-10-08 01:14:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:14:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.921795\n",
      "2021-10-08 01:14:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.018623\n",
      "2021-10-08 01:14:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 112:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:14:34 | INFO | fairseq.trainer | begin training epoch 112\n",
      "epoch 112: 100%|9| 1289/1290 [03:21<00:00,  6.78it/s, loss=3.216, nll_loss=1.4372021-10-08 01:17:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001804\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063514\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042114\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108370\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001732\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063757\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041466\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107923\n",
      "2021-10-08 01:17:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 112 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.38it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.07it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.73it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.42it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.03it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.46it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.54it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.37it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.74it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.87it/s]\u001b[A\n",
      "epoch 112 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:17:57 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 4.627 | nll_loss 2.97 | ppl 7.84 | wps 52132.8 | wpb 2691.4 | bsz 201.4 | num_updates 25795 | best_loss 4.595\n",
      "2021-10-08 01:17:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:18:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint112.pt (epoch 112 @ 25795 updates, score 4.627) (writing took 3.4312246490153484 seconds)\n",
      "2021-10-08 01:18:00 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)\n",
      "2021-10-08 01:18:00 | INFO | train | epoch 112 | loss 3.093 | nll_loss 1.3 | ppl 2.46 | wps 22822.8 | ups 6.18 | wpb 3695.9 | bsz 249.8 | num_updates 25795 | lr 0.000196894 | gnorm 1.383 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 01:18:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=113/shard_epoch=20\n",
      "2021-10-08 01:18:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=113/shard_epoch=21\n",
      "2021-10-08 01:18:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:18:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005707\n",
      "2021-10-08 01:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116361\n",
      "2021-10-08 01:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:18:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[302939]\n",
      "2021-10-08 01:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009389\n",
      "2021-10-08 01:18:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.008794\n",
      "2021-10-08 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.135503\n",
      "2021-10-08 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090591\n",
      "2021-10-08 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:18:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[302939]\n",
      "2021-10-08 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009812\n",
      "2021-10-08 01:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.975362\n",
      "2021-10-08 01:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.076848\n",
      "2021-10-08 01:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 113:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:18:03 | INFO | fairseq.trainer | begin training epoch 113\n",
      "epoch 113: 100%|9| 1289/1290 [03:23<00:00,  6.43it/s, loss=3.08, nll_loss=1.289,2021-10-08 01:21:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001796\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062948\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042564\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108216\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001685\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062251\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040405\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105188\n",
      "2021-10-08 01:21:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 113 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.38it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.04it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.74it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.43it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.91it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.21it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.13it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.23it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.37it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.65it/s]\u001b[A\n",
      "epoch 113 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:21:28 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 4.644 | nll_loss 2.982 | ppl 7.9 | wps 51171.2 | wpb 2691.4 | bsz 201.4 | num_updates 27085 | best_loss 4.595\n",
      "2021-10-08 01:21:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:21:32 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint113.pt (epoch 113 @ 27085 updates, score 4.644) (writing took 3.541030024003703 seconds)\n",
      "2021-10-08 01:21:32 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)\n",
      "2021-10-08 01:21:32 | INFO | train | epoch 113 | loss 3.078 | nll_loss 1.283 | ppl 2.43 | wps 22572.7 | ups 6.11 | wpb 3695.9 | bsz 249.8 | num_updates 27085 | lr 0.000192148 | gnorm 1.391 | loss_scale 8 | train_wall 197 | wall 0\n",
      "2021-10-08 01:21:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=114/shard_epoch=21\n",
      "2021-10-08 01:21:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=114/shard_epoch=22\n",
      "2021-10-08 01:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:21:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004809\n",
      "2021-10-08 01:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111387\n",
      "2021-10-08 01:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[303760]\n",
      "2021-10-08 01:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008603\n",
      "2021-10-08 01:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.967080\n",
      "2021-10-08 01:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.088065\n",
      "2021-10-08 01:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.100271\n",
      "2021-10-08 01:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:33 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[303760]\n",
      "2021-10-08 01:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008553\n",
      "2021-10-08 01:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.942151\n",
      "2021-10-08 01:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.051939\n",
      "2021-10-08 01:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 114:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:21:34 | INFO | fairseq.trainer | begin training epoch 114\n",
      "epoch 114: 100%|9| 1289/1290 [03:22<00:00,  6.66it/s, loss=3.104, nll_loss=1.3182021-10-08 01:24:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001733\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064089\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043201\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109936\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001993\n",
      "2021-10-08 01:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064927\n",
      "2021-10-08 01:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041615\n",
      "2021-10-08 01:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109473\n",
      "2021-10-08 01:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 114 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.75it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.33it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.02it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.75it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.40it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.83it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.99it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.88it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.10it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.41it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 114 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:24:58 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 4.636 | nll_loss 2.978 | ppl 7.88 | wps 51950.3 | wpb 2691.4 | bsz 201.4 | num_updates 28375 | best_loss 4.595\n",
      "2021-10-08 01:24:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:25:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint114.pt (epoch 114 @ 28375 updates, score 4.636) (writing took 3.438285532000009 seconds)\n",
      "2021-10-08 01:25:02 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)\n",
      "2021-10-08 01:25:02 | INFO | train | epoch 114 | loss 3.064 | nll_loss 1.267 | ppl 2.41 | wps 22706.5 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 28375 | lr 0.000187729 | gnorm 1.39 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 01:25:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=115/shard_epoch=22\n",
      "2021-10-08 01:25:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=115/shard_epoch=23\n",
      "2021-10-08 01:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:25:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004679\n",
      "2021-10-08 01:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112147\n",
      "2021-10-08 01:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:25:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[246483]\n",
      "2021-10-08 01:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008859\n",
      "2021-10-08 01:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.973749\n",
      "2021-10-08 01:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.095757\n",
      "2021-10-08 01:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.098147\n",
      "2021-10-08 01:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:25:03 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[246483]\n",
      "2021-10-08 01:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012407\n",
      "2021-10-08 01:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.965208\n",
      "2021-10-08 01:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.076794\n",
      "2021-10-08 01:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 115:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:25:04 | INFO | fairseq.trainer | begin training epoch 115\n",
      "epoch 115: 100%|9| 1289/1290 [03:21<00:00,  6.43it/s, loss=3.088, nll_loss=1.2992021-10-08 01:28:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001782\n",
      "2021-10-08 01:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071166\n",
      "2021-10-08 01:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.055403\n",
      "2021-10-08 01:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129380\n",
      "2021-10-08 01:28:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001860\n",
      "2021-10-08 01:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069772\n",
      "2021-10-08 01:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058380\n",
      "2021-10-08 01:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.130985\n",
      "2021-10-08 01:28:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 115 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.98it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.72it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.46it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.12it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.53it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.83it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.26it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.41it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.02it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.31it/s]\u001b[A\n",
      "epoch 115 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.92it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:28:27 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 4.634 | nll_loss 2.977 | ppl 7.87 | wps 51229.3 | wpb 2691.4 | bsz 201.4 | num_updates 29665 | best_loss 4.595\n",
      "2021-10-08 01:28:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:28:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint115.pt (epoch 115 @ 29665 updates, score 4.634) (writing took 3.4738175800302997 seconds)\n",
      "2021-10-08 01:28:31 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)\n",
      "2021-10-08 01:28:31 | INFO | train | epoch 115 | loss 3.052 | nll_loss 1.254 | ppl 2.39 | wps 22800.5 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 29665 | lr 0.000183602 | gnorm 1.392 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 01:28:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=116/shard_epoch=23\n",
      "2021-10-08 01:28:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=116/shard_epoch=24\n",
      "2021-10-08 01:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:28:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005516\n",
      "2021-10-08 01:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109995\n",
      "2021-10-08 01:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[31246]\n",
      "2021-10-08 01:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010865\n",
      "2021-10-08 01:28:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.925968\n",
      "2021-10-08 01:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.048217\n",
      "2021-10-08 01:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089827\n",
      "2021-10-08 01:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[31246]\n",
      "2021-10-08 01:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009841\n",
      "2021-10-08 01:28:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.932113\n",
      "2021-10-08 01:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.032740\n",
      "2021-10-08 01:28:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 116:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:28:33 | INFO | fairseq.trainer | begin training epoch 116\n",
      "epoch 116: 100%|9| 1289/1290 [03:22<00:00,  6.29it/s, loss=3.095, nll_loss=1.3052021-10-08 01:31:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002153\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063213\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043900\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110216\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001987\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067475\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046874\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117448\n",
      "2021-10-08 01:31:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 116 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.49it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.14it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.86it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.54it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.11it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.44it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.53it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.22it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.41it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.59it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.60it/s]\u001b[A\n",
      "epoch 116 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:31:57 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 4.663 | nll_loss 3.013 | ppl 8.07 | wps 51573.1 | wpb 2691.4 | bsz 201.4 | num_updates 30955 | best_loss 4.595\n",
      "2021-10-08 01:31:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:32:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint116.pt (epoch 116 @ 30955 updates, score 4.663) (writing took 3.444454758951906 seconds)\n",
      "2021-10-08 01:32:01 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)\n",
      "2021-10-08 01:32:01 | INFO | train | epoch 116 | loss 3.04 | nll_loss 1.241 | ppl 2.36 | wps 22673 | ups 6.13 | wpb 3695.9 | bsz 249.8 | num_updates 30955 | lr 0.000179736 | gnorm 1.399 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 01:32:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=117/shard_epoch=24\n",
      "2021-10-08 01:32:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=117/shard_epoch=25\n",
      "2021-10-08 01:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:32:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004749\n",
      "2021-10-08 01:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110479\n",
      "2021-10-08 01:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:32:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[42598]\n",
      "2021-10-08 01:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010797\n",
      "2021-10-08 01:32:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.933573\n",
      "2021-10-08 01:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.055902\n",
      "2021-10-08 01:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087975\n",
      "2021-10-08 01:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:32:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[42598]\n",
      "2021-10-08 01:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008618\n",
      "2021-10-08 01:32:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.928649\n",
      "2021-10-08 01:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.026233\n",
      "2021-10-08 01:32:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 117:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:32:03 | INFO | fairseq.trainer | begin training epoch 117\n",
      "epoch 117: 100%|9| 1289/1290 [03:21<00:00,  6.69it/s, loss=2.971, nll_loss=1.17,2021-10-08 01:35:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001629\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062931\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042058\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107544\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001606\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064048\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041662\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108323\n",
      "2021-10-08 01:35:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 117 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.37it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.04it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.77it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.49it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.04it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.42it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.52it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.18it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.42it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.88it/s]\u001b[A\n",
      "epoch 117 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.36it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:35:27 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 4.673 | nll_loss 3.02 | ppl 8.11 | wps 52161 | wpb 2691.4 | bsz 201.4 | num_updates 32245 | best_loss 4.595\n",
      "2021-10-08 01:35:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:35:30 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint117.pt (epoch 117 @ 32245 updates, score 4.673) (writing took 3.4765260400017723 seconds)\n",
      "2021-10-08 01:35:30 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)\n",
      "2021-10-08 01:35:30 | INFO | train | epoch 117 | loss 3.026 | nll_loss 1.226 | ppl 2.34 | wps 22779.6 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 32245 | lr 0.000176104 | gnorm 1.396 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 01:35:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=118/shard_epoch=25\n",
      "2021-10-08 01:35:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=118/shard_epoch=26\n",
      "2021-10-08 01:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:35:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004828\n",
      "2021-10-08 01:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113220\n",
      "2021-10-08 01:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[160453]\n",
      "2021-10-08 01:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009100\n",
      "2021-10-08 01:35:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.924223\n",
      "2021-10-08 01:35:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.047566\n",
      "2021-10-08 01:35:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:35:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.094921\n",
      "2021-10-08 01:35:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[160453]\n",
      "2021-10-08 01:35:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009544\n",
      "2021-10-08 01:35:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.926734\n",
      "2021-10-08 01:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.032226\n",
      "2021-10-08 01:35:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 118:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:35:32 | INFO | fairseq.trainer | begin training epoch 118\n",
      "epoch 118: 100%|9| 1289/1290 [03:21<00:00,  6.29it/s, loss=3.012, nll_loss=1.2152021-10-08 01:38:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001941\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.072534\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042174\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117609\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001740\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063181\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041518\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107393\n",
      "2021-10-08 01:38:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 118 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.18it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.93it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.69it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.36it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 15.12it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 16.10it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.88it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.56it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.65it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.86it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.95it/s]\u001b[A\n",
      "epoch 118 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:38:56 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 4.64 | nll_loss 2.985 | ppl 7.92 | wps 51992.3 | wpb 2691.4 | bsz 201.4 | num_updates 33535 | best_loss 4.595\n",
      "2021-10-08 01:38:56 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:38:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint118.pt (epoch 118 @ 33535 updates, score 4.64) (writing took 3.474589064018801 seconds)\n",
      "2021-10-08 01:38:59 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)\n",
      "2021-10-08 01:38:59 | INFO | train | epoch 118 | loss 3.016 | nll_loss 1.215 | ppl 2.32 | wps 22821.8 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 33535 | lr 0.000172684 | gnorm 1.4 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 01:38:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=119/shard_epoch=26\n",
      "2021-10-08 01:38:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=119/shard_epoch=27\n",
      "2021-10-08 01:38:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:38:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005766\n",
      "2021-10-08 01:38:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109795\n",
      "2021-10-08 01:38:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:38:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[244144]\n",
      "2021-10-08 01:38:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009812\n",
      "2021-10-08 01:38:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.924527\n",
      "2021-10-08 01:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.045179\n",
      "2021-10-08 01:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086553\n",
      "2021-10-08 01:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:39:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[244144]\n",
      "2021-10-08 01:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011314\n",
      "2021-10-08 01:39:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.977068\n",
      "2021-10-08 01:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.075940\n",
      "2021-10-08 01:39:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 119:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:39:01 | INFO | fairseq.trainer | begin training epoch 119\n",
      "epoch 119: 100%|9| 1289/1290 [03:21<00:00,  6.66it/s, loss=3.048, nll_loss=1.2532021-10-08 01:42:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001734\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064391\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043937\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110951\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001663\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064947\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042436\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109956\n",
      "2021-10-08 01:42:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 119 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.78it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.47it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.23it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.95it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.51it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.79it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.63it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.40it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.61it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.76it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.87it/s]\u001b[A\n",
      "epoch 119 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.43it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:42:25 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 4.663 | nll_loss 3.017 | ppl 8.09 | wps 52089.9 | wpb 2691.4 | bsz 201.4 | num_updates 34825 | best_loss 4.595\n",
      "2021-10-08 01:42:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:42:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint119.pt (epoch 119 @ 34825 updates, score 4.663) (writing took 3.5763133239815943 seconds)\n",
      "2021-10-08 01:42:28 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)\n",
      "2021-10-08 01:42:28 | INFO | train | epoch 119 | loss 3.004 | nll_loss 1.201 | ppl 2.3 | wps 22812.3 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 34825 | lr 0.000169455 | gnorm 1.403 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 01:42:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=120/shard_epoch=27\n",
      "2021-10-08 01:42:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=120/shard_epoch=28\n",
      "2021-10-08 01:42:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:42:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004956\n",
      "2021-10-08 01:42:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.118559\n",
      "2021-10-08 01:42:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:28 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[244371]\n",
      "2021-10-08 01:42:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010692\n",
      "2021-10-08 01:42:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.931039\n",
      "2021-10-08 01:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.061279\n",
      "2021-10-08 01:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091049\n",
      "2021-10-08 01:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[244371]\n",
      "2021-10-08 01:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012517\n",
      "2021-10-08 01:42:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.932596\n",
      "2021-10-08 01:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.037115\n",
      "2021-10-08 01:42:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 120:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:42:30 | INFO | fairseq.trainer | begin training epoch 120\n",
      "epoch 120: 100%|9| 1289/1290 [03:21<00:00,  6.87it/s, loss=2.983, nll_loss=1.1852021-10-08 01:45:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001886\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064065\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044665\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111637\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001612\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063668\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041130\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107252\n",
      "2021-10-08 01:45:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 120 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.42it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.08it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.81it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.50it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.09it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.46it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.51it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.18it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.46it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.75it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.58it/s]\u001b[A\n",
      "epoch 120 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.06it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:45:54 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 4.652 | nll_loss 2.998 | ppl 7.99 | wps 51655.6 | wpb 2691.4 | bsz 201.4 | num_updates 36115 | best_loss 4.595\n",
      "2021-10-08 01:45:54 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:45:57 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint120.pt (epoch 120 @ 36115 updates, score 4.652) (writing took 3.524727441952564 seconds)\n",
      "2021-10-08 01:45:57 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)\n",
      "2021-10-08 01:45:57 | INFO | train | epoch 120 | loss 2.995 | nll_loss 1.191 | ppl 2.28 | wps 22808 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 36115 | lr 0.000166401 | gnorm 1.409 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 01:45:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=121/shard_epoch=28\n",
      "2021-10-08 01:45:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=121/shard_epoch=29\n",
      "2021-10-08 01:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:45:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004687\n",
      "2021-10-08 01:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107594\n",
      "2021-10-08 01:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:57 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[64082]\n",
      "2021-10-08 01:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008746\n",
      "2021-10-08 01:45:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.919847\n",
      "2021-10-08 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.037150\n",
      "2021-10-08 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087884\n",
      "2021-10-08 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[64082]\n",
      "2021-10-08 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010323\n",
      "2021-10-08 01:45:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.003394\n",
      "2021-10-08 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.102616\n",
      "2021-10-08 01:45:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 121:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:45:59 | INFO | fairseq.trainer | begin training epoch 121\n",
      "epoch 121: 100%|9| 1289/1290 [03:23<00:00,  6.62it/s, loss=3.014, nll_loss=1.2152021-10-08 01:49:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001715\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070651\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047118\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120447\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001834\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066469\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041418\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110793\n",
      "2021-10-08 01:49:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 121 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.41it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.08it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.81it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.49it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.03it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.35it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.45it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.20it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.32it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.79it/s]\u001b[A\n",
      "epoch 121 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:49:25 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 4.668 | nll_loss 3.021 | ppl 8.11 | wps 51613.1 | wpb 2691.4 | bsz 201.4 | num_updates 37405 | best_loss 4.595\n",
      "2021-10-08 01:49:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:49:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint121.pt (epoch 121 @ 37405 updates, score 4.668) (writing took 3.4856506710057147 seconds)\n",
      "2021-10-08 01:49:28 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)\n",
      "2021-10-08 01:49:28 | INFO | train | epoch 121 | loss 2.985 | nll_loss 1.179 | ppl 2.26 | wps 22598.2 | ups 6.11 | wpb 3695.9 | bsz 249.8 | num_updates 37405 | lr 0.000163507 | gnorm 1.42 | loss_scale 16 | train_wall 197 | wall 0\n",
      "2021-10-08 01:49:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=122/shard_epoch=29\n",
      "2021-10-08 01:49:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=122/shard_epoch=30\n",
      "2021-10-08 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:49:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005036\n",
      "2021-10-08 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.118065\n",
      "2021-10-08 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:28 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[153074]\n",
      "2021-10-08 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011245\n",
      "2021-10-08 01:49:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.974093\n",
      "2021-10-08 01:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.104398\n",
      "2021-10-08 01:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092268\n",
      "2021-10-08 01:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[153074]\n",
      "2021-10-08 01:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010520\n",
      "2021-10-08 01:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.941269\n",
      "2021-10-08 01:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.045049\n",
      "2021-10-08 01:49:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 122:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:49:30 | INFO | fairseq.trainer | begin training epoch 122\n",
      "epoch 122: 100%|9| 1289/1290 [03:23<00:00,  6.80it/s, loss=2.997, nll_loss=1.1962021-10-08 01:52:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001770\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063978\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042929\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109618\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001799\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063937\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041615\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108288\n",
      "2021-10-08 01:52:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 122 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.69it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.39it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.15it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.81it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.33it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.54it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.63it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.53it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.32it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.49it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.90it/s]\u001b[A\n",
      "epoch 122 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:52:55 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 4.667 | nll_loss 3.014 | ppl 8.08 | wps 51864.6 | wpb 2691.4 | bsz 201.4 | num_updates 38695 | best_loss 4.595\n",
      "2021-10-08 01:52:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:52:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint122.pt (epoch 122 @ 38695 updates, score 4.667) (writing took 3.487303828995209 seconds)\n",
      "2021-10-08 01:52:59 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)\n",
      "2021-10-08 01:52:59 | INFO | train | epoch 122 | loss 2.977 | nll_loss 1.171 | ppl 2.25 | wps 22618.3 | ups 6.12 | wpb 3695.9 | bsz 249.8 | num_updates 38695 | lr 0.000160758 | gnorm 1.417 | loss_scale 16 | train_wall 197 | wall 0\n",
      "2021-10-08 01:52:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=123/shard_epoch=30\n",
      "2021-10-08 01:52:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=123/shard_epoch=31\n",
      "2021-10-08 01:52:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:52:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004762\n",
      "2021-10-08 01:52:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108752\n",
      "2021-10-08 01:52:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:52:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[148521]\n",
      "2021-10-08 01:52:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008424\n",
      "2021-10-08 01:52:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.975137\n",
      "2021-10-08 01:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.093381\n",
      "2021-10-08 01:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.095581\n",
      "2021-10-08 01:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:53:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[148521]\n",
      "2021-10-08 01:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009838\n",
      "2021-10-08 01:53:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:53:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.975547\n",
      "2021-10-08 01:53:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.081987\n",
      "2021-10-08 01:53:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 123:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:53:01 | INFO | fairseq.trainer | begin training epoch 123\n",
      "epoch 123: 100%|9| 1289/1290 [03:22<00:00,  6.38it/s, loss=2.997, nll_loss=1.1972021-10-08 01:56:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002021\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064735\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042438\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110621\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001925\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064273\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043598\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110953\n",
      "2021-10-08 01:56:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 123 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.17it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.82it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.54it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.24it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.86it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.28it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.41it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.16it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.28it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.42it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.62it/s]\u001b[A\n",
      "epoch 123 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:56:25 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 4.684 | nll_loss 3.046 | ppl 8.26 | wps 51516.8 | wpb 2691.4 | bsz 201.4 | num_updates 39985 | best_loss 4.595\n",
      "2021-10-08 01:56:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 01:56:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint123.pt (epoch 123 @ 39985 updates, score 4.684) (writing took 3.511509298987221 seconds)\n",
      "2021-10-08 01:56:29 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)\n",
      "2021-10-08 01:56:29 | INFO | train | epoch 123 | loss 2.966 | nll_loss 1.159 | ppl 2.23 | wps 22714.8 | ups 6.15 | wpb 3695.9 | bsz 249.8 | num_updates 39985 | lr 0.000158144 | gnorm 1.427 | loss_scale 16 | train_wall 196 | wall 0\n",
      "2021-10-08 01:56:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=124/shard_epoch=31\n",
      "2021-10-08 01:56:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=124/shard_epoch=32\n",
      "2021-10-08 01:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:56:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004750\n",
      "2021-10-08 01:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111257\n",
      "2021-10-08 01:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[92473]\n",
      "2021-10-08 01:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009514\n",
      "2021-10-08 01:56:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.936766\n",
      "2021-10-08 01:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.058687\n",
      "2021-10-08 01:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.093859\n",
      "2021-10-08 01:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[92473]\n",
      "2021-10-08 01:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008664\n",
      "2021-10-08 01:56:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.936052\n",
      "2021-10-08 01:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.039612\n",
      "2021-10-08 01:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 124:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 01:56:31 | INFO | fairseq.trainer | begin training epoch 124\n",
      "epoch 124: 100%|9| 1289/1290 [03:23<00:00,  6.52it/s, loss=2.969, nll_loss=1.1632021-10-08 01:59:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 01:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002090\n",
      "2021-10-08 01:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065688\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043662\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112487\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002012\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064392\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043556\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110969\n",
      "2021-10-08 01:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 124 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.98it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.60it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.33it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.05it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.65it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 10.58it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  54%|###2  | 15/28 [00:01<00:01, 12.16it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  61%|###6  | 17/28 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 15.37it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 16.42it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 17.24it/s]\u001b[A\n",
      "epoch 124 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 17.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 01:59:56 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 4.651 | nll_loss 2.998 | ppl 7.99 | wps 46648.8 | wpb 2691.4 | bsz 201.4 | num_updates 41275 | best_loss 4.595\n",
      "2021-10-08 01:59:56 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:00:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint124.pt (epoch 124 @ 41275 updates, score 4.651) (writing took 3.506914650031831 seconds)\n",
      "2021-10-08 02:00:00 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)\n",
      "2021-10-08 02:00:00 | INFO | train | epoch 124 | loss 2.959 | nll_loss 1.151 | ppl 2.22 | wps 22586.2 | ups 6.11 | wpb 3695.9 | bsz 249.8 | num_updates 41275 | lr 0.000155653 | gnorm 1.434 | loss_scale 16 | train_wall 197 | wall 0\n",
      "2021-10-08 02:00:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=125/shard_epoch=32\n",
      "2021-10-08 02:00:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=125/shard_epoch=33\n",
      "2021-10-08 02:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:00:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006005\n",
      "2021-10-08 02:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111349\n",
      "2021-10-08 02:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:00:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[89725]\n",
      "2021-10-08 02:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010454\n",
      "2021-10-08 02:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.940303\n",
      "2021-10-08 02:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.063055\n",
      "2021-10-08 02:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.093193\n",
      "2021-10-08 02:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:00:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[89725]\n",
      "2021-10-08 02:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010355\n",
      "2021-10-08 02:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.952559\n",
      "2021-10-08 02:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.057128\n",
      "2021-10-08 02:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 125:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:00:02 | INFO | fairseq.trainer | begin training epoch 125\n",
      "epoch 125: 100%|9| 1289/1290 [03:24<00:00,  6.41it/s, loss=2.914, nll_loss=1.1072021-10-08 02:03:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002207\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070238\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041503\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115141\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001766\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066338\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045470\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114650\n",
      "2021-10-08 02:03:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 125 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.75it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.48it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.23it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.92it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.44it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.75it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.26it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.27it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.49it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.68it/s]\u001b[A\n",
      "epoch 125 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:03:28 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 4.684 | nll_loss 3.03 | ppl 8.17 | wps 51483.1 | wpb 2691.4 | bsz 201.4 | num_updates 42565 | best_loss 4.595\n",
      "2021-10-08 02:03:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:03:32 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint125.pt (epoch 125 @ 42565 updates, score 4.684) (writing took 3.4997571249841712 seconds)\n",
      "2021-10-08 02:03:32 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)\n",
      "2021-10-08 02:03:32 | INFO | train | epoch 125 | loss 2.951 | nll_loss 1.142 | ppl 2.21 | wps 22485.4 | ups 6.08 | wpb 3695.9 | bsz 249.8 | num_updates 42565 | lr 0.000153276 | gnorm 1.438 | loss_scale 16 | train_wall 197 | wall 0\n",
      "2021-10-08 02:03:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=126/shard_epoch=33\n",
      "2021-10-08 02:03:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=126/shard_epoch=34\n",
      "2021-10-08 02:03:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:03:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005095\n",
      "2021-10-08 02:03:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117402\n",
      "2021-10-08 02:03:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[2431]\n",
      "2021-10-08 02:03:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009848\n",
      "2021-10-08 02:03:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.944147\n",
      "2021-10-08 02:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.072576\n",
      "2021-10-08 02:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.093509\n",
      "2021-10-08 02:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:33 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[2431]\n",
      "2021-10-08 02:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009723\n",
      "2021-10-08 02:03:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.986859\n",
      "2021-10-08 02:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.091333\n",
      "2021-10-08 02:03:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 126:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:03:34 | INFO | fairseq.trainer | begin training epoch 126\n",
      "epoch 126: 100%|9| 1289/1290 [03:21<00:00,  6.30it/s, loss=2.951, nll_loss=1.1442021-10-08 02:06:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001966\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069493\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043267\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115736\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001678\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067526\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042315\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112531\n",
      "2021-10-08 02:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 126 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.84it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.45it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.16it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.88it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.51it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.98it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.75it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.93it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.23it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.54it/s]\u001b[A\n",
      "epoch 126 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:06:58 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 4.668 | nll_loss 3.019 | ppl 8.11 | wps 51501.8 | wpb 2691.4 | bsz 201.4 | num_updates 43855 | best_loss 4.595\n",
      "2021-10-08 02:06:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:07:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint126.pt (epoch 126 @ 43855 updates, score 4.668) (writing took 3.613388698955532 seconds)\n",
      "2021-10-08 02:07:01 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)\n",
      "2021-10-08 02:07:01 | INFO | train | epoch 126 | loss 2.943 | nll_loss 1.134 | ppl 2.19 | wps 22790.5 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 43855 | lr 0.000151005 | gnorm 1.441 | loss_scale 16 | train_wall 195 | wall 0\n",
      "2021-10-08 02:07:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=127/shard_epoch=34\n",
      "2021-10-08 02:07:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=127/shard_epoch=35\n",
      "2021-10-08 02:07:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:07:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005650\n",
      "2021-10-08 02:07:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109540\n",
      "2021-10-08 02:07:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:07:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[27329]\n",
      "2021-10-08 02:07:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013255\n",
      "2021-10-08 02:07:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.923052\n",
      "2021-10-08 02:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.046889\n",
      "2021-10-08 02:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089832\n",
      "2021-10-08 02:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:07:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[27329]\n",
      "2021-10-08 02:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012221\n",
      "2021-10-08 02:07:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:07:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.990142\n",
      "2021-10-08 02:07:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.093279\n",
      "2021-10-08 02:07:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 127:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:07:03 | INFO | fairseq.trainer | begin training epoch 127\n",
      "epoch 127: 100%|9| 1289/1290 [03:21<00:00,  6.62it/s, loss=2.953, nll_loss=1.1492021-10-08 02:10:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001938\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063510\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042786\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109246\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001633\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063362\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041491\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107453\n",
      "2021-10-08 02:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 127 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.47it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.13it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.87it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.54it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.12it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.40it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.46it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.38it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.16it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.25it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.33it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 127 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:10:27 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 4.677 | nll_loss 3.033 | ppl 8.18 | wps 51096.6 | wpb 2691.4 | bsz 201.4 | num_updates 45145 | best_loss 4.595\n",
      "2021-10-08 02:10:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:10:30 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint127.pt (epoch 127 @ 45145 updates, score 4.677) (writing took 3.5493061050074175 seconds)\n",
      "2021-10-08 02:10:30 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)\n",
      "2021-10-08 02:10:30 | INFO | train | epoch 127 | loss 2.934 | nll_loss 1.123 | ppl 2.18 | wps 22802.2 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 45145 | lr 0.000148832 | gnorm 1.435 | loss_scale 16 | train_wall 195 | wall 0\n",
      "2021-10-08 02:10:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=128/shard_epoch=35\n",
      "2021-10-08 02:10:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=128/shard_epoch=36\n",
      "2021-10-08 02:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:10:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004893\n",
      "2021-10-08 02:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117575\n",
      "2021-10-08 02:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[181746]\n",
      "2021-10-08 02:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011175\n",
      "2021-10-08 02:10:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.950805\n",
      "2021-10-08 02:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.080733\n",
      "2021-10-08 02:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:10:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092969\n",
      "2021-10-08 02:10:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[181746]\n",
      "2021-10-08 02:10:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009974\n",
      "2021-10-08 02:10:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.986054\n",
      "2021-10-08 02:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.090063\n",
      "2021-10-08 02:10:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 128:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:10:33 | INFO | fairseq.trainer | begin training epoch 128\n",
      "epoch 128: 100%|9| 1289/1290 [03:23<00:00,  6.35it/s, loss=2.901, nll_loss=1.0912021-10-08 02:13:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001783\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064437\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043840\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111040\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002151\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064063\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044196\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111379\n",
      "2021-10-08 02:13:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 128 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.30it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.97it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.72it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.43it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.03it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.47it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.16it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.16it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.46it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.70it/s]\u001b[A\n",
      "epoch 128 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.24it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:13:58 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 4.67 | nll_loss 3.027 | ppl 8.15 | wps 51710.2 | wpb 2691.4 | bsz 201.4 | num_updates 46435 | best_loss 4.595\n",
      "2021-10-08 02:13:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:14:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint128.pt (epoch 128 @ 46435 updates, score 4.67) (writing took 3.4032740680268034 seconds)\n",
      "2021-10-08 02:14:01 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)\n",
      "2021-10-08 02:14:01 | INFO | train | epoch 128 | loss 2.926 | nll_loss 1.115 | ppl 2.17 | wps 22630 | ups 6.12 | wpb 3695.9 | bsz 249.8 | num_updates 46435 | lr 0.00014675 | gnorm 1.447 | loss_scale 16 | train_wall 196 | wall 0\n",
      "2021-10-08 02:14:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=129/shard_epoch=36\n",
      "2021-10-08 02:14:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=129/shard_epoch=37\n",
      "2021-10-08 02:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:14:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005466\n",
      "2021-10-08 02:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116635\n",
      "2021-10-08 02:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:14:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[12383]\n",
      "2021-10-08 02:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010043\n",
      "2021-10-08 02:14:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.079426\n",
      "2021-10-08 02:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.207111\n",
      "2021-10-08 02:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084386\n",
      "2021-10-08 02:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:14:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[12383]\n",
      "2021-10-08 02:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009017\n",
      "2021-10-08 02:14:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.954796\n",
      "2021-10-08 02:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.049123\n",
      "2021-10-08 02:14:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 129:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:14:03 | INFO | fairseq.trainer | begin training epoch 129\n",
      "epoch 129: 100%|9| 1289/1290 [03:22<00:00,  6.79it/s, loss=3.009, nll_loss=1.2032021-10-08 02:17:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001707\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063573\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046111\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112349\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001740\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061814\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043094\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107596\n",
      "2021-10-08 02:17:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 129 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.58it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.28it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.99it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.66it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.07it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.44it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.52it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.31it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.11it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.41it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.69it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.90it/s]\u001b[A\n",
      "epoch 129 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:17:28 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 4.696 | nll_loss 3.045 | ppl 8.26 | wps 51639.8 | wpb 2691.4 | bsz 201.4 | num_updates 47725 | best_loss 4.595\n",
      "2021-10-08 02:17:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:17:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint129.pt (epoch 129 @ 47725 updates, score 4.696) (writing took 3.588934646046255 seconds)\n",
      "2021-10-08 02:17:31 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)\n",
      "2021-10-08 02:17:31 | INFO | train | epoch 129 | loss 2.921 | nll_loss 1.109 | ppl 2.16 | wps 22685.9 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 47725 | lr 0.000144753 | gnorm 1.453 | loss_scale 16 | train_wall 196 | wall 0\n",
      "2021-10-08 02:17:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=130/shard_epoch=37\n",
      "2021-10-08 02:17:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=130/shard_epoch=38\n",
      "2021-10-08 02:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:17:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005095\n",
      "2021-10-08 02:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112897\n",
      "2021-10-08 02:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[92424]\n",
      "2021-10-08 02:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011383\n",
      "2021-10-08 02:17:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.948663\n",
      "2021-10-08 02:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.074027\n",
      "2021-10-08 02:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091043\n",
      "2021-10-08 02:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[92424]\n",
      "2021-10-08 02:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012531\n",
      "2021-10-08 02:17:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:17:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.944802\n",
      "2021-10-08 02:17:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.049600\n",
      "2021-10-08 02:17:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 130:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:17:33 | INFO | fairseq.trainer | begin training epoch 130\n",
      "epoch 130: 100%|9| 1289/1290 [03:21<00:00,  6.47it/s, loss=2.958, nll_loss=1.1532021-10-08 02:20:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001782\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.074664\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058717\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.136191\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001817\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068891\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042756\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114330\n",
      "2021-10-08 02:20:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 130 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.67it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.16it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.84it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.73it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.39it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.40it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.58it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.76it/s]\u001b[A\n",
      "epoch 130 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.33it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:20:57 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 4.682 | nll_loss 3.033 | ppl 8.19 | wps 51996.5 | wpb 2691.4 | bsz 201.4 | num_updates 49015 | best_loss 4.595\n",
      "2021-10-08 02:20:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:21:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint130.pt (epoch 130 @ 49015 updates, score 4.682) (writing took 3.4565601799986325 seconds)\n",
      "2021-10-08 02:21:00 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)\n",
      "2021-10-08 02:21:00 | INFO | train | epoch 130 | loss 2.913 | nll_loss 1.099 | ppl 2.14 | wps 22809.1 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 49015 | lr 0.000142835 | gnorm 1.451 | loss_scale 16 | train_wall 195 | wall 0\n",
      "2021-10-08 02:21:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=131/shard_epoch=38\n",
      "2021-10-08 02:21:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=131/shard_epoch=39\n",
      "2021-10-08 02:21:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:21:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005008\n",
      "2021-10-08 02:21:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111665\n",
      "2021-10-08 02:21:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:21:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[137591]\n",
      "2021-10-08 02:21:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009504\n",
      "2021-10-08 02:21:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:21:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.921615\n",
      "2021-10-08 02:21:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.043752\n",
      "2021-10-08 02:21:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:21:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:21:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.094274\n",
      "2021-10-08 02:21:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:21:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[137591]\n",
      "2021-10-08 02:21:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010177\n",
      "2021-10-08 02:21:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:21:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.935985\n",
      "2021-10-08 02:21:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.041442\n",
      "2021-10-08 02:21:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 131:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:21:02 | INFO | fairseq.trainer | begin training epoch 131\n",
      "epoch 131: 100%|9| 1289/1290 [03:21<00:00,  6.43it/s, loss=2.968, nll_loss=1.1642021-10-08 02:24:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002215\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065274\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044931\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.113424\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001996\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066618\n",
      "2021-10-08 02:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049037\n",
      "2021-10-08 02:24:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118782\n",
      "2021-10-08 02:24:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 131 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.56it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.13it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  8.73it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.46it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.15it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.68it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.85it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.81it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.73it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.04it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.37it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.67it/s]\u001b[A\n",
      "epoch 131 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.23it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:24:26 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 4.673 | nll_loss 3.028 | ppl 8.16 | wps 51651.5 | wpb 2691.4 | bsz 201.4 | num_updates 50305 | best_loss 4.595\n",
      "2021-10-08 02:24:26 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:24:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint131.pt (epoch 131 @ 50305 updates, score 4.673) (writing took 3.4053035419783555 seconds)\n",
      "2021-10-08 02:24:30 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)\n",
      "2021-10-08 02:24:30 | INFO | train | epoch 131 | loss 2.906 | nll_loss 1.092 | ppl 2.13 | wps 22774.9 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 50305 | lr 0.000140992 | gnorm 1.463 | loss_scale 16 | train_wall 195 | wall 0\n",
      "2021-10-08 02:24:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=132/shard_epoch=39\n",
      "2021-10-08 02:24:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=132/shard_epoch=40\n",
      "2021-10-08 02:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:24:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004869\n",
      "2021-10-08 02:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114217\n",
      "2021-10-08 02:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[224729]\n",
      "2021-10-08 02:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011563\n",
      "2021-10-08 02:24:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.077355\n",
      "2021-10-08 02:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.204191\n",
      "2021-10-08 02:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089061\n",
      "2021-10-08 02:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[224729]\n",
      "2021-10-08 02:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008639\n",
      "2021-10-08 02:24:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.952273\n",
      "2021-10-08 02:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.051017\n",
      "2021-10-08 02:24:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 132:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:24:32 | INFO | fairseq.trainer | begin training epoch 132\n",
      "epoch 132: 100%|9| 1289/1290 [03:21<00:00,  6.36it/s, loss=2.928, nll_loss=1.12,2021-10-08 02:27:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002619\n",
      "2021-10-08 02:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065070\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043145\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111831\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001946\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062997\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041767\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107650\n",
      "2021-10-08 02:27:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 132 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.23it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.89it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.61it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.32it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 10.08it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 11.70it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 13.22it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 14.55it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 15.70it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.17it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 17.66it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.01it/s]\u001b[A\n",
      "epoch 132 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.62it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:27:55 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 4.694 | nll_loss 3.053 | ppl 8.3 | wps 46774.5 | wpb 2691.4 | bsz 201.4 | num_updates 51595 | best_loss 4.595\n",
      "2021-10-08 02:27:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:27:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint132.pt (epoch 132 @ 51595 updates, score 4.694) (writing took 3.422267968999222 seconds)\n",
      "2021-10-08 02:27:59 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)\n",
      "2021-10-08 02:27:59 | INFO | train | epoch 132 | loss 2.901 | nll_loss 1.087 | ppl 2.12 | wps 22773.7 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 51595 | lr 0.000139218 | gnorm 1.468 | loss_scale 16 | train_wall 195 | wall 0\n",
      "2021-10-08 02:27:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=133/shard_epoch=40\n",
      "2021-10-08 02:27:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=133/shard_epoch=41\n",
      "2021-10-08 02:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:27:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007413\n",
      "2021-10-08 02:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112688\n",
      "2021-10-08 02:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:27:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[129983]\n",
      "2021-10-08 02:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010818\n",
      "2021-10-08 02:27:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.938855\n",
      "2021-10-08 02:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.063351\n",
      "2021-10-08 02:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086618\n",
      "2021-10-08 02:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:28:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[129983]\n",
      "2021-10-08 02:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010480\n",
      "2021-10-08 02:28:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.916478\n",
      "2021-10-08 02:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.014568\n",
      "2021-10-08 02:28:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 133:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:28:01 | INFO | fairseq.trainer | begin training epoch 133\n",
      "epoch 133: 100%|9| 1289/1290 [03:21<00:00,  6.32it/s, loss=2.937, nll_loss=1.1272021-10-08 02:31:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001759\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065921\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043029\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111743\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001644\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064034\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041686\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108185\n",
      "2021-10-08 02:31:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 133 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.98it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.73it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.47it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.15it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.66it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.96it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.39it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.68it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.83it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.97it/s]\u001b[A\n",
      "epoch 133 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.30it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:31:25 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 4.688 | nll_loss 3.043 | ppl 8.24 | wps 51730 | wpb 2691.4 | bsz 201.4 | num_updates 52885 | best_loss 4.595\n",
      "2021-10-08 02:31:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:31:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint133.pt (epoch 133 @ 52885 updates, score 4.688) (writing took 3.4958815749851055 seconds)\n",
      "2021-10-08 02:31:28 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)\n",
      "2021-10-08 02:31:28 | INFO | train | epoch 133 | loss 2.894 | nll_loss 1.078 | ppl 2.11 | wps 22759.3 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 52885 | lr 0.00013751 | gnorm 1.474 | loss_scale 16 | train_wall 195 | wall 0\n",
      "2021-10-08 02:31:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=134/shard_epoch=41\n",
      "2021-10-08 02:31:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=134/shard_epoch=42\n",
      "2021-10-08 02:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:31:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004720\n",
      "2021-10-08 02:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113274\n",
      "2021-10-08 02:31:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[312629]\n",
      "2021-10-08 02:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010835\n",
      "2021-10-08 02:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.907926\n",
      "2021-10-08 02:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.033030\n",
      "2021-10-08 02:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:31:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089014\n",
      "2021-10-08 02:31:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[312629]\n",
      "2021-10-08 02:31:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011441\n",
      "2021-10-08 02:31:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.945722\n",
      "2021-10-08 02:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.047218\n",
      "2021-10-08 02:31:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 134:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:31:31 | INFO | fairseq.trainer | begin training epoch 134\n",
      "epoch 134: 100%|9| 1289/1290 [03:22<00:00,  6.64it/s, loss=2.88, nll_loss=1.071,2021-10-08 02:34:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002135\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064518\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045266\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112931\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001685\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063813\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042581\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109020\n",
      "2021-10-08 02:34:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 134 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.53it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.22it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.89it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.59it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.08it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.44it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.54it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.45it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.23it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.33it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.42it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.49it/s]\u001b[A\n",
      "epoch 134 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.02it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:34:55 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 4.693 | nll_loss 3.049 | ppl 8.28 | wps 51190.5 | wpb 2691.4 | bsz 201.4 | num_updates 54175 | best_loss 4.595\n",
      "2021-10-08 02:34:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:34:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint134.pt (epoch 134 @ 54175 updates, score 4.693) (writing took 3.458997255016584 seconds)\n",
      "2021-10-08 02:34:58 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)\n",
      "2021-10-08 02:34:58 | INFO | train | epoch 134 | loss 2.887 | nll_loss 1.071 | ppl 2.1 | wps 22711.7 | ups 6.15 | wpb 3695.9 | bsz 249.8 | num_updates 54175 | lr 0.000135863 | gnorm 1.473 | loss_scale 32 | train_wall 196 | wall 0\n",
      "2021-10-08 02:34:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=135/shard_epoch=42\n",
      "2021-10-08 02:34:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=135/shard_epoch=43\n",
      "2021-10-08 02:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:34:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004703\n",
      "2021-10-08 02:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106248\n",
      "2021-10-08 02:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:34:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[312488]\n",
      "2021-10-08 02:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009002\n",
      "2021-10-08 02:34:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.946855\n",
      "2021-10-08 02:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.063124\n",
      "2021-10-08 02:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089570\n",
      "2021-10-08 02:34:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:35:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[312488]\n",
      "2021-10-08 02:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009112\n",
      "2021-10-08 02:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.931294\n",
      "2021-10-08 02:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.030980\n",
      "2021-10-08 02:35:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 135:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:35:00 | INFO | fairseq.trainer | begin training epoch 135\n",
      "epoch 135:  53%|5| 681/1290 [01:47<01:33,  6.49it/s, loss=2.898, nll_loss=1.082,2021-10-08 02:36:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 135: 100%|9| 1289/1290 [03:22<00:00,  6.55it/s, loss=2.898, nll_loss=1.0842021-10-08 02:38:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001714\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066661\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043047\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112326\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001625\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065641\n",
      "2021-10-08 02:38:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041424\n",
      "2021-10-08 02:38:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109669\n",
      "2021-10-08 02:38:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 135 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.88it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.63it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.39it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.02it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.54it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.84it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.85it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.70it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.39it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.43it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.86it/s]\u001b[A\n",
      "epoch 135 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:38:25 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 4.7 | nll_loss 3.06 | ppl 8.34 | wps 51932.8 | wpb 2691.4 | bsz 201.4 | num_updates 55464 | best_loss 4.595\n",
      "2021-10-08 02:38:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:38:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint135.pt (epoch 135 @ 55464 updates, score 4.7) (writing took 3.570256556966342 seconds)\n",
      "2021-10-08 02:38:29 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)\n",
      "2021-10-08 02:38:29 | INFO | train | epoch 135 | loss 2.881 | nll_loss 1.064 | ppl 2.09 | wps 22647.3 | ups 6.13 | wpb 3696.4 | bsz 249.8 | num_updates 55464 | lr 0.000134275 | gnorm 1.478 | loss_scale 16 | train_wall 196 | wall 0\n",
      "2021-10-08 02:38:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=136/shard_epoch=43\n",
      "2021-10-08 02:38:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=136/shard_epoch=44\n",
      "2021-10-08 02:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:38:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004617\n",
      "2021-10-08 02:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114935\n",
      "2021-10-08 02:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[208580]\n",
      "2021-10-08 02:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009581\n",
      "2021-10-08 02:38:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.974676\n",
      "2021-10-08 02:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.100193\n",
      "2021-10-08 02:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085656\n",
      "2021-10-08 02:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[208580]\n",
      "2021-10-08 02:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008833\n",
      "2021-10-08 02:38:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:38:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.922659\n",
      "2021-10-08 02:38:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.018140\n",
      "2021-10-08 02:38:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 136:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:38:31 | INFO | fairseq.trainer | begin training epoch 136\n",
      "epoch 136: 100%|9| 1289/1290 [03:22<00:00,  6.26it/s, loss=2.934, nll_loss=1.1242021-10-08 02:41:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001814\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066673\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043204\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112715\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001974\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061481\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040876\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105220\n",
      "2021-10-08 02:41:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 136 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.26it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.92it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.64it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.34it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.91it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.27it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.40it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.14it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.27it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.54it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.63it/s]\u001b[A\n",
      "epoch 136 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:41:55 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 4.704 | nll_loss 3.059 | ppl 8.33 | wps 51753.1 | wpb 2691.4 | bsz 201.4 | num_updates 56754 | best_loss 4.595\n",
      "2021-10-08 02:41:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:41:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint136.pt (epoch 136 @ 56754 updates, score 4.704) (writing took 3.472715685958974 seconds)\n",
      "2021-10-08 02:41:59 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)\n",
      "2021-10-08 02:41:59 | INFO | train | epoch 136 | loss 2.875 | nll_loss 1.058 | ppl 2.08 | wps 22715.5 | ups 6.15 | wpb 3695.9 | bsz 249.8 | num_updates 56754 | lr 0.00013274 | gnorm 1.482 | loss_scale 16 | train_wall 196 | wall 0\n",
      "2021-10-08 02:41:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=137/shard_epoch=44\n",
      "2021-10-08 02:41:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=137/shard_epoch=45\n",
      "2021-10-08 02:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:41:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004641\n",
      "2021-10-08 02:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113824\n",
      "2021-10-08 02:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:41:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[46713]\n",
      "2021-10-08 02:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011279\n",
      "2021-10-08 02:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:42:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.950169\n",
      "2021-10-08 02:42:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.076320\n",
      "2021-10-08 02:42:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:42:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:42:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086964\n",
      "2021-10-08 02:42:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:42:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[46713]\n",
      "2021-10-08 02:42:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009839\n",
      "2021-10-08 02:42:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.940152\n",
      "2021-10-08 02:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.037906\n",
      "2021-10-08 02:42:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 137:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:42:01 | INFO | fairseq.trainer | begin training epoch 137\n",
      "epoch 137: 100%|9| 1289/1290 [03:22<00:00,  6.23it/s, loss=2.897, nll_loss=1.0862021-10-08 02:45:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001836\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065876\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043562\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112275\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001906\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063796\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041526\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108160\n",
      "2021-10-08 02:45:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 137 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.51it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.21it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.96it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.66it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.25it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.64it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.71it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.54it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.32it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.47it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.90it/s]\u001b[A\n",
      "epoch 137 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:45:25 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 4.707 | nll_loss 3.069 | ppl 8.39 | wps 52085.8 | wpb 2691.4 | bsz 201.4 | num_updates 58044 | best_loss 4.595\n",
      "2021-10-08 02:45:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:45:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint137.pt (epoch 137 @ 58044 updates, score 4.707) (writing took 3.465532192029059 seconds)\n",
      "2021-10-08 02:45:29 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)\n",
      "2021-10-08 02:45:29 | INFO | train | epoch 137 | loss 2.871 | nll_loss 1.053 | ppl 2.07 | wps 22677.3 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 58044 | lr 0.000131257 | gnorm 1.486 | loss_scale 16 | train_wall 196 | wall 0\n",
      "2021-10-08 02:45:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=138/shard_epoch=45\n",
      "2021-10-08 02:45:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=138/shard_epoch=46\n",
      "2021-10-08 02:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:45:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004749\n",
      "2021-10-08 02:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.104178\n",
      "2021-10-08 02:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[35483]\n",
      "2021-10-08 02:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009971\n",
      "2021-10-08 02:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.922316\n",
      "2021-10-08 02:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.037583\n",
      "2021-10-08 02:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089323\n",
      "2021-10-08 02:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[35483]\n",
      "2021-10-08 02:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009895\n",
      "2021-10-08 02:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.925598\n",
      "2021-10-08 02:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.025777\n",
      "2021-10-08 02:45:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 138:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:45:31 | INFO | fairseq.trainer | begin training epoch 138\n",
      "epoch 138: 100%|9| 1289/1290 [03:22<00:00,  6.48it/s, loss=2.856, nll_loss=1.0412021-10-08 02:48:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001783\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065846\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042479\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111151\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001975\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064336\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041525\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108877\n",
      "2021-10-08 02:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 138 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.34it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.02it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.78it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.51it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.09it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.48it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.59it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.47it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.27it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.44it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.86it/s]\u001b[A\n",
      "epoch 138 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:48:55 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 4.709 | nll_loss 3.067 | ppl 8.38 | wps 52013.4 | wpb 2691.4 | bsz 201.4 | num_updates 59334 | best_loss 4.595\n",
      "2021-10-08 02:48:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:48:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint138.pt (epoch 138 @ 59334 updates, score 4.709) (writing took 3.565319863962941 seconds)\n",
      "2021-10-08 02:48:59 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)\n",
      "2021-10-08 02:48:59 | INFO | train | epoch 138 | loss 2.864 | nll_loss 1.045 | ppl 2.06 | wps 22671.3 | ups 6.13 | wpb 3695.9 | bsz 249.8 | num_updates 59334 | lr 0.000129822 | gnorm 1.491 | loss_scale 16 | train_wall 196 | wall 0\n",
      "2021-10-08 02:48:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=139/shard_epoch=46\n",
      "2021-10-08 02:48:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=139/shard_epoch=47\n",
      "2021-10-08 02:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:48:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005111\n",
      "2021-10-08 02:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110674\n",
      "2021-10-08 02:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:48:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[70516]\n",
      "2021-10-08 02:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009557\n",
      "2021-10-08 02:48:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:49:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.945147\n",
      "2021-10-08 02:49:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.066368\n",
      "2021-10-08 02:49:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:49:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:49:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086437\n",
      "2021-10-08 02:49:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:49:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[70516]\n",
      "2021-10-08 02:49:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010169\n",
      "2021-10-08 02:49:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.932312\n",
      "2021-10-08 02:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.029915\n",
      "2021-10-08 02:49:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 139:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:49:01 | INFO | fairseq.trainer | begin training epoch 139\n",
      "epoch 139: 100%|9| 1289/1290 [03:23<00:00,  6.51it/s, loss=2.961, nll_loss=1.1512021-10-08 02:52:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002024\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065709\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041132\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109774\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001615\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063575\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041240\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107188\n",
      "2021-10-08 02:52:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 139 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.44it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.12it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.81it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.53it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.13it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.49it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.59it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.27it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.43it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.79it/s]\u001b[A\n",
      "epoch 139 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:52:27 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 4.703 | nll_loss 3.062 | ppl 8.35 | wps 51568.2 | wpb 2691.4 | bsz 201.4 | num_updates 60624 | best_loss 4.595\n",
      "2021-10-08 02:52:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:52:30 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint139.pt (epoch 139 @ 60624 updates, score 4.703) (writing took 3.515333080023993 seconds)\n",
      "2021-10-08 02:52:30 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)\n",
      "2021-10-08 02:52:30 | INFO | train | epoch 139 | loss 2.861 | nll_loss 1.042 | ppl 2.06 | wps 22554.8 | ups 6.1 | wpb 3695.9 | bsz 249.8 | num_updates 60624 | lr 0.000128433 | gnorm 1.503 | loss_scale 16 | train_wall 197 | wall 0\n",
      "2021-10-08 02:52:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=140/shard_epoch=47\n",
      "2021-10-08 02:52:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=140/shard_epoch=48\n",
      "2021-10-08 02:52:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:52:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005080\n",
      "2021-10-08 02:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106255\n",
      "2021-10-08 02:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[92906]\n",
      "2021-10-08 02:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009409\n",
      "2021-10-08 02:52:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.981383\n",
      "2021-10-08 02:52:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.098037\n",
      "2021-10-08 02:52:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:52:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086030\n",
      "2021-10-08 02:52:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[92906]\n",
      "2021-10-08 02:52:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010346\n",
      "2021-10-08 02:52:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.969390\n",
      "2021-10-08 02:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.066808\n",
      "2021-10-08 02:52:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 140:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:52:33 | INFO | fairseq.trainer | begin training epoch 140\n",
      "epoch 140: 100%|9| 1289/1290 [03:23<00:00,  6.64it/s, loss=2.907, nll_loss=1.0962021-10-08 02:55:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002028\n",
      "2021-10-08 02:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070247\n",
      "2021-10-08 02:55:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045429\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.118826\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002248\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065581\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041772\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110608\n",
      "2021-10-08 02:55:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 140 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.50it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.05it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  8.75it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 10.10it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 11.79it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  50%|###   | 14/28 [00:01<00:01, 13.29it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 14.62it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 15.83it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.25it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 17.82it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.26it/s]\u001b[A\n",
      "epoch 140 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:55:58 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 4.697 | nll_loss 3.06 | ppl 8.34 | wps 46687.7 | wpb 2691.4 | bsz 201.4 | num_updates 61914 | best_loss 4.595\n",
      "2021-10-08 02:55:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:56:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint140.pt (epoch 140 @ 61914 updates, score 4.697) (writing took 3.5188485800172202 seconds)\n",
      "2021-10-08 02:56:02 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)\n",
      "2021-10-08 02:56:02 | INFO | train | epoch 140 | loss 2.854 | nll_loss 1.035 | ppl 2.05 | wps 22544.2 | ups 6.1 | wpb 3695.9 | bsz 249.8 | num_updates 61914 | lr 0.000127088 | gnorm 1.505 | loss_scale 16 | train_wall 197 | wall 0\n",
      "2021-10-08 02:56:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=141/shard_epoch=48\n",
      "2021-10-08 02:56:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=141/shard_epoch=49\n",
      "2021-10-08 02:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:56:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004833\n",
      "2021-10-08 02:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117455\n",
      "2021-10-08 02:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:56:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[142952]\n",
      "2021-10-08 02:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010416\n",
      "2021-10-08 02:56:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.952417\n",
      "2021-10-08 02:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.081252\n",
      "2021-10-08 02:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090164\n",
      "2021-10-08 02:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:56:03 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[142952]\n",
      "2021-10-08 02:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012526\n",
      "2021-10-08 02:56:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:56:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.056166\n",
      "2021-10-08 02:56:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.159927\n",
      "2021-10-08 02:56:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 141:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:56:04 | INFO | fairseq.trainer | begin training epoch 141\n",
      "epoch 141: 100%|9| 1289/1290 [03:21<00:00,  6.58it/s, loss=2.919, nll_loss=1.1082021-10-08 02:59:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001632\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065910\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043678\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112202\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002096\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066965\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041917\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112203\n",
      "2021-10-08 02:59:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 141 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.22it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.87it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.61it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.34it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.99it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.39it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.52it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.16it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.35it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.57it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.77it/s]\u001b[A\n",
      "epoch 141 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 02:59:27 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 4.716 | nll_loss 3.078 | ppl 8.44 | wps 52022.3 | wpb 2691.4 | bsz 201.4 | num_updates 63204 | best_loss 4.595\n",
      "2021-10-08 02:59:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 02:59:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint141.pt (epoch 141 @ 63204 updates, score 4.716) (writing took 3.3717343889875337 seconds)\n",
      "2021-10-08 02:59:31 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)\n",
      "2021-10-08 02:59:31 | INFO | train | epoch 141 | loss 2.851 | nll_loss 1.031 | ppl 2.04 | wps 22827.6 | ups 6.18 | wpb 3695.9 | bsz 249.8 | num_updates 63204 | lr 0.000125785 | gnorm 1.507 | loss_scale 16 | train_wall 195 | wall 0\n",
      "2021-10-08 02:59:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=142/shard_epoch=49\n",
      "2021-10-08 02:59:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=142/shard_epoch=50\n",
      "2021-10-08 02:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:59:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004677\n",
      "2021-10-08 02:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114456\n",
      "2021-10-08 02:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[208610]\n",
      "2021-10-08 02:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010778\n",
      "2021-10-08 02:59:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.913197\n",
      "2021-10-08 02:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.039503\n",
      "2021-10-08 02:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 02:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088639\n",
      "2021-10-08 02:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[208610]\n",
      "2021-10-08 02:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010192\n",
      "2021-10-08 02:59:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 02:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.913645\n",
      "2021-10-08 02:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.013438\n",
      "2021-10-08 02:59:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 142:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 02:59:33 | INFO | fairseq.trainer | begin training epoch 142\n",
      "epoch 142: 100%|9| 1289/1290 [03:23<00:00,  6.40it/s, loss=2.862, nll_loss=1.0492021-10-08 03:02:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002011\n",
      "2021-10-08 03:02:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065141\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041901\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110050\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001730\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066026\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041027\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109803\n",
      "2021-10-08 03:02:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 142 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.48it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.02it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  8.70it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.41it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.06it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.57it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.67it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.67it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.64it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.89it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.21it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.48it/s]\u001b[A\n",
      "epoch 142 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:02:58 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 4.692 | nll_loss 3.052 | ppl 8.3 | wps 51342.2 | wpb 2691.4 | bsz 201.4 | num_updates 64494 | best_loss 4.595\n",
      "2021-10-08 03:02:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:03:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint142.pt (epoch 142 @ 64494 updates, score 4.692) (writing took 3.3982566869817674 seconds)\n",
      "2021-10-08 03:03:02 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)\n",
      "2021-10-08 03:03:02 | INFO | train | epoch 142 | loss 2.845 | nll_loss 1.024 | ppl 2.03 | wps 22606.3 | ups 6.12 | wpb 3695.9 | bsz 249.8 | num_updates 64494 | lr 0.00012452 | gnorm 1.512 | loss_scale 16 | train_wall 197 | wall 0\n",
      "2021-10-08 03:03:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=143/shard_epoch=50\n",
      "2021-10-08 03:03:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=143/shard_epoch=51\n",
      "2021-10-08 03:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:03:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004661\n",
      "2021-10-08 03:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110433\n",
      "2021-10-08 03:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:03:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[106128]\n",
      "2021-10-08 03:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010894\n",
      "2021-10-08 03:03:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.910482\n",
      "2021-10-08 03:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.032848\n",
      "2021-10-08 03:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087921\n",
      "2021-10-08 03:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:03:03 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[106128]\n",
      "2021-10-08 03:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010902\n",
      "2021-10-08 03:03:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.945428\n",
      "2021-10-08 03:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.045272\n",
      "2021-10-08 03:03:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 143:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:03:04 | INFO | fairseq.trainer | begin training epoch 143\n",
      "epoch 143: 100%|9| 1289/1290 [03:23<00:00,  6.41it/s, loss=2.799, nll_loss=0.98,2021-10-08 03:06:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001791\n",
      "2021-10-08 03:06:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065087\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043237\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111079\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001745\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063048\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042110\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107864\n",
      "2021-10-08 03:06:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 143 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.67it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.10it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.77it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.33it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.69it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.74it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.64it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.45it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.60it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.78it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "epoch 143 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.14it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:06:29 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 4.706 | nll_loss 3.066 | ppl 8.38 | wps 51600.6 | wpb 2691.4 | bsz 201.4 | num_updates 65784 | best_loss 4.595\n",
      "2021-10-08 03:06:29 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:06:33 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint143.pt (epoch 143 @ 65784 updates, score 4.706) (writing took 3.453324048954528 seconds)\n",
      "2021-10-08 03:06:33 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)\n",
      "2021-10-08 03:06:33 | INFO | train | epoch 143 | loss 2.84 | nll_loss 1.019 | ppl 2.03 | wps 22595.4 | ups 6.11 | wpb 3695.9 | bsz 249.8 | num_updates 65784 | lr 0.000123293 | gnorm 1.518 | loss_scale 16 | train_wall 197 | wall 0\n",
      "2021-10-08 03:06:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=144/shard_epoch=51\n",
      "2021-10-08 03:06:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=144/shard_epoch=52\n",
      "2021-10-08 03:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:06:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004605\n",
      "2021-10-08 03:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105297\n",
      "2021-10-08 03:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:33 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[216835]\n",
      "2021-10-08 03:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009502\n",
      "2021-10-08 03:06:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.936719\n",
      "2021-10-08 03:06:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.052476\n",
      "2021-10-08 03:06:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:06:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085372\n",
      "2021-10-08 03:06:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:34 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[216835]\n",
      "2021-10-08 03:06:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009975\n",
      "2021-10-08 03:06:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.004224\n",
      "2021-10-08 03:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.100627\n",
      "2021-10-08 03:06:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 144:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:06:35 | INFO | fairseq.trainer | begin training epoch 144\n",
      "epoch 144: 100%|9| 1289/1290 [03:21<00:00,  6.45it/s, loss=2.849, nll_loss=1.0312021-10-08 03:09:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001773\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068809\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041772\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.113325\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001812\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063796\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041073\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107593\n",
      "2021-10-08 03:09:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 144 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.99it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.61it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.33it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.01it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.60it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.09it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.29it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.11it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.31it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.57it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.77it/s]\u001b[A\n",
      "epoch 144 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:09:59 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 4.71 | nll_loss 3.073 | ppl 8.41 | wps 51537.3 | wpb 2691.4 | bsz 201.4 | num_updates 67074 | best_loss 4.595\n",
      "2021-10-08 03:09:59 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:10:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint144.pt (epoch 144 @ 67074 updates, score 4.71) (writing took 3.463147234986536 seconds)\n",
      "2021-10-08 03:10:02 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)\n",
      "2021-10-08 03:10:02 | INFO | train | epoch 144 | loss 2.836 | nll_loss 1.015 | ppl 2.02 | wps 22763.6 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 67074 | lr 0.000122102 | gnorm 1.527 | loss_scale 16 | train_wall 195 | wall 0\n",
      "2021-10-08 03:10:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=145/shard_epoch=52\n",
      "2021-10-08 03:10:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=145/shard_epoch=53\n",
      "2021-10-08 03:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:10:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005141\n",
      "2021-10-08 03:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111759\n",
      "2021-10-08 03:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:10:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[146283]\n",
      "2021-10-08 03:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010229\n",
      "2021-10-08 03:10:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.954668\n",
      "2021-10-08 03:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.077623\n",
      "2021-10-08 03:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087282\n",
      "2021-10-08 03:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:10:03 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[146283]\n",
      "2021-10-08 03:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008911\n",
      "2021-10-08 03:10:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.931583\n",
      "2021-10-08 03:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.028978\n",
      "2021-10-08 03:10:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 145:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:10:04 | INFO | fairseq.trainer | begin training epoch 145\n",
      "epoch 145:  22%|2| 279/1290 [00:43<02:38,  6.37it/s, loss=2.81, nll_loss=0.985, 2021-10-08 03:10:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n",
      "epoch 145: 100%|9| 1289/1290 [03:21<00:00,  6.36it/s, loss=2.914, nll_loss=1.1042021-10-08 03:13:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001861\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063309\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048058\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114332\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001794\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063467\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045052\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111213\n",
      "2021-10-08 03:13:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 145 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.19it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.84it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.57it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.25it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.84it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.24it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.40it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.31it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.11it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.26it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.74it/s]\u001b[A\n",
      "epoch 145 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.23it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:13:28 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 4.697 | nll_loss 3.058 | ppl 8.33 | wps 51532.9 | wpb 2691.4 | bsz 201.4 | num_updates 68363 | best_loss 4.595\n",
      "2021-10-08 03:13:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:13:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint145.pt (epoch 145 @ 68363 updates, score 4.697) (writing took 3.516332438972313 seconds)\n",
      "2021-10-08 03:13:31 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)\n",
      "2021-10-08 03:13:31 | INFO | train | epoch 145 | loss 2.832 | nll_loss 1.009 | ppl 2.01 | wps 22806.1 | ups 6.17 | wpb 3696.2 | bsz 249.8 | num_updates 68363 | lr 0.000120945 | gnorm 1.523 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 03:13:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=146/shard_epoch=53\n",
      "2021-10-08 03:13:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=146/shard_epoch=54\n",
      "2021-10-08 03:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:13:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005013\n",
      "2021-10-08 03:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111067\n",
      "2021-10-08 03:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[151]\n",
      "2021-10-08 03:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011370\n",
      "2021-10-08 03:13:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.974641\n",
      "2021-10-08 03:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.098135\n",
      "2021-10-08 03:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086934\n",
      "2021-10-08 03:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[151]\n",
      "2021-10-08 03:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011109\n",
      "2021-10-08 03:13:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:13:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.020462\n",
      "2021-10-08 03:13:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.119439\n",
      "2021-10-08 03:13:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 146:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:13:33 | INFO | fairseq.trainer | begin training epoch 146\n",
      "epoch 146: 100%|9| 1289/1290 [03:22<00:00,  6.46it/s, loss=2.863, nll_loss=1.0492021-10-08 03:16:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001939\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068428\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045646\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116965\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002394\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066862\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042165\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112490\n",
      "2021-10-08 03:16:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 146 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.06it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.71it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.45it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.17it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.76it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.18it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.29it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.14it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.37it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.75it/s]\u001b[A\n",
      "epoch 146 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:16:58 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 4.72 | nll_loss 3.08 | ppl 8.46 | wps 51900.3 | wpb 2691.4 | bsz 201.4 | num_updates 69653 | best_loss 4.595\n",
      "2021-10-08 03:16:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:17:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint146.pt (epoch 146 @ 69653 updates, score 4.72) (writing took 3.470368907030206 seconds)\n",
      "2021-10-08 03:17:01 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)\n",
      "2021-10-08 03:17:01 | INFO | train | epoch 146 | loss 2.827 | nll_loss 1.004 | ppl 2.01 | wps 22682.4 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 69653 | lr 0.00011982 | gnorm 1.535 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 03:17:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=147/shard_epoch=54\n",
      "2021-10-08 03:17:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=147/shard_epoch=55\n",
      "2021-10-08 03:17:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:17:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004992\n",
      "2021-10-08 03:17:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117848\n",
      "2021-10-08 03:17:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:17:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[299014]\n",
      "2021-10-08 03:17:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010174\n",
      "2021-10-08 03:17:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:17:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.973464\n",
      "2021-10-08 03:17:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.102501\n",
      "2021-10-08 03:17:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:17:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:17:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085107\n",
      "2021-10-08 03:17:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:17:03 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[299014]\n",
      "2021-10-08 03:17:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008649\n",
      "2021-10-08 03:17:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:17:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.983438\n",
      "2021-10-08 03:17:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.078160\n",
      "2021-10-08 03:17:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 147:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:17:04 | INFO | fairseq.trainer | begin training epoch 147\n",
      "epoch 147: 100%|9| 1289/1290 [03:21<00:00,  6.45it/s, loss=2.808, nll_loss=0.9892021-10-08 03:20:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001877\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067175\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045766\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115751\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001828\n",
      "2021-10-08 03:20:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062913\n",
      "2021-10-08 03:20:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043688\n",
      "2021-10-08 03:20:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109250\n",
      "2021-10-08 03:20:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 147 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.43it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.12it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.86it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.57it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.17it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.52it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.63it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.50it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.26it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.52it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.68it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 147 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:20:27 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 4.717 | nll_loss 3.075 | ppl 8.43 | wps 51665.2 | wpb 2691.4 | bsz 201.4 | num_updates 70943 | best_loss 4.595\n",
      "2021-10-08 03:20:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:20:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint147.pt (epoch 147 @ 70943 updates, score 4.717) (writing took 3.5338234339724295 seconds)\n",
      "2021-10-08 03:20:31 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)\n",
      "2021-10-08 03:20:31 | INFO | train | epoch 147 | loss 2.822 | nll_loss 0.998 | ppl 2 | wps 22763.4 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 70943 | lr 0.000118726 | gnorm 1.538 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 03:20:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=148/shard_epoch=55\n",
      "2021-10-08 03:20:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=148/shard_epoch=56\n",
      "2021-10-08 03:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:20:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004740\n",
      "2021-10-08 03:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110886\n",
      "2021-10-08 03:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[248376]\n",
      "2021-10-08 03:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009945\n",
      "2021-10-08 03:20:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.948467\n",
      "2021-10-08 03:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.070328\n",
      "2021-10-08 03:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089604\n",
      "2021-10-08 03:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[248376]\n",
      "2021-10-08 03:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010265\n",
      "2021-10-08 03:20:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.930320\n",
      "2021-10-08 03:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.031169\n",
      "2021-10-08 03:20:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 148:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:20:33 | INFO | fairseq.trainer | begin training epoch 148\n",
      "epoch 148: 100%|9| 1289/1290 [03:22<00:00,  6.36it/s, loss=2.835, nll_loss=1.0152021-10-08 03:23:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002135\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063139\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043856\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110129\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001867\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063158\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044888\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110794\n",
      "2021-10-08 03:23:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 148 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.37it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.02it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.75it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.47it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.08it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.46it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.52it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.18it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.43it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.68it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 148 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:23:57 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 4.722 | nll_loss 3.081 | ppl 8.46 | wps 51634.2 | wpb 2691.4 | bsz 201.4 | num_updates 72233 | best_loss 4.595\n",
      "2021-10-08 03:23:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:24:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint148.pt (epoch 148 @ 72233 updates, score 4.722) (writing took 3.580819783033803 seconds)\n",
      "2021-10-08 03:24:01 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)\n",
      "2021-10-08 03:24:01 | INFO | train | epoch 148 | loss 2.819 | nll_loss 0.995 | ppl 1.99 | wps 22674 | ups 6.13 | wpb 3695.9 | bsz 249.8 | num_updates 72233 | lr 0.000117661 | gnorm 1.558 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 03:24:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=149/shard_epoch=56\n",
      "2021-10-08 03:24:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=149/shard_epoch=57\n",
      "2021-10-08 03:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:24:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005102\n",
      "2021-10-08 03:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114244\n",
      "2021-10-08 03:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:24:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[235824]\n",
      "2021-10-08 03:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010372\n",
      "2021-10-08 03:24:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.951935\n",
      "2021-10-08 03:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.077497\n",
      "2021-10-08 03:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092041\n",
      "2021-10-08 03:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:24:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[235824]\n",
      "2021-10-08 03:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012260\n",
      "2021-10-08 03:24:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.960028\n",
      "2021-10-08 03:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.065309\n",
      "2021-10-08 03:24:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 149:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:24:03 | INFO | fairseq.trainer | begin training epoch 149\n",
      "epoch 149: 100%|9| 1289/1290 [03:22<00:00,  6.32it/s, loss=2.748, nll_loss=0.9272021-10-08 03:27:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001605\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062856\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042536\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107868\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001532\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063493\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041519\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107374\n",
      "2021-10-08 03:27:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 149 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.55it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.23it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.98it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.71it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.31it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.70it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.68it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.50it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.75it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.82it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "epoch 149 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.32it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:27:28 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 4.731 | nll_loss 3.094 | ppl 8.54 | wps 52121.6 | wpb 2691.4 | bsz 201.4 | num_updates 73523 | best_loss 4.595\n",
      "2021-10-08 03:27:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:27:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint149.pt (epoch 149 @ 73523 updates, score 4.731) (writing took 3.4671780730132014 seconds)\n",
      "2021-10-08 03:27:31 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)\n",
      "2021-10-08 03:27:31 | INFO | train | epoch 149 | loss 2.813 | nll_loss 0.989 | ppl 1.98 | wps 22694 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 73523 | lr 0.000116624 | gnorm 1.544 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 03:27:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=150/shard_epoch=57\n",
      "2021-10-08 03:27:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=150/shard_epoch=58\n",
      "2021-10-08 03:27:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:27:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004661\n",
      "2021-10-08 03:27:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110273\n",
      "2021-10-08 03:27:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[261544]\n",
      "2021-10-08 03:27:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015891\n",
      "2021-10-08 03:27:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902023\n",
      "2021-10-08 03:27:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.029314\n",
      "2021-10-08 03:27:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:27:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084942\n",
      "2021-10-08 03:27:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[261544]\n",
      "2021-10-08 03:27:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009973\n",
      "2021-10-08 03:27:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.880952\n",
      "2021-10-08 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.976876\n",
      "2021-10-08 03:27:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 150:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:27:33 | INFO | fairseq.trainer | begin training epoch 150\n",
      "epoch 150: 100%|9| 1289/1290 [03:19<00:00,  6.51it/s, loss=2.869, nll_loss=1.0492021-10-08 03:30:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001959\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065080\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041039\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108931\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001762\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062487\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040865\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105964\n",
      "2021-10-08 03:30:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 150 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.38it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.08it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.83it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.55it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.15it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.52it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.65it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.39it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.33it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.60it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.90it/s]\u001b[A\n",
      "epoch 150 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:30:55 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 4.719 | nll_loss 3.084 | ppl 8.48 | wps 52234.4 | wpb 2691.4 | bsz 201.4 | num_updates 74813 | best_loss 4.595\n",
      "2021-10-08 03:30:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:30:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint150.pt (epoch 150 @ 74813 updates, score 4.719) (writing took 3.2910978959989734 seconds)\n",
      "2021-10-08 03:30:58 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)\n",
      "2021-10-08 03:30:58 | INFO | train | epoch 150 | loss 2.811 | nll_loss 0.987 | ppl 1.98 | wps 23003.7 | ups 6.22 | wpb 3695.9 | bsz 249.8 | num_updates 74813 | lr 0.000115614 | gnorm 1.553 | loss_scale 8 | train_wall 194 | wall 0\n",
      "2021-10-08 03:30:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=151/shard_epoch=58\n",
      "2021-10-08 03:30:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=151/shard_epoch=59\n",
      "2021-10-08 03:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:30:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004764\n",
      "2021-10-08 03:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.101442\n",
      "2021-10-08 03:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:30:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[243773]\n",
      "2021-10-08 03:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007814\n",
      "2021-10-08 03:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.878244\n",
      "2021-10-08 03:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.988458\n",
      "2021-10-08 03:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084279\n",
      "2021-10-08 03:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:30:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[243773]\n",
      "2021-10-08 03:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008835\n",
      "2021-10-08 03:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.870647\n",
      "2021-10-08 03:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.964734\n",
      "2021-10-08 03:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 151:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:31:00 | INFO | fairseq.trainer | begin training epoch 151\n",
      "epoch 151: 100%|9| 1289/1290 [03:18<00:00,  6.41it/s, loss=2.784, nll_loss=0.9632021-10-08 03:34:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001830\n",
      "2021-10-08 03:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065583\n",
      "2021-10-08 03:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040936\n",
      "2021-10-08 03:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109074\n",
      "2021-10-08 03:34:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001580\n",
      "2021-10-08 03:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062390\n",
      "2021-10-08 03:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041315\n",
      "2021-10-08 03:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105967\n",
      "2021-10-08 03:34:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 151 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.39it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.08it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.82it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.55it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.17it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.53it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.62it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.38it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.71it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.87it/s]\u001b[A\n",
      "epoch 151 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:34:21 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 4.722 | nll_loss 3.086 | ppl 8.49 | wps 52310.1 | wpb 2691.4 | bsz 201.4 | num_updates 76103 | best_loss 4.595\n",
      "2021-10-08 03:34:21 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:34:25 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint151.pt (epoch 151 @ 76103 updates, score 4.722) (writing took 3.425568985985592 seconds)\n",
      "2021-10-08 03:34:25 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)\n",
      "2021-10-08 03:34:25 | INFO | train | epoch 151 | loss 2.806 | nll_loss 0.981 | ppl 1.97 | wps 23112.3 | ups 6.25 | wpb 3695.9 | bsz 249.8 | num_updates 76103 | lr 0.00011463 | gnorm 1.559 | loss_scale 8 | train_wall 193 | wall 0\n",
      "2021-10-08 03:34:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=152/shard_epoch=59\n",
      "2021-10-08 03:34:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=152/shard_epoch=60\n",
      "2021-10-08 03:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:34:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005522\n",
      "2021-10-08 03:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106927\n",
      "2021-10-08 03:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:25 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[88099]\n",
      "2021-10-08 03:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009910\n",
      "2021-10-08 03:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.023795\n",
      "2021-10-08 03:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.141618\n",
      "2021-10-08 03:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087986\n",
      "2021-10-08 03:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:26 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[88099]\n",
      "2021-10-08 03:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010410\n",
      "2021-10-08 03:34:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.901161\n",
      "2021-10-08 03:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.000503\n",
      "2021-10-08 03:34:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 152:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:34:27 | INFO | fairseq.trainer | begin training epoch 152\n",
      "epoch 152: 100%|9| 1289/1290 [03:19<00:00,  6.53it/s, loss=2.807, nll_loss=0.9872021-10-08 03:37:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002069\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063310\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042630\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108822\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001448\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062418\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042678\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107123\n",
      "2021-10-08 03:37:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 152 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.25it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.93it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.70it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.45it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.06it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.45it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.54it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.03it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.53it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.41it/s]\u001b[A\n",
      "epoch 152 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:37:48 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 4.71 | nll_loss 3.071 | ppl 8.4 | wps 52462.4 | wpb 2691.4 | bsz 201.4 | num_updates 77393 | best_loss 4.595\n",
      "2021-10-08 03:37:48 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:37:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint152.pt (epoch 152 @ 77393 updates, score 4.71) (writing took 3.3394044889719225 seconds)\n",
      "2021-10-08 03:37:52 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)\n",
      "2021-10-08 03:37:52 | INFO | train | epoch 152 | loss 2.803 | nll_loss 0.977 | ppl 1.97 | wps 23026.2 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 77393 | lr 0.000113671 | gnorm 1.563 | loss_scale 8 | train_wall 194 | wall 0\n",
      "2021-10-08 03:37:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=153/shard_epoch=60\n",
      "2021-10-08 03:37:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=153/shard_epoch=61\n",
      "2021-10-08 03:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:37:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004643\n",
      "2021-10-08 03:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.101605\n",
      "2021-10-08 03:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[157139]\n",
      "2021-10-08 03:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009887\n",
      "2021-10-08 03:37:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.910330\n",
      "2021-10-08 03:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.022802\n",
      "2021-10-08 03:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.081907\n",
      "2021-10-08 03:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:53 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[157139]\n",
      "2021-10-08 03:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008357\n",
      "2021-10-08 03:37:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.915805\n",
      "2021-10-08 03:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.007011\n",
      "2021-10-08 03:37:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 153:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:37:54 | INFO | fairseq.trainer | begin training epoch 153\n",
      "epoch 153: 100%|9| 1289/1290 [03:18<00:00,  6.55it/s, loss=2.786, nll_loss=0.9642021-10-08 03:41:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.003045\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064472\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041777\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110267\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001519\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062635\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043551\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108458\n",
      "2021-10-08 03:41:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 153 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.97it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.60it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.33it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.08it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.76it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.22it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.40it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.09it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.38it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.67it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.96it/s]\u001b[A\n",
      "epoch 153 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.55it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:41:15 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 4.722 | nll_loss 3.084 | ppl 8.48 | wps 52449 | wpb 2691.4 | bsz 201.4 | num_updates 78683 | best_loss 4.595\n",
      "2021-10-08 03:41:15 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:41:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint153.pt (epoch 153 @ 78683 updates, score 4.722) (writing took 3.308572814974468 seconds)\n",
      "2021-10-08 03:41:18 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)\n",
      "2021-10-08 03:41:18 | INFO | train | epoch 153 | loss 2.798 | nll_loss 0.972 | ppl 1.96 | wps 23106.1 | ups 6.25 | wpb 3695.9 | bsz 249.8 | num_updates 78683 | lr 0.000112735 | gnorm 1.561 | loss_scale 8 | train_wall 193 | wall 0\n",
      "2021-10-08 03:41:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=154/shard_epoch=61\n",
      "2021-10-08 03:41:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=154/shard_epoch=62\n",
      "2021-10-08 03:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:41:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005567\n",
      "2021-10-08 03:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108802\n",
      "2021-10-08 03:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[251237]\n",
      "2021-10-08 03:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008752\n",
      "2021-10-08 03:41:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.916201\n",
      "2021-10-08 03:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.034684\n",
      "2021-10-08 03:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.082795\n",
      "2021-10-08 03:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[251237]\n",
      "2021-10-08 03:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008333\n",
      "2021-10-08 03:41:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.909287\n",
      "2021-10-08 03:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.001389\n",
      "2021-10-08 03:41:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 154:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:41:20 | INFO | fairseq.trainer | begin training epoch 154\n",
      "epoch 154: 100%|9| 1289/1290 [03:19<00:00,  6.56it/s, loss=2.846, nll_loss=1.0272021-10-08 03:44:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:44:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:44:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001727\n",
      "2021-10-08 03:44:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063012\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041284\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106803\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001606\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063965\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041203\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107444\n",
      "2021-10-08 03:44:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 154 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.68it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.41it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.18it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.91it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.52it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.89it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.78it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.56it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.83it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.05it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.12it/s]\u001b[A\n",
      "epoch 154 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.71it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:44:41 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 4.726 | nll_loss 3.088 | ppl 8.51 | wps 52618.3 | wpb 2691.4 | bsz 201.4 | num_updates 79973 | best_loss 4.595\n",
      "2021-10-08 03:44:41 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:44:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint154.pt (epoch 154 @ 79973 updates, score 4.726) (writing took 3.4478493770002387 seconds)\n",
      "2021-10-08 03:44:45 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)\n",
      "2021-10-08 03:44:45 | INFO | train | epoch 154 | loss 2.796 | nll_loss 0.969 | ppl 1.96 | wps 23065.4 | ups 6.24 | wpb 3695.9 | bsz 249.8 | num_updates 79973 | lr 0.000111822 | gnorm 1.58 | loss_scale 8 | train_wall 193 | wall 0\n",
      "2021-10-08 03:44:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=155/shard_epoch=62\n",
      "2021-10-08 03:44:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=155/shard_epoch=63\n",
      "2021-10-08 03:44:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:44:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004523\n",
      "2021-10-08 03:44:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107494\n",
      "2021-10-08 03:44:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:45 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[13993]\n",
      "2021-10-08 03:44:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009808\n",
      "2021-10-08 03:44:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.913533\n",
      "2021-10-08 03:44:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.031757\n",
      "2021-10-08 03:44:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:44:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085134\n",
      "2021-10-08 03:44:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:46 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[13993]\n",
      "2021-10-08 03:44:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007956\n",
      "2021-10-08 03:44:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:44:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.911991\n",
      "2021-10-08 03:44:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.005983\n",
      "2021-10-08 03:44:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 155:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:44:47 | INFO | fairseq.trainer | begin training epoch 155\n",
      "epoch 155:  91%|9| 1170/1290 [03:01<00:17,  6.75it/s, loss=2.858, nll_loss=1.0352021-10-08 03:47:48 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0\n",
      "epoch 155: 100%|9| 1289/1290 [03:19<00:00,  6.40it/s, loss=2.859, nll_loss=1.0412021-10-08 03:48:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001743\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063596\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041201\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107437\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001755\n",
      "2021-10-08 03:48:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062835\n",
      "2021-10-08 03:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040632\n",
      "2021-10-08 03:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105954\n",
      "2021-10-08 03:48:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 155 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.39it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.07it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.81it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.54it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.13it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.59it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.56it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.34it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.59it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.88it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.03it/s]\u001b[A\n",
      "epoch 155 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:48:08 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 4.743 | nll_loss 3.109 | ppl 8.63 | wps 52257.6 | wpb 2691.4 | bsz 201.4 | num_updates 81262 | best_loss 4.595\n",
      "2021-10-08 03:48:08 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:48:11 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint155.pt (epoch 155 @ 81262 updates, score 4.743) (writing took 3.2787870559841394 seconds)\n",
      "2021-10-08 03:48:11 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)\n",
      "2021-10-08 03:48:11 | INFO | train | epoch 155 | loss 2.792 | nll_loss 0.965 | ppl 1.95 | wps 23045.9 | ups 6.24 | wpb 3695.6 | bsz 249.9 | num_updates 81262 | lr 0.000110932 | gnorm 1.574 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 03:48:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=156/shard_epoch=63\n",
      "2021-10-08 03:48:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=156/shard_epoch=64\n",
      "2021-10-08 03:48:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:48:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004708\n",
      "2021-10-08 03:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105960\n",
      "2021-10-08 03:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:12 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[56576]\n",
      "2021-10-08 03:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008476\n",
      "2021-10-08 03:48:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.971054\n",
      "2021-10-08 03:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.086440\n",
      "2021-10-08 03:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.082584\n",
      "2021-10-08 03:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:13 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[56576]\n",
      "2021-10-08 03:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008299\n",
      "2021-10-08 03:48:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:48:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.904420\n",
      "2021-10-08 03:48:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.996228\n",
      "2021-10-08 03:48:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 156:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:48:14 | INFO | fairseq.trainer | begin training epoch 156\n",
      "epoch 156: 100%|9| 1289/1290 [03:19<00:00,  6.29it/s, loss=2.889, nll_loss=1.0722021-10-08 03:51:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001735\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063240\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042721\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108467\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001712\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063548\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046577\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112650\n",
      "2021-10-08 03:51:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 156 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.37it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.06it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.82it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.57it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.55it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 11.54it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 13.14it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 14.56it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 16.19it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 17.03it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 17.72it/s]\u001b[A\n",
      "epoch 156 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:51:35 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 4.725 | nll_loss 3.089 | ppl 8.51 | wps 47112.2 | wpb 2691.4 | bsz 201.4 | num_updates 82552 | best_loss 4.595\n",
      "2021-10-08 03:51:35 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:51:38 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint156.pt (epoch 156 @ 82552 updates, score 4.725) (writing took 3.3603916590218432 seconds)\n",
      "2021-10-08 03:51:38 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)\n",
      "2021-10-08 03:51:38 | INFO | train | epoch 156 | loss 2.789 | nll_loss 0.961 | ppl 1.95 | wps 23030.9 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 82552 | lr 0.000110062 | gnorm 1.581 | loss_scale 4 | train_wall 193 | wall 0\n",
      "2021-10-08 03:51:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=157/shard_epoch=64\n",
      "2021-10-08 03:51:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=157/shard_epoch=65\n",
      "2021-10-08 03:51:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:51:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005008\n",
      "2021-10-08 03:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106352\n",
      "2021-10-08 03:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:39 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[87373]\n",
      "2021-10-08 03:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009030\n",
      "2021-10-08 03:51:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.937935\n",
      "2021-10-08 03:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.054315\n",
      "2021-10-08 03:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086454\n",
      "2021-10-08 03:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:40 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[87373]\n",
      "2021-10-08 03:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009973\n",
      "2021-10-08 03:51:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.930664\n",
      "2021-10-08 03:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.028103\n",
      "2021-10-08 03:51:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 157:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:51:41 | INFO | fairseq.trainer | begin training epoch 157\n",
      "epoch 157:  24%|2| 312/1290 [00:48<02:32,  6.40it/s, loss=2.778, nll_loss=0.946,2021-10-08 03:52:29 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2.0\n",
      "epoch 157: 100%|9| 1289/1290 [03:19<00:00,  6.95it/s, loss=2.762, nll_loss=0.9362021-10-08 03:55:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001550\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063165\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040744\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106205\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001441\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063409\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040707\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106457\n",
      "2021-10-08 03:55:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 157 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.34it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.02it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.78it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.51it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.11it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.49it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.42it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.74it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.93it/s]\u001b[A\n",
      "epoch 157 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:55:02 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 4.731 | nll_loss 3.096 | ppl 8.55 | wps 52406.9 | wpb 2691.4 | bsz 201.4 | num_updates 83841 | best_loss 4.595\n",
      "2021-10-08 03:55:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:55:05 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint157.pt (epoch 157 @ 83841 updates, score 4.731) (writing took 3.437177771003917 seconds)\n",
      "2021-10-08 03:55:05 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)\n",
      "2021-10-08 03:55:05 | INFO | train | epoch 157 | loss 2.785 | nll_loss 0.957 | ppl 1.94 | wps 23056.3 | ups 6.24 | wpb 3695.7 | bsz 249.9 | num_updates 83841 | lr 0.000109212 | gnorm 1.598 | loss_scale 2 | train_wall 193 | wall 0\n",
      "2021-10-08 03:55:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=158/shard_epoch=65\n",
      "2021-10-08 03:55:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=158/shard_epoch=66\n",
      "2021-10-08 03:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:55:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004761\n",
      "2021-10-08 03:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106183\n",
      "2021-10-08 03:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:05 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[107261]\n",
      "2021-10-08 03:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008839\n",
      "2021-10-08 03:55:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.888740\n",
      "2021-10-08 03:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.004752\n",
      "2021-10-08 03:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087692\n",
      "2021-10-08 03:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:06 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[107261]\n",
      "2021-10-08 03:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010054\n",
      "2021-10-08 03:55:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.889956\n",
      "2021-10-08 03:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.988657\n",
      "2021-10-08 03:55:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 158:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:55:07 | INFO | fairseq.trainer | begin training epoch 158\n",
      "epoch 158: 100%|9| 1289/1290 [03:19<00:00,  6.23it/s, loss=2.779, nll_loss=0.9562021-10-08 03:58:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001912\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064484\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043006\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110194\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001551\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066023\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040738\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109119\n",
      "2021-10-08 03:58:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 158 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.11it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.77it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.50it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.22it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.88it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.35it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.52it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.27it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.46it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.75it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.95it/s]\u001b[A\n",
      "epoch 158 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 03:58:29 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 4.745 | nll_loss 3.111 | ppl 8.64 | wps 52257.5 | wpb 2691.4 | bsz 201.4 | num_updates 85131 | best_loss 4.595\n",
      "2021-10-08 03:58:29 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 03:58:32 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint158.pt (epoch 158 @ 85131 updates, score 4.745) (writing took 3.2794774050125852 seconds)\n",
      "2021-10-08 03:58:32 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)\n",
      "2021-10-08 03:58:32 | INFO | train | epoch 158 | loss 2.782 | nll_loss 0.953 | ppl 1.94 | wps 23028.5 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 85131 | lr 0.000108382 | gnorm 1.584 | loss_scale 2 | train_wall 194 | wall 0\n",
      "2021-10-08 03:58:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=159/shard_epoch=66\n",
      "2021-10-08 03:58:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=159/shard_epoch=67\n",
      "2021-10-08 03:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:58:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004747\n",
      "2021-10-08 03:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109564\n",
      "2021-10-08 03:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[254621]\n",
      "2021-10-08 03:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010120\n",
      "2021-10-08 03:58:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.887352\n",
      "2021-10-08 03:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.008035\n",
      "2021-10-08 03:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 03:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.093216\n",
      "2021-10-08 03:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:33 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[254621]\n",
      "2021-10-08 03:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015389\n",
      "2021-10-08 03:58:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 03:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.898994\n",
      "2021-10-08 03:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.012156\n",
      "2021-10-08 03:58:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 159:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 03:58:34 | INFO | fairseq.trainer | begin training epoch 159\n",
      "epoch 159: 100%|9| 1289/1290 [03:20<00:00,  6.68it/s, loss=2.744, nll_loss=0.9212021-10-08 04:01:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001756\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063479\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041473\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107632\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001600\n",
      "2021-10-08 04:01:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:01:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069211\n",
      "2021-10-08 04:01:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:01:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040608\n",
      "2021-10-08 04:01:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112260\n",
      "2021-10-08 04:01:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 159 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.58it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.30it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.08it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.82it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.43it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.79it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.74it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.12it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.91it/s]\u001b[A\n",
      "epoch 159 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:01:57 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 4.723 | nll_loss 3.089 | ppl 8.51 | wps 52473.4 | wpb 2691.4 | bsz 201.4 | num_updates 86421 | best_loss 4.595\n",
      "2021-10-08 04:01:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:02:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint159.pt (epoch 159 @ 86421 updates, score 4.723) (writing took 3.383512737986166 seconds)\n",
      "2021-10-08 04:02:00 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)\n",
      "2021-10-08 04:02:00 | INFO | train | epoch 159 | loss 2.777 | nll_loss 0.949 | ppl 1.93 | wps 22878.7 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 86421 | lr 0.00010757 | gnorm 1.591 | loss_scale 2 | train_wall 195 | wall 0\n",
      "2021-10-08 04:02:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=160/shard_epoch=67\n",
      "2021-10-08 04:02:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=160/shard_epoch=68\n",
      "2021-10-08 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:02:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004543\n",
      "2021-10-08 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105152\n",
      "2021-10-08 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:02:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[105204]\n",
      "2021-10-08 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008542\n",
      "2021-10-08 04:02:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.926622\n",
      "2021-10-08 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.041264\n",
      "2021-10-08 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083801\n",
      "2021-10-08 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:02:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[105204]\n",
      "2021-10-08 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007608\n",
      "2021-10-08 04:02:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:02:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.899579\n",
      "2021-10-08 04:02:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.991884\n",
      "2021-10-08 04:02:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 160:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:02:03 | INFO | fairseq.trainer | begin training epoch 160\n",
      "epoch 160: 100%|9| 1289/1290 [03:20<00:00,  6.82it/s, loss=2.774, nll_loss=0.9532021-10-08 04:05:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001763\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.073385\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041921\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117950\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001519\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065079\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040857\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108319\n",
      "2021-10-08 04:05:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 160 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.11it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.76it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.51it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.23it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.86it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.19it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.41it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.92it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.39it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.65it/s]\u001b[A\n",
      "epoch 160 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:05:25 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 4.724 | nll_loss 3.091 | ppl 8.52 | wps 52414.1 | wpb 2691.4 | bsz 201.4 | num_updates 87711 | best_loss 4.595\n",
      "2021-10-08 04:05:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:05:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint160.pt (epoch 160 @ 87711 updates, score 4.724) (writing took 3.280645768973045 seconds)\n",
      "2021-10-08 04:05:28 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)\n",
      "2021-10-08 04:05:28 | INFO | train | epoch 160 | loss 2.776 | nll_loss 0.947 | ppl 1.93 | wps 22966.1 | ups 6.21 | wpb 3695.9 | bsz 249.8 | num_updates 87711 | lr 0.000106776 | gnorm 1.593 | loss_scale 2 | train_wall 194 | wall 0\n",
      "2021-10-08 04:05:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=161/shard_epoch=68\n",
      "2021-10-08 04:05:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=161/shard_epoch=69\n",
      "2021-10-08 04:05:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:05:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004719\n",
      "2021-10-08 04:05:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108783\n",
      "2021-10-08 04:05:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:28 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[159769]\n",
      "2021-10-08 04:05:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.014324\n",
      "2021-10-08 04:05:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.912010\n",
      "2021-10-08 04:05:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.036113\n",
      "2021-10-08 04:05:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:05:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085553\n",
      "2021-10-08 04:05:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[159769]\n",
      "2021-10-08 04:05:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009714\n",
      "2021-10-08 04:05:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:05:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.906579\n",
      "2021-10-08 04:05:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.002790\n",
      "2021-10-08 04:05:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 161:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:05:30 | INFO | fairseq.trainer | begin training epoch 161\n",
      "epoch 161: 100%|9| 1289/1290 [03:20<00:00,  6.27it/s, loss=2.831, nll_loss=1.0092021-10-08 04:08:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001749\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071177\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.048147\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.122030\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001747\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062241\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043377\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108304\n",
      "2021-10-08 04:08:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 161 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.54it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.12it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  8.83it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 10.60it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.29it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 13.76it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 14.99it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.11it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.57it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.13it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.47it/s]\u001b[A\n",
      "epoch 161 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:08:53 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 4.742 | nll_loss 3.11 | ppl 8.63 | wps 52043.5 | wpb 2691.4 | bsz 201.4 | num_updates 89001 | best_loss 4.595\n",
      "2021-10-08 04:08:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:08:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint161.pt (epoch 161 @ 89001 updates, score 4.742) (writing took 3.322042392042931 seconds)\n",
      "2021-10-08 04:08:56 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)\n",
      "2021-10-08 04:08:56 | INFO | train | epoch 161 | loss 2.772 | nll_loss 0.943 | ppl 1.92 | wps 22904.5 | ups 6.2 | wpb 3695.9 | bsz 249.8 | num_updates 89001 | lr 0.000105999 | gnorm 1.61 | loss_scale 2 | train_wall 195 | wall 0\n",
      "2021-10-08 04:08:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=162/shard_epoch=69\n",
      "2021-10-08 04:08:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=162/shard_epoch=70\n",
      "2021-10-08 04:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:08:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005544\n",
      "2021-10-08 04:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105528\n",
      "2021-10-08 04:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:56 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[637]\n",
      "2021-10-08 04:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009149\n",
      "2021-10-08 04:08:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.916964\n",
      "2021-10-08 04:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.032609\n",
      "2021-10-08 04:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090212\n",
      "2021-10-08 04:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:57 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[637]\n",
      "2021-10-08 04:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009940\n",
      "2021-10-08 04:08:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.153532\n",
      "2021-10-08 04:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.254678\n",
      "2021-10-08 04:08:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 162:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:08:59 | INFO | fairseq.trainer | begin training epoch 162\n",
      "epoch 162: 100%|9| 1289/1290 [03:20<00:00,  6.49it/s, loss=2.79, nll_loss=0.967,2021-10-08 04:12:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001730\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062818\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042399\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107826\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004672\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066719\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041864\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114376\n",
      "2021-10-08 04:12:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 162 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.89it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.51it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.15it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.86it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.48it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.95it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.20it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.14it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.36it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.65it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.90it/s]\u001b[A\n",
      "epoch 162 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.62it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:12:21 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 4.732 | nll_loss 3.099 | ppl 8.57 | wps 52045 | wpb 2691.4 | bsz 201.4 | num_updates 90291 | best_loss 4.595\n",
      "2021-10-08 04:12:21 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:12:24 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint162.pt (epoch 162 @ 90291 updates, score 4.732) (writing took 3.333715830987785 seconds)\n",
      "2021-10-08 04:12:24 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)\n",
      "2021-10-08 04:12:24 | INFO | train | epoch 162 | loss 2.769 | nll_loss 0.94 | ppl 1.92 | wps 22948.8 | ups 6.21 | wpb 3695.9 | bsz 249.8 | num_updates 90291 | lr 0.000105239 | gnorm 1.601 | loss_scale 2 | train_wall 194 | wall 0\n",
      "2021-10-08 04:12:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=163/shard_epoch=70\n",
      "2021-10-08 04:12:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=163/shard_epoch=71\n",
      "2021-10-08 04:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:12:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004931\n",
      "2021-10-08 04:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112278\n",
      "2021-10-08 04:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:24 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[38516]\n",
      "2021-10-08 04:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011624\n",
      "2021-10-08 04:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.896253\n",
      "2021-10-08 04:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.021182\n",
      "2021-10-08 04:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085890\n",
      "2021-10-08 04:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:25 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[38516]\n",
      "2021-10-08 04:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008239\n",
      "2021-10-08 04:12:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:12:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.897409\n",
      "2021-10-08 04:12:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.992472\n",
      "2021-10-08 04:12:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 163:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:12:26 | INFO | fairseq.trainer | begin training epoch 163\n",
      "epoch 163: 100%|9| 1289/1290 [03:20<00:00,  6.55it/s, loss=2.834, nll_loss=1.01,2021-10-08 04:15:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001689\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063578\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042731\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108894\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001522\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063828\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042455\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108513\n",
      "2021-10-08 04:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 163 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.74it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.35it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.05it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.81it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.48it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.05it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.32it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.95it/s]\u001b[A\n",
      "epoch 163 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:15:49 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 4.751 | nll_loss 3.114 | ppl 8.66 | wps 52043.2 | wpb 2691.4 | bsz 201.4 | num_updates 91581 | best_loss 4.595\n",
      "2021-10-08 04:15:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:15:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint163.pt (epoch 163 @ 91581 updates, score 4.751) (writing took 3.360021009983029 seconds)\n",
      "2021-10-08 04:15:52 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)\n",
      "2021-10-08 04:15:52 | INFO | train | epoch 163 | loss 2.766 | nll_loss 0.936 | ppl 1.91 | wps 22892.7 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 91581 | lr 0.000104495 | gnorm 1.614 | loss_scale 2 | train_wall 195 | wall 0\n",
      "2021-10-08 04:15:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=164/shard_epoch=71\n",
      "2021-10-08 04:15:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=164/shard_epoch=72\n",
      "2021-10-08 04:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:15:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004641\n",
      "2021-10-08 04:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107799\n",
      "2021-10-08 04:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[72391]\n",
      "2021-10-08 04:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008562\n",
      "2021-10-08 04:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.912997\n",
      "2021-10-08 04:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.030306\n",
      "2021-10-08 04:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086844\n",
      "2021-10-08 04:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:53 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[72391]\n",
      "2021-10-08 04:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008729\n",
      "2021-10-08 04:15:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.901487\n",
      "2021-10-08 04:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.997969\n",
      "2021-10-08 04:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 164:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:15:54 | INFO | fairseq.trainer | begin training epoch 164\n",
      "epoch 164: 100%|9| 1289/1290 [03:20<00:00,  6.78it/s, loss=2.824, nll_loss=1.0012021-10-08 04:19:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001989\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065166\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043817\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111801\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001713\n",
      "2021-10-08 04:19:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062490\n",
      "2021-10-08 04:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043266\n",
      "2021-10-08 04:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108021\n",
      "2021-10-08 04:19:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 164 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.46it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.15it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.89it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.61it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 10.16it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 11.84it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 13.35it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 14.71it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 15.93it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.46it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 17.90it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.26it/s]\u001b[A\n",
      "epoch 164 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.86it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:19:17 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 4.733 | nll_loss 3.095 | ppl 8.54 | wps 46927.3 | wpb 2691.4 | bsz 201.4 | num_updates 92871 | best_loss 4.595\n",
      "2021-10-08 04:19:17 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:19:21 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint164.pt (epoch 164 @ 92871 updates, score 4.733) (writing took 3.390536343038548 seconds)\n",
      "2021-10-08 04:19:21 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)\n",
      "2021-10-08 04:19:21 | INFO | train | epoch 164 | loss 2.764 | nll_loss 0.933 | ppl 1.91 | wps 22871.6 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 92871 | lr 0.000103767 | gnorm 1.616 | loss_scale 2 | train_wall 195 | wall 0\n",
      "2021-10-08 04:19:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=165/shard_epoch=72\n",
      "2021-10-08 04:19:21 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=165/shard_epoch=73\n",
      "2021-10-08 04:19:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:19:21 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004951\n",
      "2021-10-08 04:19:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111208\n",
      "2021-10-08 04:19:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:21 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[231151]\n",
      "2021-10-08 04:19:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.012495\n",
      "2021-10-08 04:19:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.908435\n",
      "2021-10-08 04:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.033330\n",
      "2021-10-08 04:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.081194\n",
      "2021-10-08 04:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:22 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[231151]\n",
      "2021-10-08 04:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007779\n",
      "2021-10-08 04:19:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:19:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.911755\n",
      "2021-10-08 04:19:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.001640\n",
      "2021-10-08 04:19:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 165:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:19:23 | INFO | fairseq.trainer | begin training epoch 165\n",
      "epoch 165: 100%|9| 1289/1290 [03:18<00:00,  6.49it/s, loss=2.711, nll_loss=0.8812021-10-08 04:22:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001559\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062092\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041067\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105351\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001374\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066369\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041292\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109946\n",
      "2021-10-08 04:22:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 165 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.34it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.03it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.78it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.51it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.09it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.48it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.05it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.44it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 165 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:22:44 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 4.742 | nll_loss 3.113 | ppl 8.65 | wps 52394.4 | wpb 2691.4 | bsz 201.4 | num_updates 94161 | best_loss 4.595\n",
      "2021-10-08 04:22:44 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:22:47 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint165.pt (epoch 165 @ 94161 updates, score 4.742) (writing took 3.4435241160099395 seconds)\n",
      "2021-10-08 04:22:47 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)\n",
      "2021-10-08 04:22:47 | INFO | train | epoch 165 | loss 2.759 | nll_loss 0.928 | ppl 1.9 | wps 23108.1 | ups 6.25 | wpb 3695.9 | bsz 249.8 | num_updates 94161 | lr 0.000103054 | gnorm 1.616 | loss_scale 2 | train_wall 193 | wall 0\n",
      "2021-10-08 04:22:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=166/shard_epoch=73\n",
      "2021-10-08 04:22:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=166/shard_epoch=74\n",
      "2021-10-08 04:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:22:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005014\n",
      "2021-10-08 04:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106232\n",
      "2021-10-08 04:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:47 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[232447]\n",
      "2021-10-08 04:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008254\n",
      "2021-10-08 04:22:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.875136\n",
      "2021-10-08 04:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.990527\n",
      "2021-10-08 04:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086237\n",
      "2021-10-08 04:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[232447]\n",
      "2021-10-08 04:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008620\n",
      "2021-10-08 04:22:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.886264\n",
      "2021-10-08 04:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.982036\n",
      "2021-10-08 04:22:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 166:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:22:49 | INFO | fairseq.trainer | begin training epoch 166\n",
      "epoch 166: 100%|9| 1289/1290 [03:19<00:00,  6.33it/s, loss=2.812, nll_loss=0.9882021-10-08 04:26:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001811\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063859\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041182\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107774\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001568\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064474\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041079\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108070\n",
      "2021-10-08 04:26:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 166 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.04it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  11%|7      | 3/28 [00:00<00:03,  7.61it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.36it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.11it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.77it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.27it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.49it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.39it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.61it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.89it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.15it/s]\u001b[A\n",
      "epoch 166 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:26:10 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 4.748 | nll_loss 3.115 | ppl 8.66 | wps 51969.6 | wpb 2691.4 | bsz 201.4 | num_updates 95451 | best_loss 4.595\n",
      "2021-10-08 04:26:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:26:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint166.pt (epoch 166 @ 95451 updates, score 4.748) (writing took 3.4710516359773465 seconds)\n",
      "2021-10-08 04:26:14 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)\n",
      "2021-10-08 04:26:14 | INFO | train | epoch 166 | loss 2.756 | nll_loss 0.925 | ppl 1.9 | wps 23088.4 | ups 6.25 | wpb 3695.9 | bsz 249.8 | num_updates 95451 | lr 0.000102355 | gnorm 1.621 | loss_scale 2 | train_wall 193 | wall 0\n",
      "2021-10-08 04:26:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=167/shard_epoch=74\n",
      "2021-10-08 04:26:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=167/shard_epoch=75\n",
      "2021-10-08 04:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:26:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004908\n",
      "2021-10-08 04:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103859\n",
      "2021-10-08 04:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:14 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[178878]\n",
      "2021-10-08 04:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007729\n",
      "2021-10-08 04:26:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.886020\n",
      "2021-10-08 04:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.998557\n",
      "2021-10-08 04:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085699\n",
      "2021-10-08 04:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:15 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[178878]\n",
      "2021-10-08 04:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007814\n",
      "2021-10-08 04:26:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:26:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.878182\n",
      "2021-10-08 04:26:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.972617\n",
      "2021-10-08 04:26:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 167:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:26:16 | INFO | fairseq.trainer | begin training epoch 167\n",
      "epoch 167: 100%|9| 1289/1290 [03:19<00:00,  6.41it/s, loss=2.771, nll_loss=0.9452021-10-08 04:29:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001746\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064194\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041353\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108288\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001755\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063491\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042404\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108631\n",
      "2021-10-08 04:29:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 167 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.75it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.34it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.06it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.78it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.38it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.09it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.40it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.68it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.89it/s]\u001b[A\n",
      "epoch 167 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:29:37 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 4.742 | nll_loss 3.107 | ppl 8.62 | wps 51980.4 | wpb 2691.4 | bsz 201.4 | num_updates 96741 | best_loss 4.595\n",
      "2021-10-08 04:29:37 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:29:40 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint167.pt (epoch 167 @ 96741 updates, score 4.742) (writing took 3.2871246699942276 seconds)\n",
      "2021-10-08 04:29:40 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)\n",
      "2021-10-08 04:29:40 | INFO | train | epoch 167 | loss 2.754 | nll_loss 0.923 | ppl 1.9 | wps 23060.2 | ups 6.24 | wpb 3695.9 | bsz 249.8 | num_updates 96741 | lr 0.00010167 | gnorm 1.635 | loss_scale 2 | train_wall 194 | wall 0\n",
      "2021-10-08 04:29:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=168/shard_epoch=75\n",
      "2021-10-08 04:29:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=168/shard_epoch=76\n",
      "2021-10-08 04:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:29:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005211\n",
      "2021-10-08 04:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107145\n",
      "2021-10-08 04:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:40 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[185718]\n",
      "2021-10-08 04:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008749\n",
      "2021-10-08 04:29:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.198920\n",
      "2021-10-08 04:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.315777\n",
      "2021-10-08 04:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083096\n",
      "2021-10-08 04:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:42 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[185718]\n",
      "2021-10-08 04:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010493\n",
      "2021-10-08 04:29:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:29:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.077835\n",
      "2021-10-08 04:29:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.172542\n",
      "2021-10-08 04:29:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 168:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:29:43 | INFO | fairseq.trainer | begin training epoch 168\n",
      "epoch 168: 100%|9| 1289/1290 [03:19<00:00,  6.37it/s, loss=2.828, nll_loss=1.0082021-10-08 04:33:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001714\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062807\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042918\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108141\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001480\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061971\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047823\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111971\n",
      "2021-10-08 04:33:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 168 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.33it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.03it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.74it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.41it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.01it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.42it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.54it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.32it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.79it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.98it/s]\u001b[A\n",
      "epoch 168 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.55it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:33:04 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 4.75 | nll_loss 3.118 | ppl 8.68 | wps 51894.3 | wpb 2691.4 | bsz 201.4 | num_updates 98031 | best_loss 4.595\n",
      "2021-10-08 04:33:04 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:33:07 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint168.pt (epoch 168 @ 98031 updates, score 4.75) (writing took 3.4559048759983853 seconds)\n",
      "2021-10-08 04:33:08 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)\n",
      "2021-10-08 04:33:08 | INFO | train | epoch 168 | loss 2.752 | nll_loss 0.92 | ppl 1.89 | wps 23008.6 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 98031 | lr 0.000100999 | gnorm 1.632 | loss_scale 2 | train_wall 193 | wall 0\n",
      "2021-10-08 04:33:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=169/shard_epoch=76\n",
      "2021-10-08 04:33:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=169/shard_epoch=77\n",
      "2021-10-08 04:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:33:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006069\n",
      "2021-10-08 04:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109829\n",
      "2021-10-08 04:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:08 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[115604]\n",
      "2021-10-08 04:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007831\n",
      "2021-10-08 04:33:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.913379\n",
      "2021-10-08 04:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.031950\n",
      "2021-10-08 04:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087495\n",
      "2021-10-08 04:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:09 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[115604]\n",
      "2021-10-08 04:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008319\n",
      "2021-10-08 04:33:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.897434\n",
      "2021-10-08 04:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.994165\n",
      "2021-10-08 04:33:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 169:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:33:10 | INFO | fairseq.trainer | begin training epoch 169\n",
      "epoch 169: 100%|9| 1289/1290 [03:19<00:00,  6.59it/s, loss=2.766, nll_loss=0.94,2021-10-08 04:36:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001746\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064842\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042892\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110338\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001580\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062300\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043113\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107660\n",
      "2021-10-08 04:36:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 169 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.05it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.66it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.39it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.10it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.75it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.22it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.41it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.33it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.18it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.51it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.89it/s]\u001b[A\n",
      "epoch 169 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:36:31 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 4.741 | nll_loss 3.106 | ppl 8.61 | wps 51727.4 | wpb 2691.4 | bsz 201.4 | num_updates 99321 | best_loss 4.595\n",
      "2021-10-08 04:36:31 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:36:35 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint169.pt (epoch 169 @ 99321 updates, score 4.741) (writing took 3.286060866957996 seconds)\n",
      "2021-10-08 04:36:35 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)\n",
      "2021-10-08 04:36:35 | INFO | train | epoch 169 | loss 2.748 | nll_loss 0.916 | ppl 1.89 | wps 23012.7 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 99321 | lr 0.000100341 | gnorm 1.636 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 04:36:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=170/shard_epoch=77\n",
      "2021-10-08 04:36:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=170/shard_epoch=78\n",
      "2021-10-08 04:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:36:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004858\n",
      "2021-10-08 04:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.101368\n",
      "2021-10-08 04:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:35 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[161031]\n",
      "2021-10-08 04:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008174\n",
      "2021-10-08 04:36:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902540\n",
      "2021-10-08 04:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.013001\n",
      "2021-10-08 04:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.081823\n",
      "2021-10-08 04:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:36 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[161031]\n",
      "2021-10-08 04:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007807\n",
      "2021-10-08 04:36:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:36:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.899791\n",
      "2021-10-08 04:36:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.990311\n",
      "2021-10-08 04:36:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 170:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:36:37 | INFO | fairseq.trainer | begin training epoch 170\n",
      "epoch 170: 100%|9| 1289/1290 [03:20<00:00,  6.40it/s, loss=2.829, nll_loss=1.0042021-10-08 04:39:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001696\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067765\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043517\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.113653\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001395\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064893\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041178\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108765\n",
      "2021-10-08 04:39:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 170 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.80it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.41it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.14it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 10.84it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.47it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.16it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.27it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.74it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.35it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.77it/s]\u001b[A\n",
      "epoch 170 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.99it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:39:59 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 4.763 | nll_loss 3.13 | ppl 8.76 | wps 52574.8 | wpb 2691.4 | bsz 201.4 | num_updates 100611 | best_loss 4.595\n",
      "2021-10-08 04:39:59 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:40:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint170.pt (epoch 170 @ 100611 updates, score 4.763) (writing took 3.3632976950029843 seconds)\n",
      "2021-10-08 04:40:02 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)\n",
      "2021-10-08 04:40:02 | INFO | train | epoch 170 | loss 2.745 | nll_loss 0.913 | ppl 1.88 | wps 22959.6 | ups 6.21 | wpb 3695.9 | bsz 249.8 | num_updates 100611 | lr 9.96959e-05 | gnorm 1.64 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 04:40:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=171/shard_epoch=78\n",
      "2021-10-08 04:40:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=171/shard_epoch=79\n",
      "2021-10-08 04:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:40:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005038\n",
      "2021-10-08 04:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105990\n",
      "2021-10-08 04:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:40:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[213739]\n",
      "2021-10-08 04:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008174\n",
      "2021-10-08 04:40:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.903910\n",
      "2021-10-08 04:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.018974\n",
      "2021-10-08 04:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:40:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085956\n",
      "2021-10-08 04:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:40:04 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[213739]\n",
      "2021-10-08 04:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009237\n",
      "2021-10-08 04:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.896334\n",
      "2021-10-08 04:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.992489\n",
      "2021-10-08 04:40:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 171:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:40:04 | INFO | fairseq.trainer | begin training epoch 171\n",
      "epoch 171: 100%|9| 1289/1290 [03:19<00:00,  6.27it/s, loss=2.774, nll_loss=0.9472021-10-08 04:43:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002942\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062593\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041301\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107730\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001857\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062359\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040852\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105905\n",
      "2021-10-08 04:43:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 171 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.11it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.78it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.52it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.25it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.84it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.25it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.38it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.31it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.65it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.17it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.59it/s]\u001b[A\n",
      "epoch 171 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.82it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:43:25 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 4.757 | nll_loss 3.126 | ppl 8.73 | wps 51930.7 | wpb 2691.4 | bsz 201.4 | num_updates 101901 | best_loss 4.595\n",
      "2021-10-08 04:43:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:43:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint171.pt (epoch 171 @ 101901 updates, score 4.757) (writing took 3.313872207014356 seconds)\n",
      "2021-10-08 04:43:29 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)\n",
      "2021-10-08 04:43:29 | INFO | train | epoch 171 | loss 2.743 | nll_loss 0.91 | ppl 1.88 | wps 23089.7 | ups 6.25 | wpb 3695.9 | bsz 249.8 | num_updates 101901 | lr 9.90628e-05 | gnorm 1.645 | loss_scale 4 | train_wall 193 | wall 0\n",
      "2021-10-08 04:43:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=172/shard_epoch=79\n",
      "2021-10-08 04:43:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=172/shard_epoch=80\n",
      "2021-10-08 04:43:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:43:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005081\n",
      "2021-10-08 04:43:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103625\n",
      "2021-10-08 04:43:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[103520]\n",
      "2021-10-08 04:43:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008143\n",
      "2021-10-08 04:43:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902692\n",
      "2021-10-08 04:43:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.015413\n",
      "2021-10-08 04:43:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:43:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088127\n",
      "2021-10-08 04:43:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[103520]\n",
      "2021-10-08 04:43:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010412\n",
      "2021-10-08 04:43:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.900539\n",
      "2021-10-08 04:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.000012\n",
      "2021-10-08 04:43:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 172:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:43:31 | INFO | fairseq.trainer | begin training epoch 172\n",
      "epoch 172: 100%|9| 1289/1290 [03:21<00:00,  6.74it/s, loss=2.726, nll_loss=0.8942021-10-08 04:46:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002148\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063824\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041468\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108315\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002007\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062297\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040595\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105709\n",
      "2021-10-08 04:46:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 172 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.37it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.07it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  8.65it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 10.42it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.12it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 13.64it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 14.89it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  61%|###6  | 17/28 [00:01<00:00, 15.98it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.56it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.18it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.57it/s]\u001b[A\n",
      "epoch 172 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:46:55 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 4.763 | nll_loss 3.136 | ppl 8.79 | wps 47789.2 | wpb 2691.4 | bsz 201.4 | num_updates 103191 | best_loss 4.595\n",
      "2021-10-08 04:46:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:46:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint172.pt (epoch 172 @ 103191 updates, score 4.763) (writing took 3.2974249710096046 seconds)\n",
      "2021-10-08 04:46:58 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)\n",
      "2021-10-08 04:46:58 | INFO | train | epoch 172 | loss 2.741 | nll_loss 0.907 | ppl 1.88 | wps 22768.7 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 103191 | lr 9.84417e-05 | gnorm 1.655 | loss_scale 4 | train_wall 196 | wall 0\n",
      "2021-10-08 04:46:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=173/shard_epoch=80\n",
      "2021-10-08 04:46:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=173/shard_epoch=81\n",
      "2021-10-08 04:46:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:46:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004869\n",
      "2021-10-08 04:46:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.100583\n",
      "2021-10-08 04:46:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:46:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[128827]\n",
      "2021-10-08 04:46:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009101\n",
      "2021-10-08 04:46:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:46:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.891211\n",
      "2021-10-08 04:46:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.001794\n",
      "2021-10-08 04:46:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:46:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:46:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083900\n",
      "2021-10-08 04:46:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:46:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[128827]\n",
      "2021-10-08 04:46:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009047\n",
      "2021-10-08 04:46:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.895244\n",
      "2021-10-08 04:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.989105\n",
      "2021-10-08 04:47:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 173:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:47:00 | INFO | fairseq.trainer | begin training epoch 173\n",
      "epoch 173: 100%|9| 1289/1290 [03:19<00:00,  6.59it/s, loss=2.776, nll_loss=0.9482021-10-08 04:50:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001566\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066959\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041420\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110893\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001538\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063469\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041146\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107033\n",
      "2021-10-08 04:50:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 173 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.22it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.90it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.64it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.37it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.98it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.46it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.66it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.59it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.39it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.87it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.13it/s]\u001b[A\n",
      "epoch 173 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.52it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:50:22 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 4.767 | nll_loss 3.139 | ppl 8.81 | wps 52217 | wpb 2691.4 | bsz 201.4 | num_updates 104481 | best_loss 4.595\n",
      "2021-10-08 04:50:22 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:50:25 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint173.pt (epoch 173 @ 104481 updates, score 4.767) (writing took 3.3843499760259874 seconds)\n",
      "2021-10-08 04:50:25 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)\n",
      "2021-10-08 04:50:25 | INFO | train | epoch 173 | loss 2.738 | nll_loss 0.905 | ppl 1.87 | wps 23038 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 104481 | lr 9.78321e-05 | gnorm 1.655 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 04:50:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=174/shard_epoch=81\n",
      "2021-10-08 04:50:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=174/shard_epoch=82\n",
      "2021-10-08 04:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:50:25 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005949\n",
      "2021-10-08 04:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107446\n",
      "2021-10-08 04:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:25 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[228561]\n",
      "2021-10-08 04:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008138\n",
      "2021-10-08 04:50:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.867494\n",
      "2021-10-08 04:50:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.984005\n",
      "2021-10-08 04:50:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:50:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083014\n",
      "2021-10-08 04:50:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:26 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[228561]\n",
      "2021-10-08 04:50:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008399\n",
      "2021-10-08 04:50:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.889665\n",
      "2021-10-08 04:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.982037\n",
      "2021-10-08 04:50:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 174:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:50:27 | INFO | fairseq.trainer | begin training epoch 174\n",
      "epoch 174: 100%|9| 1289/1290 [03:19<00:00,  6.39it/s, loss=2.79, nll_loss=0.961,2021-10-08 04:53:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001875\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064245\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043262\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110378\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001617\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067116\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043992\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.113668\n",
      "2021-10-08 04:53:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 174 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.71it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.31it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.03it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.80it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.49it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.98it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.21it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.18it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.98it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.13it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.47it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.78it/s]\u001b[A\n",
      "epoch 174 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.33it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:53:49 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 4.748 | nll_loss 3.117 | ppl 8.67 | wps 51973.5 | wpb 2691.4 | bsz 201.4 | num_updates 105771 | best_loss 4.595\n",
      "2021-10-08 04:53:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:53:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint174.pt (epoch 174 @ 105771 updates, score 4.748) (writing took 3.421478501986712 seconds)\n",
      "2021-10-08 04:53:52 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)\n",
      "2021-10-08 04:53:52 | INFO | train | epoch 174 | loss 2.736 | nll_loss 0.903 | ppl 1.87 | wps 23036.8 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 105771 | lr 9.72337e-05 | gnorm 1.66 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 04:53:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=175/shard_epoch=82\n",
      "2021-10-08 04:53:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=175/shard_epoch=83\n",
      "2021-10-08 04:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:53:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005024\n",
      "2021-10-08 04:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103427\n",
      "2021-10-08 04:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[25880]\n",
      "2021-10-08 04:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007969\n",
      "2021-10-08 04:53:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.871434\n",
      "2021-10-08 04:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.983760\n",
      "2021-10-08 04:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086946\n",
      "2021-10-08 04:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:53 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[25880]\n",
      "2021-10-08 04:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009689\n",
      "2021-10-08 04:53:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.849313\n",
      "2021-10-08 04:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.946872\n",
      "2021-10-08 04:53:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 175:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:53:54 | INFO | fairseq.trainer | begin training epoch 175\n",
      "epoch 175: 100%|9| 1289/1290 [03:19<00:00,  6.22it/s, loss=2.74, nll_loss=0.909,2021-10-08 04:57:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001745\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065622\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046348\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114726\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001690\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062301\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040455\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105224\n",
      "2021-10-08 04:57:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 175 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.24it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.91it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.65it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.39it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.05it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.51it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.38it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.67it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.92it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.13it/s]\u001b[A\n",
      "epoch 175 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.69it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 04:57:16 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 4.734 | nll_loss 3.102 | ppl 8.59 | wps 52425.1 | wpb 2691.4 | bsz 201.4 | num_updates 107061 | best_loss 4.595\n",
      "2021-10-08 04:57:16 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 04:57:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint175.pt (epoch 175 @ 107061 updates, score 4.734) (writing took 3.3478603849653155 seconds)\n",
      "2021-10-08 04:57:19 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)\n",
      "2021-10-08 04:57:19 | INFO | train | epoch 175 | loss 2.733 | nll_loss 0.898 | ppl 1.86 | wps 23031.4 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 107061 | lr 9.66461e-05 | gnorm 1.665 | loss_scale 4 | train_wall 194 | wall 0\n",
      "2021-10-08 04:57:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=176/shard_epoch=83\n",
      "2021-10-08 04:57:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=176/shard_epoch=84\n",
      "2021-10-08 04:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:57:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005008\n",
      "2021-10-08 04:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108477\n",
      "2021-10-08 04:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[205173]\n",
      "2021-10-08 04:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007934\n",
      "2021-10-08 04:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.900064\n",
      "2021-10-08 04:57:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.017394\n",
      "2021-10-08 04:57:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 04:57:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090491\n",
      "2021-10-08 04:57:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[205173]\n",
      "2021-10-08 04:57:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008375\n",
      "2021-10-08 04:57:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 04:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.893965\n",
      "2021-10-08 04:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.993788\n",
      "2021-10-08 04:57:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 176:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 04:57:21 | INFO | fairseq.trainer | begin training epoch 176\n",
      "epoch 176: 100%|9| 1289/1290 [03:18<00:00,  6.33it/s, loss=2.714, nll_loss=0.8822021-10-08 05:00:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001776\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069272\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052299\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.123997\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001609\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070852\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051850\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.124916\n",
      "2021-10-08 05:00:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 176 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.83it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.45it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.17it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 10.89it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.48it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 13.93it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.18it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.68it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.27it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 176 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.86it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:00:42 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 4.748 | nll_loss 3.119 | ppl 8.69 | wps 52189 | wpb 2691.4 | bsz 201.4 | num_updates 108351 | best_loss 4.595\n",
      "2021-10-08 05:00:42 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:00:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint176.pt (epoch 176 @ 108351 updates, score 4.748) (writing took 3.3709238080191426 seconds)\n",
      "2021-10-08 05:00:45 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)\n",
      "2021-10-08 05:00:45 | INFO | train | epoch 176 | loss 2.73 | nll_loss 0.896 | ppl 1.86 | wps 23137.1 | ups 6.26 | wpb 3695.9 | bsz 249.8 | num_updates 108351 | lr 9.60691e-05 | gnorm 1.659 | loss_scale 4 | train_wall 193 | wall 0\n",
      "2021-10-08 05:00:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=177/shard_epoch=84\n",
      "2021-10-08 05:00:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=177/shard_epoch=85\n",
      "2021-10-08 05:00:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:00:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004967\n",
      "2021-10-08 05:00:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107538\n",
      "2021-10-08 05:00:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:45 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[194570]\n",
      "2021-10-08 05:00:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011054\n",
      "2021-10-08 05:00:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.906387\n",
      "2021-10-08 05:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.026105\n",
      "2021-10-08 05:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084289\n",
      "2021-10-08 05:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:46 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[194570]\n",
      "2021-10-08 05:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008825\n",
      "2021-10-08 05:00:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.901161\n",
      "2021-10-08 05:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.995239\n",
      "2021-10-08 05:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 177:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:00:47 | INFO | fairseq.trainer | begin training epoch 177\n",
      "epoch 177: 100%|9| 1289/1290 [03:18<00:00,  6.90it/s, loss=2.776, nll_loss=0.9452021-10-08 05:04:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001747\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063483\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042489\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108358\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001546\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062823\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042855\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108054\n",
      "2021-10-08 05:04:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 177 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.51it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.08it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  8.80it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 10.55it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.18it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 13.66it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 14.93it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.33it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.02it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.49it/s]\u001b[A\n",
      "epoch 177 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:04:08 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 4.765 | nll_loss 3.136 | ppl 8.79 | wps 52275.9 | wpb 2691.4 | bsz 201.4 | num_updates 109641 | best_loss 4.595\n",
      "2021-10-08 05:04:08 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:04:11 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint177.pt (epoch 177 @ 109641 updates, score 4.765) (writing took 3.268602913012728 seconds)\n",
      "2021-10-08 05:04:11 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)\n",
      "2021-10-08 05:04:11 | INFO | train | epoch 177 | loss 2.729 | nll_loss 0.894 | ppl 1.86 | wps 23151 | ups 6.26 | wpb 3695.9 | bsz 249.8 | num_updates 109641 | lr 9.55022e-05 | gnorm 1.688 | loss_scale 4 | train_wall 193 | wall 0\n",
      "2021-10-08 05:04:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=178/shard_epoch=85\n",
      "2021-10-08 05:04:11 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=178/shard_epoch=86\n",
      "2021-10-08 05:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:04:11 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005484\n",
      "2021-10-08 05:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106170\n",
      "2021-10-08 05:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:11 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[114396]\n",
      "2021-10-08 05:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008581\n",
      "2021-10-08 05:04:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.904788\n",
      "2021-10-08 05:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.020443\n",
      "2021-10-08 05:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083044\n",
      "2021-10-08 05:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:12 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[114396]\n",
      "2021-10-08 05:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007649\n",
      "2021-10-08 05:04:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.907755\n",
      "2021-10-08 05:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.999326\n",
      "2021-10-08 05:04:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 178:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:04:13 | INFO | fairseq.trainer | begin training epoch 178\n",
      "epoch 178: 100%|9| 1289/1290 [03:18<00:00,  6.66it/s, loss=2.781, nll_loss=0.9512021-10-08 05:07:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001800\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063823\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042773\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109426\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001791\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067391\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045255\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115456\n",
      "2021-10-08 05:07:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 178 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.17it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.82it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.55it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.28it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.93it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.17it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.29it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.25it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.12it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.42it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.77it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.93it/s]\u001b[A\n",
      "epoch 178 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:07:34 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 4.77 | nll_loss 3.142 | ppl 8.83 | wps 51654.4 | wpb 2691.4 | bsz 201.4 | num_updates 110931 | best_loss 4.595\n",
      "2021-10-08 05:07:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:07:37 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint178.pt (epoch 178 @ 110931 updates, score 4.77) (writing took 3.313636253995355 seconds)\n",
      "2021-10-08 05:07:37 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)\n",
      "2021-10-08 05:07:37 | INFO | train | epoch 178 | loss 2.726 | nll_loss 0.892 | ppl 1.86 | wps 23121.2 | ups 6.26 | wpb 3695.9 | bsz 249.8 | num_updates 110931 | lr 9.49453e-05 | gnorm 1.674 | loss_scale 4 | train_wall 193 | wall 0\n",
      "2021-10-08 05:07:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=179/shard_epoch=86\n",
      "2021-10-08 05:07:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=179/shard_epoch=87\n",
      "2021-10-08 05:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:07:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004655\n",
      "2021-10-08 05:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105105\n",
      "2021-10-08 05:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:37 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[178230]\n",
      "2021-10-08 05:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008454\n",
      "2021-10-08 05:07:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.908100\n",
      "2021-10-08 05:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.022610\n",
      "2021-10-08 05:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:07:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090046\n",
      "2021-10-08 05:07:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:39 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[178230]\n",
      "2021-10-08 05:07:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011788\n",
      "2021-10-08 05:07:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:07:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902466\n",
      "2021-10-08 05:07:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.005283\n",
      "2021-10-08 05:07:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 179:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:07:39 | INFO | fairseq.trainer | begin training epoch 179\n",
      "epoch 179: 100%|9| 1289/1290 [03:19<00:00,  6.48it/s, loss=2.686, nll_loss=0.8552021-10-08 05:10:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001820\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061987\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042211\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106652\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001626\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062848\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040616\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105657\n",
      "2021-10-08 05:10:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 179 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.02it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.68it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.42it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.18it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 12.75it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.23it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.44it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.48it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.89it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.28it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.60it/s]\u001b[A\n",
      "epoch 179 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.15it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:11:01 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 4.763 | nll_loss 3.131 | ppl 8.76 | wps 52324.3 | wpb 2691.4 | bsz 201.4 | num_updates 112221 | best_loss 4.595\n",
      "2021-10-08 05:11:01 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:11:04 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint179.pt (epoch 179 @ 112221 updates, score 4.763) (writing took 3.3616789979860187 seconds)\n",
      "2021-10-08 05:11:04 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)\n",
      "2021-10-08 05:11:04 | INFO | train | epoch 179 | loss 2.723 | nll_loss 0.888 | ppl 1.85 | wps 23081.8 | ups 6.25 | wpb 3695.9 | bsz 249.8 | num_updates 112221 | lr 9.4398e-05 | gnorm 1.678 | loss_scale 4 | train_wall 193 | wall 0\n",
      "2021-10-08 05:11:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=180/shard_epoch=87\n",
      "2021-10-08 05:11:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=180/shard_epoch=88\n",
      "2021-10-08 05:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:11:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005125\n",
      "2021-10-08 05:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103570\n",
      "2021-10-08 05:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:11:04 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[37532]\n",
      "2021-10-08 05:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011669\n",
      "2021-10-08 05:11:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.918067\n",
      "2021-10-08 05:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.034282\n",
      "2021-10-08 05:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083044\n",
      "2021-10-08 05:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:11:05 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[37532]\n",
      "2021-10-08 05:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008543\n",
      "2021-10-08 05:11:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.894887\n",
      "2021-10-08 05:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.987390\n",
      "2021-10-08 05:11:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 180:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:11:06 | INFO | fairseq.trainer | begin training epoch 180\n",
      "epoch 180: 100%|9| 1289/1290 [03:18<00:00,  6.43it/s, loss=2.804, nll_loss=0.9762021-10-08 05:14:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002032\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062835\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042293\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108012\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001657\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062493\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.039867\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104601\n",
      "2021-10-08 05:14:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 180 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.21it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.87it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.60it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.36it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.02it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.40it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.47it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.34it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.60it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.87it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.01it/s]\u001b[A\n",
      "epoch 180 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:14:26 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 4.768 | nll_loss 3.143 | ppl 8.84 | wps 52293.8 | wpb 2691.4 | bsz 201.4 | num_updates 113511 | best_loss 4.595\n",
      "2021-10-08 05:14:26 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:14:30 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint180.pt (epoch 180 @ 113511 updates, score 4.768) (writing took 3.3976371539756656 seconds)\n",
      "2021-10-08 05:14:30 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)\n",
      "2021-10-08 05:14:30 | INFO | train | epoch 180 | loss 2.722 | nll_loss 0.886 | ppl 1.85 | wps 23148.4 | ups 6.26 | wpb 3695.9 | bsz 249.8 | num_updates 113511 | lr 9.38601e-05 | gnorm 1.683 | loss_scale 4 | train_wall 193 | wall 0\n",
      "2021-10-08 05:14:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=181/shard_epoch=88\n",
      "2021-10-08 05:14:30 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=181/shard_epoch=89\n",
      "2021-10-08 05:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:14:30 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005023\n",
      "2021-10-08 05:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105319\n",
      "2021-10-08 05:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[134633]\n",
      "2021-10-08 05:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008512\n",
      "2021-10-08 05:14:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.895696\n",
      "2021-10-08 05:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.010476\n",
      "2021-10-08 05:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083344\n",
      "2021-10-08 05:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[134633]\n",
      "2021-10-08 05:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008167\n",
      "2021-10-08 05:14:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.903259\n",
      "2021-10-08 05:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.995709\n",
      "2021-10-08 05:14:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 181:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:14:32 | INFO | fairseq.trainer | begin training epoch 181\n",
      "epoch 181: 100%|9| 1289/1290 [03:21<00:00,  6.60it/s, loss=2.749, nll_loss=0.9212021-10-08 05:17:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001656\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061912\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042504\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106707\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001314\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062616\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041024\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105569\n",
      "2021-10-08 05:17:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 181 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.70it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.30it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.03it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 10.79it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 12.46it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 13.90it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.31it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.81it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.21it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 181 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:17:56 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 4.785 | nll_loss 3.158 | ppl 8.93 | wps 52677.6 | wpb 2691.4 | bsz 201.4 | num_updates 114801 | best_loss 4.595\n",
      "2021-10-08 05:17:56 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:17:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint181.pt (epoch 181 @ 114801 updates, score 4.785) (writing took 3.3320336709730327 seconds)\n",
      "2021-10-08 05:17:59 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)\n",
      "2021-10-08 05:17:59 | INFO | train | epoch 181 | loss 2.719 | nll_loss 0.883 | ppl 1.84 | wps 22800.1 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 114801 | lr 9.33313e-05 | gnorm 1.691 | loss_scale 4 | train_wall 196 | wall 0\n",
      "2021-10-08 05:17:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=182/shard_epoch=89\n",
      "2021-10-08 05:17:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=182/shard_epoch=90\n",
      "2021-10-08 05:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:17:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004928\n",
      "2021-10-08 05:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105857\n",
      "2021-10-08 05:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:17:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[2218]\n",
      "2021-10-08 05:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010548\n",
      "2021-10-08 05:17:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:18:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.171148\n",
      "2021-10-08 05:18:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.288603\n",
      "2021-10-08 05:18:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:18:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:18:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085216\n",
      "2021-10-08 05:18:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:18:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[2218]\n",
      "2021-10-08 05:18:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010297\n",
      "2021-10-08 05:18:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.175793\n",
      "2021-10-08 05:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.272260\n",
      "2021-10-08 05:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 182:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:18:02 | INFO | fairseq.trainer | begin training epoch 182\n",
      "epoch 182: 100%|9| 1289/1290 [03:20<00:00,  6.26it/s, loss=2.713, nll_loss=0.8812021-10-08 05:21:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:21:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:21:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001866\n",
      "2021-10-08 05:21:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062055\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040700\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105215\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001523\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062680\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040849\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105860\n",
      "2021-10-08 05:21:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 182 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.99it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.63it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.36it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.12it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.75it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.19it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.45it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.53it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.95it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.37it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "epoch 182 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:21:24 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 4.768 | nll_loss 3.138 | ppl 8.8 | wps 52471.5 | wpb 2691.4 | bsz 201.4 | num_updates 116091 | best_loss 4.595\n",
      "2021-10-08 05:21:24 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:21:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint182.pt (epoch 182 @ 116091 updates, score 4.768) (writing took 3.3603426729678176 seconds)\n",
      "2021-10-08 05:21:28 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)\n",
      "2021-10-08 05:21:28 | INFO | train | epoch 182 | loss 2.716 | nll_loss 0.88 | ppl 1.84 | wps 22847.1 | ups 6.18 | wpb 3695.9 | bsz 249.8 | num_updates 116091 | lr 9.28113e-05 | gnorm 1.692 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 05:21:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=183/shard_epoch=90\n",
      "2021-10-08 05:21:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=183/shard_epoch=91\n",
      "2021-10-08 05:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:21:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005608\n",
      "2021-10-08 05:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107558\n",
      "2021-10-08 05:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:28 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[160033]\n",
      "2021-10-08 05:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010374\n",
      "2021-10-08 05:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.866081\n",
      "2021-10-08 05:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.984939\n",
      "2021-10-08 05:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086087\n",
      "2021-10-08 05:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[160033]\n",
      "2021-10-08 05:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009964\n",
      "2021-10-08 05:21:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.862652\n",
      "2021-10-08 05:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.959589\n",
      "2021-10-08 05:21:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 183:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:21:30 | INFO | fairseq.trainer | begin training epoch 183\n",
      "epoch 183: 100%|9| 1289/1290 [03:18<00:00,  6.58it/s, loss=2.747, nll_loss=0.9172021-10-08 05:24:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001828\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064561\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041040\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108262\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001500\n",
      "2021-10-08 05:24:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061811\n",
      "2021-10-08 05:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040803\n",
      "2021-10-08 05:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104727\n",
      "2021-10-08 05:24:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 183 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.92it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.53it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.26it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.02it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.68it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.12it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.26it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.12it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.44it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "epoch 183 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:24:50 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 4.77 | nll_loss 3.143 | ppl 8.84 | wps 51513.9 | wpb 2691.4 | bsz 201.4 | num_updates 117381 | best_loss 4.595\n",
      "2021-10-08 05:24:50 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:24:53 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint183.pt (epoch 183 @ 117381 updates, score 4.77) (writing took 3.312987772980705 seconds)\n",
      "2021-10-08 05:24:54 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)\n",
      "2021-10-08 05:24:54 | INFO | train | epoch 183 | loss 2.715 | nll_loss 0.879 | ppl 1.84 | wps 23162.5 | ups 6.27 | wpb 3695.9 | bsz 249.8 | num_updates 117381 | lr 9.22999e-05 | gnorm 1.692 | loss_scale 8 | train_wall 193 | wall 0\n",
      "2021-10-08 05:24:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=184/shard_epoch=91\n",
      "2021-10-08 05:24:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=184/shard_epoch=92\n",
      "2021-10-08 05:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:24:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004992\n",
      "2021-10-08 05:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.101714\n",
      "2021-10-08 05:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:54 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[230754]\n",
      "2021-10-08 05:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.013525\n",
      "2021-10-08 05:24:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.102135\n",
      "2021-10-08 05:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.218398\n",
      "2021-10-08 05:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089006\n",
      "2021-10-08 05:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:55 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[230754]\n",
      "2021-10-08 05:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010406\n",
      "2021-10-08 05:24:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.887337\n",
      "2021-10-08 05:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.987800\n",
      "2021-10-08 05:24:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 184:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:24:56 | INFO | fairseq.trainer | begin training epoch 184\n",
      "epoch 184: 100%|9| 1289/1290 [03:18<00:00,  6.50it/s, loss=2.735, nll_loss=0.9012021-10-08 05:28:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001664\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064048\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041927\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108278\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001485\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062910\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041157\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106236\n",
      "2021-10-08 05:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 184 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.59it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.18it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  8.89it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 10.65it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.32it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 13.82it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.06it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.75it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.29it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.62it/s]\u001b[A\n",
      "epoch 184 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:28:17 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 4.776 | nll_loss 3.149 | ppl 8.87 | wps 52565.3 | wpb 2691.4 | bsz 201.4 | num_updates 118671 | best_loss 4.595\n",
      "2021-10-08 05:28:17 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:28:20 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint184.pt (epoch 184 @ 118671 updates, score 4.776) (writing took 3.3959134710021317 seconds)\n",
      "2021-10-08 05:28:20 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)\n",
      "2021-10-08 05:28:20 | INFO | train | epoch 184 | loss 2.713 | nll_loss 0.876 | ppl 1.84 | wps 23071.9 | ups 6.24 | wpb 3695.9 | bsz 249.8 | num_updates 118671 | lr 9.17968e-05 | gnorm 1.693 | loss_scale 8 | train_wall 193 | wall 0\n",
      "2021-10-08 05:28:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=185/shard_epoch=92\n",
      "2021-10-08 05:28:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=185/shard_epoch=93\n",
      "2021-10-08 05:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:28:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005452\n",
      "2021-10-08 05:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.102328\n",
      "2021-10-08 05:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[182025]\n",
      "2021-10-08 05:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009764\n",
      "2021-10-08 05:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.896788\n",
      "2021-10-08 05:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.009850\n",
      "2021-10-08 05:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.082328\n",
      "2021-10-08 05:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:21 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[182025]\n",
      "2021-10-08 05:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008301\n",
      "2021-10-08 05:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.891760\n",
      "2021-10-08 05:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.983350\n",
      "2021-10-08 05:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 185:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:28:22 | INFO | fairseq.trainer | begin training epoch 185\n",
      "epoch 185: 100%|9| 1289/1290 [03:21<00:00,  6.24it/s, loss=2.759, nll_loss=0.9282021-10-08 05:31:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001748\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065092\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049165\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116937\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002033\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064996\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043765\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111725\n",
      "2021-10-08 05:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 185 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:05,  5.29it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  11%|7      | 3/28 [00:00<00:03,  6.79it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  18%|#2     | 5/28 [00:00<00:02,  8.46it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.17it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.88it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.42it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.77it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  61%|###6  | 17/28 [00:01<00:00, 16.61it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.95it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.40it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "epoch 185 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:31:46 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 4.791 | nll_loss 3.164 | ppl 8.96 | wps 51273.8 | wpb 2691.4 | bsz 201.4 | num_updates 119961 | best_loss 4.595\n",
      "2021-10-08 05:31:46 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:31:49 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint185.pt (epoch 185 @ 119961 updates, score 4.791) (writing took 3.371663354046177 seconds)\n",
      "2021-10-08 05:31:49 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)\n",
      "2021-10-08 05:31:49 | INFO | train | epoch 185 | loss 2.711 | nll_loss 0.874 | ppl 1.83 | wps 22795.7 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 119961 | lr 9.13019e-05 | gnorm 1.705 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 05:31:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=186/shard_epoch=93\n",
      "2021-10-08 05:31:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=186/shard_epoch=94\n",
      "2021-10-08 05:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:31:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004915\n",
      "2021-10-08 05:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105963\n",
      "2021-10-08 05:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[45890]\n",
      "2021-10-08 05:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010052\n",
      "2021-10-08 05:31:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.917193\n",
      "2021-10-08 05:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.034187\n",
      "2021-10-08 05:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083272\n",
      "2021-10-08 05:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:50 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[45890]\n",
      "2021-10-08 05:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008409\n",
      "2021-10-08 05:31:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.909276\n",
      "2021-10-08 05:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.001923\n",
      "2021-10-08 05:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 186:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:31:51 | INFO | fairseq.trainer | begin training epoch 186\n",
      "epoch 186: 100%|9| 1289/1290 [03:21<00:00,  6.45it/s, loss=2.732, nll_loss=0.8972021-10-08 05:35:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001704\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062551\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042331\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107333\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001773\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061469\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040794\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104850\n",
      "2021-10-08 05:35:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 186 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:05,  5.25it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  6.78it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  8.46it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.19it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.89it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.81it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.91it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.85it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.08it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.41it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 186 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.36it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:35:14 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 4.765 | nll_loss 3.133 | ppl 8.77 | wps 52116.5 | wpb 2691.4 | bsz 201.4 | num_updates 121251 | best_loss 4.595\n",
      "2021-10-08 05:35:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:35:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint186.pt (epoch 186 @ 121251 updates, score 4.765) (writing took 3.31333931395784 seconds)\n",
      "2021-10-08 05:35:18 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)\n",
      "2021-10-08 05:35:18 | INFO | train | epoch 186 | loss 2.708 | nll_loss 0.871 | ppl 1.83 | wps 22874.2 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 121251 | lr 9.08149e-05 | gnorm 1.707 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 05:35:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=187/shard_epoch=94\n",
      "2021-10-08 05:35:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=187/shard_epoch=95\n",
      "2021-10-08 05:35:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:35:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004637\n",
      "2021-10-08 05:35:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.102343\n",
      "2021-10-08 05:35:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[172507]\n",
      "2021-10-08 05:35:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008149\n",
      "2021-10-08 05:35:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.926710\n",
      "2021-10-08 05:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.038138\n",
      "2021-10-08 05:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086986\n",
      "2021-10-08 05:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[172507]\n",
      "2021-10-08 05:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009594\n",
      "2021-10-08 05:35:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:35:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.899008\n",
      "2021-10-08 05:35:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.996505\n",
      "2021-10-08 05:35:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 187:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:35:20 | INFO | fairseq.trainer | begin training epoch 187\n",
      "epoch 187: 100%|9| 1289/1290 [03:20<00:00,  6.28it/s, loss=2.77, nll_loss=0.937,2021-10-08 05:38:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001895\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069827\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053175\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125780\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001876\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064917\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042550\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110269\n",
      "2021-10-08 05:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 187 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.76it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.36it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.04it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.72it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.37it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.81it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.99it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.92it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.14it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.31it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.50it/s]\u001b[A\n",
      "epoch 187 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:38:43 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 4.78 | nll_loss 3.153 | ppl 8.9 | wps 50962.4 | wpb 2691.4 | bsz 201.4 | num_updates 122541 | best_loss 4.595\n",
      "2021-10-08 05:38:43 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:38:46 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint187.pt (epoch 187 @ 122541 updates, score 4.78) (writing took 3.486667339981068 seconds)\n",
      "2021-10-08 05:38:46 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)\n",
      "2021-10-08 05:38:46 | INFO | train | epoch 187 | loss 2.707 | nll_loss 0.87 | ppl 1.83 | wps 22870.7 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 122541 | lr 9.03357e-05 | gnorm 1.711 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 05:38:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=188/shard_epoch=95\n",
      "2021-10-08 05:38:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=188/shard_epoch=96\n",
      "2021-10-08 05:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:38:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004746\n",
      "2021-10-08 05:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113859\n",
      "2021-10-08 05:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:46 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[109368]\n",
      "2021-10-08 05:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010805\n",
      "2021-10-08 05:38:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.014695\n",
      "2021-10-08 05:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.140337\n",
      "2021-10-08 05:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089023\n",
      "2021-10-08 05:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[109368]\n",
      "2021-10-08 05:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009874\n",
      "2021-10-08 05:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.939442\n",
      "2021-10-08 05:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.039267\n",
      "2021-10-08 05:38:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 188:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:38:48 | INFO | fairseq.trainer | begin training epoch 188\n",
      "epoch 188: 100%|9| 1289/1290 [03:23<00:00,  6.32it/s, loss=2.772, nll_loss=0.9392021-10-08 05:42:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:42:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:42:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001985\n",
      "2021-10-08 05:42:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062459\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042241\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107641\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002095\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063435\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040894\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107325\n",
      "2021-10-08 05:42:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 188 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.61it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  11%|7      | 3/28 [00:00<00:03,  7.13it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  18%|#2     | 5/28 [00:00<00:02,  8.82it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.57it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.23it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.71it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 11.13it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  54%|###2  | 15/28 [00:01<00:01, 12.65it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  61%|###6  | 17/28 [00:01<00:00, 14.10it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 15.76it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 16.72it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 17.39it/s]\u001b[A\n",
      "epoch 188 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 17.83it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:42:15 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 4.785 | nll_loss 3.155 | ppl 8.91 | wps 45610.9 | wpb 2691.4 | bsz 201.4 | num_updates 123831 | best_loss 4.595\n",
      "2021-10-08 05:42:15 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:42:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint188.pt (epoch 188 @ 123831 updates, score 4.785) (writing took 3.4637306060176343 seconds)\n",
      "2021-10-08 05:42:18 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)\n",
      "2021-10-08 05:42:18 | INFO | train | epoch 188 | loss 2.705 | nll_loss 0.867 | ppl 1.82 | wps 22509.7 | ups 6.09 | wpb 3695.9 | bsz 249.8 | num_updates 123831 | lr 8.98639e-05 | gnorm 1.718 | loss_scale 8 | train_wall 197 | wall 0\n",
      "2021-10-08 05:42:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=189/shard_epoch=96\n",
      "2021-10-08 05:42:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=189/shard_epoch=97\n",
      "2021-10-08 05:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:42:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004941\n",
      "2021-10-08 05:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112976\n",
      "2021-10-08 05:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[107078]\n",
      "2021-10-08 05:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010568\n",
      "2021-10-08 05:42:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.955216\n",
      "2021-10-08 05:42:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.079926\n",
      "2021-10-08 05:42:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:42:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091856\n",
      "2021-10-08 05:42:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[107078]\n",
      "2021-10-08 05:42:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010974\n",
      "2021-10-08 05:42:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:42:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.948652\n",
      "2021-10-08 05:42:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.052441\n",
      "2021-10-08 05:42:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 189:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:42:20 | INFO | fairseq.trainer | begin training epoch 189\n",
      "epoch 189: 100%|9| 1289/1290 [03:22<00:00,  6.39it/s, loss=2.722, nll_loss=0.8872021-10-08 05:45:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001640\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065530\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047242\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115431\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001507\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062758\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041224\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106367\n",
      "2021-10-08 05:45:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 189 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.08it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.72it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.42it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.08it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.67it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.13it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.30it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.10it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.35it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.58it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 189 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.23it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:45:45 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 4.777 | nll_loss 3.15 | ppl 8.88 | wps 51252.4 | wpb 2691.4 | bsz 201.4 | num_updates 125121 | best_loss 4.595\n",
      "2021-10-08 05:45:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:45:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint189.pt (epoch 189 @ 125121 updates, score 4.777) (writing took 3.453108126006555 seconds)\n",
      "2021-10-08 05:45:48 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)\n",
      "2021-10-08 05:45:48 | INFO | train | epoch 189 | loss 2.702 | nll_loss 0.863 | ppl 1.82 | wps 22655.6 | ups 6.13 | wpb 3695.9 | bsz 249.8 | num_updates 125121 | lr 8.93995e-05 | gnorm 1.718 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 05:45:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=190/shard_epoch=97\n",
      "2021-10-08 05:45:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=190/shard_epoch=98\n",
      "2021-10-08 05:45:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:45:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005094\n",
      "2021-10-08 05:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112837\n",
      "2021-10-08 05:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[248801]\n",
      "2021-10-08 05:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010470\n",
      "2021-10-08 05:45:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.914172\n",
      "2021-10-08 05:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.038431\n",
      "2021-10-08 05:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086288\n",
      "2021-10-08 05:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:50 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[248801]\n",
      "2021-10-08 05:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010133\n",
      "2021-10-08 05:45:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.918804\n",
      "2021-10-08 05:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.016207\n",
      "2021-10-08 05:45:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 190:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:45:51 | INFO | fairseq.trainer | begin training epoch 190\n",
      "epoch 190: 100%|9| 1289/1290 [03:22<00:00,  6.30it/s, loss=2.734, nll_loss=0.8992021-10-08 05:49:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002350\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071708\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053029\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.128061\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001719\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070982\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053476\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.127209\n",
      "2021-10-08 05:49:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 190 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:05,  5.20it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  6.70it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.07it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.75it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.27it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.56it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.67it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.92it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.22it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.40it/s]\u001b[A\n",
      "epoch 190 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:49:15 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 4.782 | nll_loss 3.154 | ppl 8.9 | wps 50952.1 | wpb 2691.4 | bsz 201.4 | num_updates 126411 | best_loss 4.595\n",
      "2021-10-08 05:49:15 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:49:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint190.pt (epoch 190 @ 126411 updates, score 4.782) (writing took 3.5117673089844175 seconds)\n",
      "2021-10-08 05:49:18 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)\n",
      "2021-10-08 05:49:18 | INFO | train | epoch 190 | loss 2.703 | nll_loss 0.865 | ppl 1.82 | wps 22710.2 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 126411 | lr 8.89421e-05 | gnorm 1.727 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 05:49:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=191/shard_epoch=98\n",
      "2021-10-08 05:49:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=191/shard_epoch=99\n",
      "2021-10-08 05:49:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:49:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005428\n",
      "2021-10-08 05:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112167\n",
      "2021-10-08 05:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[175604]\n",
      "2021-10-08 05:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010846\n",
      "2021-10-08 05:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.900843\n",
      "2021-10-08 05:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.024844\n",
      "2021-10-08 05:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:49:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088936\n",
      "2021-10-08 05:49:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[175604]\n",
      "2021-10-08 05:49:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009247\n",
      "2021-10-08 05:49:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.929221\n",
      "2021-10-08 05:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.028443\n",
      "2021-10-08 05:49:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 191:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:49:21 | INFO | fairseq.trainer | begin training epoch 191\n",
      "epoch 191: 100%|9| 1289/1290 [03:22<00:00,  6.39it/s, loss=2.746, nll_loss=0.9122021-10-08 05:52:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001759\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066113\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042903\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111728\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001680\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062338\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045105\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110031\n",
      "2021-10-08 05:52:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 191 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.57it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  11%|7      | 3/28 [00:00<00:03,  7.08it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  18%|#2     | 5/28 [00:00<00:02,  8.78it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.53it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.22it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.97it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  61%|###6  | 17/28 [00:01<00:00, 16.72it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.92it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.42it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.69it/s]\u001b[A\n",
      "epoch 191 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.74it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:52:45 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 4.778 | nll_loss 3.151 | ppl 8.88 | wps 50979.8 | wpb 2691.4 | bsz 201.4 | num_updates 127701 | best_loss 4.595\n",
      "2021-10-08 05:52:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:52:49 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint191.pt (epoch 191 @ 127701 updates, score 4.778) (writing took 3.57125910196919 seconds)\n",
      "2021-10-08 05:52:49 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)\n",
      "2021-10-08 05:52:49 | INFO | train | epoch 191 | loss 2.699 | nll_loss 0.86 | ppl 1.82 | wps 22634 | ups 6.12 | wpb 3695.9 | bsz 249.8 | num_updates 127701 | lr 8.84918e-05 | gnorm 1.728 | loss_scale 8 | train_wall 196 | wall 0\n",
      "2021-10-08 05:52:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=192/shard_epoch=99\n",
      "2021-10-08 05:52:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=192/shard_epoch=100\n",
      "2021-10-08 05:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:52:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005118\n",
      "2021-10-08 05:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.119019\n",
      "2021-10-08 05:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[23117]\n",
      "2021-10-08 05:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011576\n",
      "2021-10-08 05:52:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.922267\n",
      "2021-10-08 05:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.053867\n",
      "2021-10-08 05:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.095874\n",
      "2021-10-08 05:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:50 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[23117]\n",
      "2021-10-08 05:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011737\n",
      "2021-10-08 05:52:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:52:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.974823\n",
      "2021-10-08 05:52:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.083489\n",
      "2021-10-08 05:52:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 192:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:52:51 | INFO | fairseq.trainer | begin training epoch 192\n",
      "epoch 192: 100%|9| 1289/1290 [03:23<00:00,  6.34it/s, loss=2.679, nll_loss=0.8422021-10-08 05:56:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001696\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064450\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040891\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107888\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001812\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062989\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041238\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107041\n",
      "2021-10-08 05:56:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 192 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.68it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.25it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  8.94it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.65it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.26it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.89it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.90it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.79it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.02it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.28it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.57it/s]\u001b[A\n",
      "epoch 192 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:56:17 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 4.787 | nll_loss 3.159 | ppl 8.93 | wps 51052.2 | wpb 2691.4 | bsz 201.4 | num_updates 128991 | best_loss 4.595\n",
      "2021-10-08 05:56:17 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:56:20 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint192.pt (epoch 192 @ 128991 updates, score 4.787) (writing took 3.514990090043284 seconds)\n",
      "2021-10-08 05:56:20 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)\n",
      "2021-10-08 05:56:20 | INFO | train | epoch 192 | loss 2.698 | nll_loss 0.859 | ppl 1.81 | wps 22558.6 | ups 6.1 | wpb 3695.9 | bsz 249.8 | num_updates 128991 | lr 8.80482e-05 | gnorm 1.732 | loss_scale 8 | train_wall 197 | wall 0\n",
      "2021-10-08 05:56:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=193/shard_epoch=100\n",
      "2021-10-08 05:56:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=193/shard_epoch=101\n",
      "2021-10-08 05:56:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:56:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004668\n",
      "2021-10-08 05:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112135\n",
      "2021-10-08 05:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:21 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[47962]\n",
      "2021-10-08 05:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010026\n",
      "2021-10-08 05:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.947649\n",
      "2021-10-08 05:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.070766\n",
      "2021-10-08 05:56:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090670\n",
      "2021-10-08 05:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:22 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[47962]\n",
      "2021-10-08 05:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009133\n",
      "2021-10-08 05:56:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.936151\n",
      "2021-10-08 05:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.036947\n",
      "2021-10-08 05:56:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 193:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:56:23 | INFO | fairseq.trainer | begin training epoch 193\n",
      "epoch 193: 100%|9| 1289/1290 [03:24<00:00,  6.37it/s, loss=2.727, nll_loss=0.8932021-10-08 05:59:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001843\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064244\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042946\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109988\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001679\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063145\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041477\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107202\n",
      "2021-10-08 05:59:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 193 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.51it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.05it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  8.73it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.46it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.12it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.83it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.76it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.09it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.38it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.61it/s]\u001b[A\n",
      "epoch 193 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 05:59:49 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 4.787 | nll_loss 3.16 | ppl 8.94 | wps 51057.9 | wpb 2691.4 | bsz 201.4 | num_updates 130281 | best_loss 4.595\n",
      "2021-10-08 05:59:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 05:59:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint193.pt (epoch 193 @ 130281 updates, score 4.787) (writing took 3.46284856996499 seconds)\n",
      "2021-10-08 05:59:52 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)\n",
      "2021-10-08 05:59:52 | INFO | train | epoch 193 | loss 2.696 | nll_loss 0.857 | ppl 1.81 | wps 22488.9 | ups 6.08 | wpb 3695.9 | bsz 249.8 | num_updates 130281 | lr 8.76112e-05 | gnorm 1.734 | loss_scale 8 | train_wall 198 | wall 0\n",
      "2021-10-08 05:59:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=194/shard_epoch=101\n",
      "2021-10-08 05:59:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=194/shard_epoch=102\n",
      "2021-10-08 05:59:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:59:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005912\n",
      "2021-10-08 05:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114471\n",
      "2021-10-08 05:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:53 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[65084]\n",
      "2021-10-08 05:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011122\n",
      "2021-10-08 05:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.975823\n",
      "2021-10-08 05:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.102450\n",
      "2021-10-08 05:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 05:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088924\n",
      "2021-10-08 05:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:54 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[65084]\n",
      "2021-10-08 05:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010720\n",
      "2021-10-08 05:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 05:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.926943\n",
      "2021-10-08 05:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.027569\n",
      "2021-10-08 05:59:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 194:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 05:59:55 | INFO | fairseq.trainer | begin training epoch 194\n",
      "epoch 194: 100%|9| 1289/1290 [03:21<00:00,  6.41it/s, loss=2.742, nll_loss=0.91,2021-10-08 06:03:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001681\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063392\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045260\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111272\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001747\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065559\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041287\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109607\n",
      "2021-10-08 06:03:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 194 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:05,  5.02it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  6.49it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  8.12it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:02,  9.85it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.47it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.04it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.32it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.43it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.40it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.77it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.11it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.40it/s]\u001b[A\n",
      "epoch 194 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 06:03:19 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 4.779 | nll_loss 3.153 | ppl 8.9 | wps 51085.5 | wpb 2691.4 | bsz 201.4 | num_updates 131571 | best_loss 4.595\n",
      "2021-10-08 06:03:19 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 06:03:22 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint194.pt (epoch 194 @ 131571 updates, score 4.779) (writing took 3.632946430006996 seconds)\n",
      "2021-10-08 06:03:22 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)\n",
      "2021-10-08 06:03:22 | INFO | train | epoch 194 | loss 2.695 | nll_loss 0.856 | ppl 1.81 | wps 22721.4 | ups 6.15 | wpb 3695.9 | bsz 249.8 | num_updates 131571 | lr 8.71806e-05 | gnorm 1.75 | loss_scale 8 | train_wall 195 | wall 0\n",
      "2021-10-08 06:03:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=195/shard_epoch=102\n",
      "2021-10-08 06:03:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=195/shard_epoch=103\n",
      "2021-10-08 06:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 06:03:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005152\n",
      "2021-10-08 06:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110576\n",
      "2021-10-08 06:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:22 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[278962]\n",
      "2021-10-08 06:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009610\n",
      "2021-10-08 06:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.962175\n",
      "2021-10-08 06:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.083322\n",
      "2021-10-08 06:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 06:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089183\n",
      "2021-10-08 06:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:23 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[278962]\n",
      "2021-10-08 06:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009391\n",
      "2021-10-08 06:03:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 06:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.926388\n",
      "2021-10-08 06:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.025916\n",
      "2021-10-08 06:03:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 195:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 06:03:24 | INFO | fairseq.trainer | begin training epoch 195\n",
      "epoch 195:   8%| | 103/1290 [00:16<03:10,  6.23it/s, loss=2.678, nll_loss=0.838,"
     ]
    }
   ],
   "source": [
    "! fairseq-train $BIN_DIR \\\n",
    "    --arch=transformer --share-all-embeddings \\\n",
    "    --task translation_multi_simple_epoch --lang-pairs quy-es,es-en \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --restore-file $MODEL_DIR/checkpoint_last.pt \\\n",
    "    --save-dir $MODEL_DIR/ \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --reset-optimizer \\\n",
    "    --encoder-langtok \"src\" \\\n",
    "    --decoder-langtok \\\n",
    "    --fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "! echo $MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "cd /storage/master-thesis/models/quy-es+es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm checkpoint108.pt\n",
    "! rm checkpoint109.pt\n",
    "! rm checkpoint_best.pt\n",
    "! rm checkpoint_last.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm dict.en.txt\n",
    "! rm dict.es.txt\n",
    "! rm dict.quy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5K\t/notebooks/CITATION.cff\n",
      "5.5K\t/notebooks/CODE_OF_CONDUCT.md\n",
      "15K\t/notebooks/CONTRIBUTING.md\n",
      "19K\t/notebooks/ISSUES.md\n",
      "12K\t/notebooks/LICENSE\n",
      "512\t/notebooks/MANIFEST.in\n",
      "3.5K\t/notebooks/Makefile\n",
      "41K\t/notebooks/README.md\n",
      "41K\t/notebooks/README_zh-hans.md\n",
      "42K\t/notebooks/README_zh-hant.md\n",
      "12K\t/notebooks/docker\n",
      "4.6M\t/notebooks/docs\n",
      "5.0M\t/notebooks/examples\n",
      "8.5K\t/notebooks/hubconf.py\n",
      "7.8G\t/notebooks/master-thesis\n",
      "1.5K\t/notebooks/model_cards\n",
      "9.5K\t/notebooks/notebooks\n",
      "512\t/notebooks/pyproject.toml\n",
      "64K\t/notebooks/scripts\n",
      "1.0K\t/notebooks/setup.cfg\n",
      "13K\t/notebooks/setup.py\n",
      "13M\t/notebooks/src\n",
      "731K\t/notebooks/templates\n",
      "4.5K\t/notebooks/test quy-es -> es-en model.ipynb\n",
      "6.8M\t/notebooks/tests\n",
      "147K\t/notebooks/train es-en model.ipynb\n",
      "158K\t/notebooks/train quy-es + es-en model.ipynb\n",
      "160K\t/notebooks/utils\n",
      "3.5K\t/notebooks/valohai.yaml\n",
      "7.9G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\t/notebooks/master-thesis/Untitled.ipynb\n",
      "362M\t/notebooks/master-thesis/corpora\n",
      "7.5G\t/notebooks/master-thesis/models\n",
      "7.8G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6G\t/notebooks/master-thesis/models/es-en\n",
      "802M\t/notebooks/master-thesis/models/quy-es\n",
      "5.2G\t/notebooks/master-thesis/models/quy-es+es-en\n",
      "7.5G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19M\t/apex\n",
      "5.0M\t/bin\n",
      "4.0K\t/boot\n",
      "24K\t/content\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! du -shc /*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CITATION.cff         \u001b[0m\u001b[01;34mdocker\u001b[0m/          setup.py\n",
      " CODE_OF_CONDUCT.md   \u001b[01;34mdocs\u001b[0m/            \u001b[01;34msrc\u001b[0m/\n",
      " CONTRIBUTING.md      \u001b[01;34mexamples\u001b[0m/        \u001b[01;34mtemplates\u001b[0m/\n",
      " ISSUES.md            hubconf.py      'test quy-es -> es-en model.ipynb'\n",
      " LICENSE              \u001b[01;34mmaster-thesis\u001b[0m/   \u001b[01;34mtests\u001b[0m/\n",
      " MANIFEST.in          \u001b[01;34mmodel_cards\u001b[0m/    'train es-en model.ipynb'\n",
      " Makefile             \u001b[01;34mnotebooks\u001b[0m/      'train quy-es + es-en model.ipynb'\n",
      " README.md            pyproject.toml   \u001b[01;34mutils\u001b[0m/\n",
      " README_zh-hans.md    \u001b[01;34mscripts\u001b[0m/         valohai.yaml\n",
      " README_zh-hant.md    setup.cfg\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
