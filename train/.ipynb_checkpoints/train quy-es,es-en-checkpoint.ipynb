{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUY_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZER_PATH=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZER_PATH = /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_path_es_en = \"/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/\"\n",
    "tokenized_path_quy_es = \"/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZED_PATH_ES_EN=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en\n",
      "env: TOKENIZED_PATH_QUY_ES=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZED_PATH_ES_EN = /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en\n",
    "%env TOKENIZED_PATH_QUY_ES = /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BIN_DIR=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "%env BIN_DIR = /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_DIR=/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_DIR = /storage/master-thesis/models/quy-es+es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locale: Cannot set LC_CTYPE to default locale: No such file or directory\n",
      "locale: Cannot set LC_MESSAGES to default locale: No such file or directory\n",
      "locale: Cannot set LC_ALL to default locale: No such file or directory\n",
      "LANG=en_US.UTF-8\n",
      "LANGUAGE=\n",
      "LC_CTYPE=\"en_US.UTF-8\"\n",
      "LC_NUMERIC=\"en_US.UTF-8\"\n",
      "LC_TIME=\"en_US.UTF-8\"\n",
      "LC_COLLATE=\"en_US.UTF-8\"\n",
      "LC_MONETARY=\"en_US.UTF-8\"\n",
      "LC_MESSAGES=\"en_US.UTF-8\"\n",
      "LC_PAPER=\"en_US.UTF-8\"\n",
      "LC_NAME=\"en_US.UTF-8\"\n",
      "LC_ADDRESS=\"en_US.UTF-8\"\n",
      "LC_TELEPHONE=\"en_US.UTF-8\"\n",
      "LC_MEASUREMENT=\"en_US.UTF-8\"\n",
      "LC_IDENTIFICATION=\"en_US.UTF-8\"\n",
      "LC_ALL=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "! locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: update-locale: not found\n",
      "env: LANG=en_US.UTF-8\n",
      "env: LC_CTYPE=en_US.UTF-8\n",
      "env: LC_ALL=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "! update-locale LANG=en_US.UTF-8 LANGUAGE=en.UTF-8\n",
    "\n",
    "%env LANG=en_US.UTF-8\n",
    "%env LC_CTYPE=en_US.UTF-8\n",
    "%env LC_ALL=en_US.UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /etc/rc.conf: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! cat /etc/rc.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.10.3 in /usr/local/lib/python3.6/dist-packages (0.10.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers==0.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (0.10.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.24)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.51.0)\n",
      "Requirement already satisfied: hydra-core in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.1.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2020.11.13)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (2.0.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.6.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
      "Requirement already satisfied: omegaconf==2.1.* in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (2.1.1)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (5.2.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (4.8)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.3.2)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.4)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.18.2)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq) (5.4.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tatoeba format to standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(EN_ES_CORPUS_DIR + \"original/valid.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"dev.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"dev.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "    en.close()\n",
    "    es.close()\n",
    "    \n",
    "with open(EN_ES_CORPUS_DIR + \"original/dev.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"train.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"train.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        if \"\\n\" in line[2] or \"\\n\" in line[3]:\n",
    "            continue\n",
    "        if len(line) != 4:\n",
    "            continue\n",
    "            \n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "        \n",
    "    en.close()\n",
    "    es.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "en_es_quy_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "en_es_quy_trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=30000, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "en_es_quy_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "en_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.en\" for split in [\"dev\", \"train\"]]\n",
    "es_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "quy_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/{split}.quy\" for split in [\"dev\", \"dict\", \"train\"]]\n",
    "es2_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/{split}.es\" for split in [\"dev\", \"dict\", \"train\"]]\n",
    "\n",
    "files = en_files + es_files + es2_files + quy_files\n",
    "en_es_quy_tokenizer.train(files= files, trainer=en_es_quy_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_files(tokenizer, files, extension, output_path):\n",
    "    for file in files:\n",
    "        print(f\"Reading file {file}\")\n",
    "        with open(file, encoding='utf8') as f:\n",
    "          lines = f.readlines()\n",
    "          tokenized_lines = tokenizer.encode_batch(lines)\n",
    "          tokenized_name = Path(file).stem\n",
    "          tokenized_name = output_path + tokenized_name + \".\" + extension\n",
    "          print(tokenized_name)\n",
    "          with open(tokenized_name, 'w', encoding='utf8') as wf:\n",
    "\n",
    "            wf.writelines([\" \".join(t.tokens) + \"\\n\" for t in tokenized_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_quy_tokenizer.save(tokenizer_path + \"quy-es-en-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.Tokenizer object at 0x28a09a0>\n"
     ]
    }
   ],
   "source": [
    "print(en_es_quy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'hold', 'this', 'truth', 'to', 'be', 'self', 'evid', '##ent', 'that', 'everyone', 'is', 'created', 'equal', '?']\n"
     ]
    }
   ],
   "source": [
    "tok = en_es_quy_tokenizer.encode(\"we hold this truth to be self evident that everyone is created equal?\")\n",
    "print(tok.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.en\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.en\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.quy\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dict.quy\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dict.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.quy\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dict.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dict.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.es\n"
     ]
    }
   ],
   "source": [
    "tokenize_files(en_es_quy_tokenizer, en_files, \"en\", tokenized_path_es_en)\n",
    "tokenize_files(en_es_quy_tokenizer, es_files, \"es\", tokenized_path_es_en)\n",
    "\n",
    "tokenize_files(en_es_quy_tokenizer, quy_files, \"quy\", tokenized_path_quy_es)\n",
    "tokenize_files(en_es_quy_tokenizer, es2_files, \"es\", tokenized_path_quy_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.en\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.quy.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.es.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.en.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "### Removing previous dict files\n",
    "! rm $BIN_DIR/dict.quy.txt\n",
    "! rm $BIN_DIR/dict.es.txt\n",
    "! rm $BIN_DIR/dict.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenating all training data\n",
    "! cat $TOKENIZED_PATH_QUY_ES/train.quy $TOKENIZED_PATH_QUY_ES/train.es $TOKENIZED_PATH_ES_EN/train.es $TOKENIZED_PATH_ES_EN/train.en > $BIN_DIR/train.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict.all.txt         test.quy-es.quy.idx     train.quy-es.quy.bin\n",
      "dict.en.txt          train.all               train.quy-es.quy.idx\n",
      "dict.es.txt          train.all-None.all.bin  valid.es-en.en.bin\n",
      "dict.quy.txt         train.all-None.all.idx  valid.es-en.en.idx\n",
      "preprocess.log       train.es-en.en.bin      valid.es-en.es.bin\n",
      "test.es-en.es.bin    train.es-en.en.idx      valid.es-en.es.idx\n",
      "test.es-en.es.idx    train.es-en.es.bin      valid.quy-es.es.bin\n",
      "test.quy-en.quy.bin  train.es-en.es.idx      valid.quy-es.es.idx\n",
      "test.quy-en.quy.idx  train.quy-es.es.bin     valid.quy-es.quy.bin\n",
      "test.quy-es.quy.bin  train.quy-es.es.idx     valid.quy-es.quy.idx\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:44:27 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='all', srcdict=None, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train', user_dir=None, validpref=None, workers=20)\n",
      "2021-10-07 17:44:34 | INFO | fairseq_cli.preprocess | [all] Dictionary: 28976 types\n",
      "2021-10-07 17:44:48 | INFO | fairseq_cli.preprocess | [all] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.all: 644610 sents, 8263690 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:44:48 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang all \\\n",
    "    --trainpref $BIN_DIR/train \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --workers 20 \\\n",
    "    --only-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:46:10 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='quy', srcdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', target_lang='es', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train', user_dir=None, validpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev', workers=20)\n",
      "2021-10-07 17:46:10 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 28976 types\n",
      "2021-10-07 17:46:13 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.quy: 125008 sents, 1922581 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:13 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 28976 types\n",
      "2021-10-07 17:46:14 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.quy: 996 sents, 14488 tokens, 0.456% replaced by <unk>\n",
      "2021-10-07 17:46:14 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:18 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.es: 125008 sents, 2452298 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:18 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:19 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.es: 996 sents, 15303 tokens, 0.242% replaced by <unk>\n",
      "2021-10-07 17:46:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang quy --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_QUY_ES/train --validpref $TOKENIZED_PATH_QUY_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing es dict from quy-es preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:46:28 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='es', srcdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train', user_dir=None, validpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev', workers=20)\n",
      "2021-10-07 17:46:28 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:33 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.es: 197297 sents, 1894674 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:33 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:34 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.es: 4643 sents, 51839 tokens, 0.0405% replaced by <unk>\n",
      "2021-10-07 17:46:34 | INFO | fairseq_cli.preprocess | [en] Dictionary: 28976 types\n",
      "2021-10-07 17:46:38 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.en: 197297 sents, 1994137 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:38 | INFO | fairseq_cli.preprocess | [en] Dictionary: 28976 types\n",
      "2021-10-07 17:46:39 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.en: 4643 sents, 54416 tokens, 0.0404% replaced by <unk>\n",
      "2021-10-07 17:46:39 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang es --target-lang en \\\n",
    "    --trainpref $TOKENIZED_PATH_ES_EN/train --validpref $TOKENIZED_PATH_ES_EN/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:46:56 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=2, label_smoothing=0.1, lang_dict=None, lang_pairs='quy-es,es-en', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='/storage/master-thesis/models/quy-es+es-en/checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='/storage/master-thesis/models/quy-es+es-en/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')\n",
      "2021-10-07 17:46:56 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['en', 'es', 'quy']\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 28979 types\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 28979 types\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | [quy] dictionary: 28979 types\n",
      "2021-10-07 17:46:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n",
      "2021-10-07 17:46:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:quy-es': 1, 'main:es-en': 1}\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 28978; tgt_langtok: 28977\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.quy-es.quy\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.quy-es.es\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin valid quy-es 996 examples\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 28977; tgt_langtok: 28976\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.es-en.es\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.es-en.en\n",
      "2021-10-07 17:46:56 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin valid es-en 4643 examples\n",
      "2021-10-07 17:46:57 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(28979, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(28979, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=28979, bias=False)\n",
      "  )\n",
      ")\n",
      "2021-10-07 17:46:57 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)\n",
      "2021-10-07 17:46:57 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
      "2021-10-07 17:46:57 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
      "2021-10-07 17:46:57 | INFO | fairseq_cli.train | num. model params: 58975744 (num. trained: 58975744)\n",
      "2021-10-07 17:47:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
      "2021-10-07 17:47:03 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2021-10-07 17:47:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-10-07 17:47:03 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 7.795 GB ; name = Quadro RTX 4000                         \n",
      "2021-10-07 17:47:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-10-07 17:47:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2021-10-07 17:47:03 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None\n",
      "2021-10-07 17:47:03 | INFO | fairseq.trainer | no existing checkpoint found /storage/master-thesis/models/quy-es+es-en/checkpoint_last.pt\n",
      "2021-10-07 17:47:03 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2021-10-07 17:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None\n",
      "2021-10-07 17:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:quy-es': 1, 'main:es-en': 1}\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 28978; tgt_langtok: 28977\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.data_utils | loaded 125008 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.quy-es.quy\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.data_utils | loaded 125008 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.quy-es.es\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin train quy-es 125008 examples\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 28977; tgt_langtok: 28976\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.es-en.es\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.es-en.en\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin train es-en 197297 examples\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:quy-es', 125008), ('main:es-en', 197297)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat\n",
      "2021-10-07 17:47:03 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 322305\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 322305; virtual dataset size 322305\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:quy-es': 125008, 'main:es-en': 197297}; raw total size: 322305\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:quy-es': 125008, 'main:es-en': 197297}; resampled total size: 322305\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.039957\n",
      "2021-10-07 17:47:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:47:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006209\n",
      "2021-10-07 17:47:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111191\n",
      "2021-10-07 17:47:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:47:04 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[176689]\n",
      "2021-10-07 17:47:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009340\n",
      "2021-10-07 17:47:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.930925\n",
      "2021-10-07 17:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.052421\n",
      "2021-10-07 17:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:47:05 | INFO | fairseq.optim.adam | using FusedAdam\n",
      "2021-10-07 17:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088894\n",
      "2021-10-07 17:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:47:05 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[176689]\n",
      "2021-10-07 17:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008550\n",
      "2021-10-07 17:47:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.926362\n",
      "2021-10-07 17:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.024771\n",
      "2021-10-07 17:47:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 001:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 17:47:06 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2021-10-07 17:47:06 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
      "epoch 001:   3%| | 38/1290 [00:05<03:06,  6.73it/s, loss_scale=64, train_wall=0,2021-10-07 17:47:12 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
      "epoch 001: 100%|9| 1289/1290 [03:16<00:00,  6.44it/s, loss=8.45, nll_loss=7.451,2021-10-07 17:50:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001960\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060397\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041072\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104429\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001560\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061648\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.039815\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103716\n",
      "2021-10-07 17:50:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.84it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  11%|7      | 3/28 [00:00<00:02,  8.45it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.21it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.91it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.45it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.69it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.50it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.98it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 17:50:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.164 | nll_loss 7.039 | ppl 131.54 | wps 51999.8 | wpb 2691.4 | bsz 201.4 | num_updates 1288\n",
      "2021-10-07 17:50:24 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 17:50:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint1.pt (epoch 1 @ 1288 updates, score 8.164) (writing took 4.528408034704626 seconds)\n",
      "2021-10-07 17:50:29 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2021-10-07 17:50:29 | INFO | train | epoch 001 | loss 9.603 | nll_loss 8.796 | ppl 444.4 | wps 23443.2 | ups 6.34 | wpb 3695.3 | bsz 249.6 | num_updates 1288 | lr 0.000161 | gnorm 2.054 | loss_scale 32 | train_wall 191 | wall 206\n",
      "2021-10-07 17:50:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1\n",
      "2021-10-07 17:50:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2\n",
      "2021-10-07 17:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:50:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004959\n",
      "2021-10-07 17:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109624\n",
      "2021-10-07 17:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[315437]\n",
      "2021-10-07 17:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009333\n",
      "2021-10-07 17:50:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.940759\n",
      "2021-10-07 17:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.060669\n",
      "2021-10-07 17:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087126\n",
      "2021-10-07 17:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[315437]\n",
      "2021-10-07 17:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008876\n",
      "2021-10-07 17:50:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.958272\n",
      "2021-10-07 17:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.055168\n",
      "2021-10-07 17:50:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 002:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 17:50:31 | INFO | fairseq.trainer | begin training epoch 2\n",
      "epoch 002: 100%|9| 1289/1290 [03:19<00:00,  6.31it/s, loss=7.071, nll_loss=5.8722021-10-07 17:53:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001642\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062183\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041094\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105794\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001661\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064246\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040453\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107322\n",
      "2021-10-07 17:53:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.19it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.95it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.73it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.43it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.86it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.01it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.35it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.75it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.97it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.06it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 17:53:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.098 | nll_loss 5.831 | ppl 56.94 | wps 52863.9 | wpb 2691.4 | bsz 201.4 | num_updates 2578 | best_loss 7.098\n",
      "2021-10-07 17:53:52 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 17:53:57 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint2.pt (epoch 2 @ 2578 updates, score 7.098) (writing took 4.651057702023536 seconds)\n",
      "2021-10-07 17:53:57 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2021-10-07 17:53:57 | INFO | train | epoch 002 | loss 7.478 | nll_loss 6.34 | ppl 81.02 | wps 22928.4 | ups 6.2 | wpb 3695.9 | bsz 249.8 | num_updates 2578 | lr 0.00032225 | gnorm 1.295 | loss_scale 32 | train_wall 194 | wall 414\n",
      "2021-10-07 17:53:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2\n",
      "2021-10-07 17:53:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3\n",
      "2021-10-07 17:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:53:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.008621\n",
      "2021-10-07 17:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.130902\n",
      "2021-10-07 17:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:57 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[282912]\n",
      "2021-10-07 17:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009298\n",
      "2021-10-07 17:53:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.010214\n",
      "2021-10-07 17:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.151421\n",
      "2021-10-07 17:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087526\n",
      "2021-10-07 17:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[282912]\n",
      "2021-10-07 17:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008672\n",
      "2021-10-07 17:53:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.012644\n",
      "2021-10-07 17:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.109828\n",
      "2021-10-07 17:53:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 003:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 17:53:59 | INFO | fairseq.trainer | begin training epoch 3\n",
      "epoch 003: 100%|9| 1289/1290 [03:19<00:00,  6.52it/s, loss=6.625, nll_loss=5.3562021-10-07 17:57:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001704\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070191\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052985\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125769\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001800\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066169\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041896\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110772\n",
      "2021-10-07 17:57:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.13it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.76it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.47it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.23it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.88it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.31it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.49it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.34it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.65it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.93it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.12it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.53it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 17:57:21 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.568 | nll_loss 5.199 | ppl 36.74 | wps 52715.8 | wpb 2691.4 | bsz 201.4 | num_updates 3868 | best_loss 6.568\n",
      "2021-10-07 17:57:21 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 17:57:25 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint3.pt (epoch 3 @ 3868 updates, score 6.568) (writing took 4.767033326905221 seconds)\n",
      "2021-10-07 17:57:25 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2021-10-07 17:57:25 | INFO | train | epoch 003 | loss 6.634 | nll_loss 5.372 | ppl 41.41 | wps 22867.2 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 3868 | lr 0.0004835 | gnorm 1.118 | loss_scale 32 | train_wall 193 | wall 622\n",
      "2021-10-07 17:57:25 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3\n",
      "2021-10-07 17:57:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4\n",
      "2021-10-07 17:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:57:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005218\n",
      "2021-10-07 17:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.119943\n",
      "2021-10-07 17:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:26 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[250240]\n",
      "2021-10-07 17:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011547\n",
      "2021-10-07 17:57:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.023940\n",
      "2021-10-07 17:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.156425\n",
      "2021-10-07 17:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 17:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.095794\n",
      "2021-10-07 17:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:27 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[250240]\n",
      "2021-10-07 17:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.015695\n",
      "2021-10-07 17:57:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 17:57:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.034588\n",
      "2021-10-07 17:57:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.147028\n",
      "2021-10-07 17:57:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 004:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 17:57:28 | INFO | fairseq.trainer | begin training epoch 4\n",
      "epoch 004: 100%|9| 1289/1290 [03:19<00:00,  6.45it/s, loss=5.791, nll_loss=4.4032021-10-07 18:00:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001734\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063605\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042168\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108435\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001736\n",
      "2021-10-07 18:00:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062669\n",
      "2021-10-07 18:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041948\n",
      "2021-10-07 18:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107201\n",
      "2021-10-07 18:00:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.17it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.94it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.36it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 15.16it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 16.05it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.83it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.61it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.74it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.90it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.05it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.69it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:00:49 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.124 | nll_loss 4.681 | ppl 25.65 | wps 52520.3 | wpb 2691.4 | bsz 201.4 | num_updates 5158 | best_loss 6.124\n",
      "2021-10-07 18:00:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:00:54 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint4.pt (epoch 4 @ 5158 updates, score 6.124) (writing took 4.642116737086326 seconds)\n",
      "2021-10-07 18:00:54 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2021-10-07 18:00:54 | INFO | train | epoch 004 | loss 6.068 | nll_loss 4.72 | ppl 26.36 | wps 22890.4 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 5158 | lr 0.000440311 | gnorm 1.013 | loss_scale 32 | train_wall 193 | wall 830\n",
      "2021-10-07 18:00:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4\n",
      "2021-10-07 18:00:54 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5\n",
      "2021-10-07 18:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:00:54 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005123\n",
      "2021-10-07 18:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116101\n",
      "2021-10-07 18:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:54 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[90416]\n",
      "2021-10-07 18:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009479\n",
      "2021-10-07 18:00:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.004425\n",
      "2021-10-07 18:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.131063\n",
      "2021-10-07 18:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088768\n",
      "2021-10-07 18:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:55 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[90416]\n",
      "2021-10-07 18:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009403\n",
      "2021-10-07 18:00:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.983350\n",
      "2021-10-07 18:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.082500\n",
      "2021-10-07 18:00:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 005:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:00:56 | INFO | fairseq.trainer | begin training epoch 5\n",
      "epoch 005:   0%|                               | 3/1290 [00:00<04:41,  4.57it/s]2021-10-07 18:00:57 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 005: 100%|9| 1289/1290 [03:19<00:00,  6.29it/s, loss=5.607, nll_loss=4.1882021-10-07 18:04:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001981\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062961\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042047\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108043\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001484\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062286\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043315\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107944\n",
      "2021-10-07 18:04:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.79it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.48it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.22it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.93it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.71it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.50it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.63it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.98it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.52it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:04:18 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.859 | nll_loss 4.368 | ppl 20.65 | wps 52107.3 | wpb 2691.4 | bsz 201.4 | num_updates 6447 | best_loss 5.859\n",
      "2021-10-07 18:04:18 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:04:22 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint5.pt (epoch 5 @ 6447 updates, score 5.859) (writing took 4.711885653901845 seconds)\n",
      "2021-10-07 18:04:23 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2021-10-07 18:04:23 | INFO | train | epoch 005 | loss 5.633 | nll_loss 4.219 | ppl 18.62 | wps 22829.8 | ups 6.18 | wpb 3697.1 | bsz 249.5 | num_updates 6447 | lr 0.000393841 | gnorm 0.957 | loss_scale 16 | train_wall 194 | wall 1039\n",
      "2021-10-07 18:04:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5\n",
      "2021-10-07 18:04:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6\n",
      "2021-10-07 18:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:04:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005074\n",
      "2021-10-07 18:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.102597\n",
      "2021-10-07 18:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:23 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[115279]\n",
      "2021-10-07 18:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010910\n",
      "2021-10-07 18:04:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.972687\n",
      "2021-10-07 18:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.087150\n",
      "2021-10-07 18:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085341\n",
      "2021-10-07 18:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:24 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[115279]\n",
      "2021-10-07 18:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008899\n",
      "2021-10-07 18:04:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.027561\n",
      "2021-10-07 18:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.122724\n",
      "2021-10-07 18:04:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 006:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:04:25 | INFO | fairseq.trainer | begin training epoch 6\n",
      "epoch 006: 100%|9| 1289/1290 [03:20<00:00,  6.94it/s, loss=5.184, nll_loss=3.7022021-10-07 18:07:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001774\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063506\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042074\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108123\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001856\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061237\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046919\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111121\n",
      "2021-10-07 18:07:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.84it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.56it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.20it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.86it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.78it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.68it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.48it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.99it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:07:47 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.657 | nll_loss 4.144 | ppl 17.68 | wps 51959 | wpb 2691.4 | bsz 201.4 | num_updates 7737 | best_loss 5.657\n",
      "2021-10-07 18:07:47 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:07:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint6.pt (epoch 6 @ 7737 updates, score 5.657) (writing took 4.728259512688965 seconds)\n",
      "2021-10-07 18:07:52 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2021-10-07 18:07:52 | INFO | train | epoch 006 | loss 5.338 | nll_loss 3.878 | ppl 14.71 | wps 22800.5 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 7737 | lr 0.000359512 | gnorm 0.951 | loss_scale 16 | train_wall 194 | wall 1248\n",
      "2021-10-07 18:07:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6\n",
      "2021-10-07 18:07:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7\n",
      "2021-10-07 18:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:07:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005553\n",
      "2021-10-07 18:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113976\n",
      "2021-10-07 18:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[227703]\n",
      "2021-10-07 18:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009120\n",
      "2021-10-07 18:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.021752\n",
      "2021-10-07 18:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.145815\n",
      "2021-10-07 18:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088422\n",
      "2021-10-07 18:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:53 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[227703]\n",
      "2021-10-07 18:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010528\n",
      "2021-10-07 18:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:07:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.000959\n",
      "2021-10-07 18:07:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.101122\n",
      "2021-10-07 18:07:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 007:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:07:54 | INFO | fairseq.trainer | begin training epoch 7\n",
      "epoch 007: 100%|9| 1289/1290 [03:19<00:00,  6.40it/s, loss=5.235, nll_loss=3.7562021-10-07 18:11:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001739\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064157\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041974\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108710\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001649\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061296\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041068\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104937\n",
      "2021-10-07 18:11:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.33it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.99it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.74it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.47it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.06it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.46it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.47it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.26it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.44it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.60it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.76it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.09it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:11:16 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.576 | nll_loss 4.044 | ppl 16.49 | wps 51979.1 | wpb 2691.4 | bsz 201.4 | num_updates 9027 | best_loss 5.576\n",
      "2021-10-07 18:11:16 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:11:20 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint7.pt (epoch 7 @ 9027 updates, score 5.576) (writing took 4.567965703085065 seconds)\n",
      "2021-10-07 18:11:20 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2021-10-07 18:11:20 | INFO | train | epoch 007 | loss 5.125 | nll_loss 3.632 | ppl 12.39 | wps 22857.7 | ups 6.18 | wpb 3695.9 | bsz 249.8 | num_updates 9027 | lr 0.000332834 | gnorm 0.948 | loss_scale 16 | train_wall 194 | wall 1457\n",
      "2021-10-07 18:11:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7\n",
      "2021-10-07 18:11:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8\n",
      "2021-10-07 18:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:11:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005037\n",
      "2021-10-07 18:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108030\n",
      "2021-10-07 18:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[241793]\n",
      "2021-10-07 18:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011133\n",
      "2021-10-07 18:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.018886\n",
      "2021-10-07 18:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.139144\n",
      "2021-10-07 18:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090656\n",
      "2021-10-07 18:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:22 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[241793]\n",
      "2021-10-07 18:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009268\n",
      "2021-10-07 18:11:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:11:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.105190\n",
      "2021-10-07 18:11:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.206217\n",
      "2021-10-07 18:11:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 008:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:11:23 | INFO | fairseq.trainer | begin training epoch 8\n",
      "epoch 008: 100%|9| 1289/1290 [03:19<00:00,  6.32it/s, loss=5.009, nll_loss=3.4942021-10-07 18:14:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001870\n",
      "2021-10-07 18:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062923\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040923\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106548\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001962\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060306\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.039792\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102885\n",
      "2021-10-07 18:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.63it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.34it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.08it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.75it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.30it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.63it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 11.49it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 13.06it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 14.50it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 16.20it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 16.93it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 17.57it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:14:44 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.491 | nll_loss 3.941 | ppl 15.36 | wps 46801.8 | wpb 2691.4 | bsz 201.4 | num_updates 10317 | best_loss 5.491\n",
      "2021-10-07 18:14:44 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:14:49 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint8.pt (epoch 8 @ 10317 updates, score 5.491) (writing took 4.724430920090526 seconds)\n",
      "2021-10-07 18:14:49 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2021-10-07 18:14:49 | INFO | train | epoch 008 | loss 4.959 | nll_loss 3.439 | ppl 10.85 | wps 22812.1 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 10317 | lr 0.000311332 | gnorm 0.948 | loss_scale 16 | train_wall 194 | wall 1666\n",
      "2021-10-07 18:14:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8\n",
      "2021-10-07 18:14:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9\n",
      "2021-10-07 18:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:14:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005701\n",
      "2021-10-07 18:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113992\n",
      "2021-10-07 18:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[322092]\n",
      "2021-10-07 18:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009900\n",
      "2021-10-07 18:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.051619\n",
      "2021-10-07 18:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.176546\n",
      "2021-10-07 18:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091610\n",
      "2021-10-07 18:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:51 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[322092]\n",
      "2021-10-07 18:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010646\n",
      "2021-10-07 18:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.984082\n",
      "2021-10-07 18:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.087326\n",
      "2021-10-07 18:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 009:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:14:52 | INFO | fairseq.trainer | begin training epoch 9\n",
      "epoch 009: 100%|9| 1289/1290 [03:19<00:00,  6.55it/s, loss=4.821, nll_loss=3.2792021-10-07 18:18:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001672\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061634\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040607\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104836\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001481\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061031\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040076\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103428\n",
      "2021-10-07 18:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.91it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.65it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.42it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.11it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.70it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.99it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.80it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.61it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.00it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.15it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:18:13 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.482 | nll_loss 3.937 | ppl 15.31 | wps 52651.4 | wpb 2691.4 | bsz 201.4 | num_updates 11607 | best_loss 5.482\n",
      "2021-10-07 18:18:13 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:18:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint9.pt (epoch 9 @ 11607 updates, score 5.482) (writing took 4.670377546921372 seconds)\n",
      "2021-10-07 18:18:18 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2021-10-07 18:18:18 | INFO | train | epoch 009 | loss 4.825 | nll_loss 3.283 | ppl 9.73 | wps 22876.4 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 11607 | lr 0.000293522 | gnorm 0.953 | loss_scale 16 | train_wall 193 | wall 1874\n",
      "2021-10-07 18:18:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9\n",
      "2021-10-07 18:18:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10\n",
      "2021-10-07 18:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:18:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004970\n",
      "2021-10-07 18:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114632\n",
      "2021-10-07 18:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[144740]\n",
      "2021-10-07 18:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009701\n",
      "2021-10-07 18:18:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.980069\n",
      "2021-10-07 18:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.105413\n",
      "2021-10-07 18:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.093708\n",
      "2021-10-07 18:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[144740]\n",
      "2021-10-07 18:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009592\n",
      "2021-10-07 18:18:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.993196\n",
      "2021-10-07 18:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.097607\n",
      "2021-10-07 18:18:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 010:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:18:20 | INFO | fairseq.trainer | begin training epoch 10\n",
      "epoch 010: 100%|9| 1289/1290 [03:19<00:00,  6.44it/s, loss=4.705, nll_loss=3.1452021-10-07 18:21:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001937\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061729\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042223\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106881\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001728\n",
      "2021-10-07 18:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061341\n",
      "2021-10-07 18:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042529\n",
      "2021-10-07 18:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106673\n",
      "2021-10-07 18:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.82it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.53it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.29it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.01it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.57it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.90it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.64it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.38it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.45it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.75it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.30it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:21:41 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.396 | nll_loss 3.827 | ppl 14.19 | wps 51828.2 | wpb 2691.4 | bsz 201.4 | num_updates 12897 | best_loss 5.396\n",
      "2021-10-07 18:21:41 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:21:46 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint10.pt (epoch 10 @ 12897 updates, score 5.396) (writing took 4.604029756970704 seconds)\n",
      "2021-10-07 18:21:46 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2021-10-07 18:21:46 | INFO | train | epoch 010 | loss 4.712 | nll_loss 3.153 | ppl 8.89 | wps 22909 | ups 6.2 | wpb 3695.9 | bsz 249.8 | num_updates 12897 | lr 0.000278455 | gnorm 0.959 | loss_scale 16 | train_wall 193 | wall 2082\n",
      "2021-10-07 18:21:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10\n",
      "2021-10-07 18:21:46 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11\n",
      "2021-10-07 18:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:21:46 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004956\n",
      "2021-10-07 18:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107998\n",
      "2021-10-07 18:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:46 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[143301]\n",
      "2021-10-07 18:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008866\n",
      "2021-10-07 18:21:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.170648\n",
      "2021-10-07 18:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.288513\n",
      "2021-10-07 18:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.093820\n",
      "2021-10-07 18:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:47 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[143301]\n",
      "2021-10-07 18:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010968\n",
      "2021-10-07 18:21:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.008890\n",
      "2021-10-07 18:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.114701\n",
      "2021-10-07 18:21:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 011:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:21:48 | INFO | fairseq.trainer | begin training epoch 11\n",
      "epoch 011: 100%|9| 1289/1290 [03:19<00:00,  6.94it/s, loss=4.666, nll_loss=3.0992021-10-07 18:25:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001770\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065059\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042534\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110210\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001718\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061942\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040939\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105445\n",
      "2021-10-07 18:25:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 011 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.06it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.80it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.53it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.25it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.74it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 15.00it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.81it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.65it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.45it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.85it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.93it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:25:10 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.238 | nll_loss 3.653 | ppl 12.58 | wps 52084.6 | wpb 2691.4 | bsz 201.4 | num_updates 14187 | best_loss 5.238\n",
      "2021-10-07 18:25:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:25:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint11.pt (epoch 11 @ 14187 updates, score 5.238) (writing took 4.700080268085003 seconds)\n",
      "2021-10-07 18:25:14 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2021-10-07 18:25:14 | INFO | train | epoch 011 | loss 4.592 | nll_loss 3.014 | ppl 8.08 | wps 22843.7 | ups 6.18 | wpb 3695.9 | bsz 249.8 | num_updates 14187 | lr 0.000265494 | gnorm 0.981 | loss_scale 16 | train_wall 194 | wall 2291\n",
      "2021-10-07 18:25:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11\n",
      "2021-10-07 18:25:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12\n",
      "2021-10-07 18:25:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:25:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004908\n",
      "2021-10-07 18:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111575\n",
      "2021-10-07 18:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:15 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[310675]\n",
      "2021-10-07 18:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008900\n",
      "2021-10-07 18:25:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.953370\n",
      "2021-10-07 18:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.074920\n",
      "2021-10-07 18:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.100040\n",
      "2021-10-07 18:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:16 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[310675]\n",
      "2021-10-07 18:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010484\n",
      "2021-10-07 18:25:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:25:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.114138\n",
      "2021-10-07 18:25:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.225672\n",
      "2021-10-07 18:25:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 012:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:25:17 | INFO | fairseq.trainer | begin training epoch 12\n",
      "epoch 012: 100%|9| 1289/1290 [03:19<00:00,  6.47it/s, loss=4.417, nll_loss=2.8132021-10-07 18:28:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001677\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062078\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042923\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107561\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001583\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063865\n",
      "2021-10-07 18:28:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040555\n",
      "2021-10-07 18:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106942\n",
      "2021-10-07 18:28:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 012 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.48it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.18it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.94it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.66it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.56it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.68it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.02it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.52it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.82it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:28:38 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.201 | nll_loss 3.596 | ppl 12.09 | wps 52659.6 | wpb 2691.4 | bsz 201.4 | num_updates 15477 | best_loss 5.201\n",
      "2021-10-07 18:28:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:28:43 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint12.pt (epoch 12 @ 15477 updates, score 5.201) (writing took 4.692378852982074 seconds)\n",
      "2021-10-07 18:28:43 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2021-10-07 18:28:43 | INFO | train | epoch 012 | loss 4.493 | nll_loss 2.899 | ppl 7.46 | wps 22859.2 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 15477 | lr 0.000254189 | gnorm 0.989 | loss_scale 16 | train_wall 193 | wall 2500\n",
      "2021-10-07 18:28:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12\n",
      "2021-10-07 18:28:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13\n",
      "2021-10-07 18:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:28:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004938\n",
      "2021-10-07 18:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114453\n",
      "2021-10-07 18:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:43 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[254515]\n",
      "2021-10-07 18:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009825\n",
      "2021-10-07 18:28:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.129758\n",
      "2021-10-07 18:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.255125\n",
      "2021-10-07 18:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090327\n",
      "2021-10-07 18:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:44 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[254515]\n",
      "2021-10-07 18:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008543\n",
      "2021-10-07 18:28:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.076917\n",
      "2021-10-07 18:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.176920\n",
      "2021-10-07 18:28:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 013:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:28:46 | INFO | fairseq.trainer | begin training epoch 13\n",
      "epoch 013: 100%|9| 1289/1290 [03:19<00:00,  6.32it/s, loss=4.397, nll_loss=2.79,2021-10-07 18:32:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001724\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059667\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040289\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102513\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001710\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060868\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040140\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103463\n",
      "2021-10-07 18:32:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 013 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.52it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.22it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.96it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.65it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.22it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.59it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.32it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.47it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.45it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:32:07 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.112 | nll_loss 3.487 | ppl 11.22 | wps 52167.3 | wpb 2691.4 | bsz 201.4 | num_updates 16767 | best_loss 5.112\n",
      "2021-10-07 18:32:07 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:32:12 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint13.pt (epoch 13 @ 16767 updates, score 5.112) (writing took 5.3021323047578335 seconds)\n",
      "2021-10-07 18:32:12 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2021-10-07 18:32:12 | INFO | train | epoch 013 | loss 4.412 | nll_loss 2.805 | ppl 6.99 | wps 22788.6 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 16767 | lr 0.000244215 | gnorm 1.004 | loss_scale 16 | train_wall 193 | wall 2709\n",
      "2021-10-07 18:32:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13\n",
      "2021-10-07 18:32:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14\n",
      "2021-10-07 18:32:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:32:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005024\n",
      "2021-10-07 18:32:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112434\n",
      "2021-10-07 18:32:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:12 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[159220]\n",
      "2021-10-07 18:32:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009029\n",
      "2021-10-07 18:32:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.983590\n",
      "2021-10-07 18:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.106041\n",
      "2021-10-07 18:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.091244\n",
      "2021-10-07 18:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:13 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[159220]\n",
      "2021-10-07 18:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008649\n",
      "2021-10-07 18:32:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:32:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.006067\n",
      "2021-10-07 18:32:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.107026\n",
      "2021-10-07 18:32:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 014:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:32:15 | INFO | fairseq.trainer | begin training epoch 14\n",
      "epoch 014: 100%|9| 1289/1290 [03:19<00:00,  6.44it/s, loss=4.36, nll_loss=2.746,2021-10-07 18:35:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001768\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066103\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043208\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112075\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001850\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062080\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042121\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106889\n",
      "2021-10-07 18:35:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 014 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.56it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.25it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.00it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.72it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.30it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.67it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.54it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.27it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.51it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.69it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:35:36 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.098 | nll_loss 3.472 | ppl 11.09 | wps 52164.2 | wpb 2691.4 | bsz 201.4 | num_updates 18057 | best_loss 5.098\n",
      "2021-10-07 18:35:36 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:35:41 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint14.pt (epoch 14 @ 18057 updates, score 5.098) (writing took 4.827297416049987 seconds)\n",
      "2021-10-07 18:35:41 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2021-10-07 18:35:41 | INFO | train | epoch 014 | loss 4.339 | nll_loss 2.722 | ppl 6.6 | wps 22853.7 | ups 6.18 | wpb 3695.9 | bsz 249.8 | num_updates 18057 | lr 0.00023533 | gnorm 1.004 | loss_scale 16 | train_wall 194 | wall 2917\n",
      "2021-10-07 18:35:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14\n",
      "2021-10-07 18:35:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15\n",
      "2021-10-07 18:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:35:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005436\n",
      "2021-10-07 18:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109285\n",
      "2021-10-07 18:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:41 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[136366]\n",
      "2021-10-07 18:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008830\n",
      "2021-10-07 18:35:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.055209\n",
      "2021-10-07 18:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.174262\n",
      "2021-10-07 18:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085278\n",
      "2021-10-07 18:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:42 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[136366]\n",
      "2021-10-07 18:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008077\n",
      "2021-10-07 18:35:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.009895\n",
      "2021-10-07 18:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.104138\n",
      "2021-10-07 18:35:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 015:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:35:43 | INFO | fairseq.trainer | begin training epoch 15\n",
      "epoch 015: 100%|9| 1289/1290 [03:19<00:00,  6.52it/s, loss=4.257, nll_loss=2.6292021-10-07 18:39:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001927\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061719\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042788\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107444\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001744\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060423\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046211\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109331\n",
      "2021-10-07 18:39:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 015 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.07it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.72it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.46it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.21it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.84it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.25it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.41it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.38it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.73it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.29it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.61it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.77it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:39:05 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.028 | nll_loss 3.394 | ppl 10.51 | wps 52583.6 | wpb 2691.4 | bsz 201.4 | num_updates 19347 | best_loss 5.028\n",
      "2021-10-07 18:39:05 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:39:10 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint15.pt (epoch 15 @ 19347 updates, score 5.028) (writing took 5.093563203699887 seconds)\n",
      "2021-10-07 18:39:10 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2021-10-07 18:39:10 | INFO | train | epoch 015 | loss 4.273 | nll_loss 2.645 | ppl 6.26 | wps 22810.1 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 19347 | lr 0.000227349 | gnorm 1.009 | loss_scale 16 | train_wall 194 | wall 3126\n",
      "2021-10-07 18:39:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15\n",
      "2021-10-07 18:39:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16\n",
      "2021-10-07 18:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:39:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004845\n",
      "2021-10-07 18:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109739\n",
      "2021-10-07 18:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:10 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[256488]\n",
      "2021-10-07 18:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009669\n",
      "2021-10-07 18:39:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.979273\n",
      "2021-10-07 18:39:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.099765\n",
      "2021-10-07 18:39:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:39:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092288\n",
      "2021-10-07 18:39:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:11 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[256488]\n",
      "2021-10-07 18:39:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010103\n",
      "2021-10-07 18:39:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.075946\n",
      "2021-10-07 18:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.179310\n",
      "2021-10-07 18:39:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 016:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:39:12 | INFO | fairseq.trainer | begin training epoch 16\n",
      "epoch 016:  69%|6| 888/1290 [02:17<01:02,  6.42it/s, loss=4.281, nll_loss=2.655,2021-10-07 18:41:30 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n",
      "epoch 016: 100%|9| 1289/1290 [03:19<00:00,  6.71it/s, loss=4.25, nll_loss=2.619,2021-10-07 18:42:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002175\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066923\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042656\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.113620\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002165\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069932\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042040\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.115204\n",
      "2021-10-07 18:42:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 016 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.48it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.17it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.87it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.53it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 10.06it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 11.74it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 13.24it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 14.56it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 15.75it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.25it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 17.85it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.35it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.99it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:42:34 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.006 | nll_loss 3.361 | ppl 10.28 | wps 46842.9 | wpb 2691.4 | bsz 201.4 | num_updates 20636 | best_loss 5.006\n",
      "2021-10-07 18:42:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:42:39 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint16.pt (epoch 16 @ 20636 updates, score 5.006) (writing took 4.632788356859237 seconds)\n",
      "2021-10-07 18:42:39 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2021-10-07 18:42:39 | INFO | train | epoch 016 | loss 4.213 | nll_loss 2.578 | ppl 5.97 | wps 22826.6 | ups 6.17 | wpb 3697.1 | bsz 249.5 | num_updates 20636 | lr 0.000220134 | gnorm 1.011 | loss_scale 8 | train_wall 194 | wall 3335\n",
      "2021-10-07 18:42:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16\n",
      "2021-10-07 18:42:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17\n",
      "2021-10-07 18:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:42:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004922\n",
      "2021-10-07 18:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117039\n",
      "2021-10-07 18:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:39 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[19119]\n",
      "2021-10-07 18:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011726\n",
      "2021-10-07 18:42:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.942156\n",
      "2021-10-07 18:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.071883\n",
      "2021-10-07 18:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.093479\n",
      "2021-10-07 18:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:40 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[19119]\n",
      "2021-10-07 18:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010377\n",
      "2021-10-07 18:42:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.000701\n",
      "2021-10-07 18:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.105550\n",
      "2021-10-07 18:42:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 017:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:42:41 | INFO | fairseq.trainer | begin training epoch 17\n",
      "epoch 017: 100%|9| 1289/1290 [03:19<00:00,  6.34it/s, loss=4.277, nll_loss=2.65,2021-10-07 18:46:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:46:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:46:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001649\n",
      "2021-10-07 18:46:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062714\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040839\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106129\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001634\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059586\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041768\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103746\n",
      "2021-10-07 18:46:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 017 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.77it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.48it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.24it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.92it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.80it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.78it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.19it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.61it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:46:02 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.96 | nll_loss 3.313 | ppl 9.94 | wps 52449 | wpb 2691.4 | bsz 201.4 | num_updates 21926 | best_loss 4.96\n",
      "2021-10-07 18:46:02 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:46:07 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint17.pt (epoch 17 @ 21926 updates, score 4.96) (writing took 4.595745521131903 seconds)\n",
      "2021-10-07 18:46:07 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2021-10-07 18:46:07 | INFO | train | epoch 017 | loss 4.157 | nll_loss 2.513 | ppl 5.71 | wps 22893.4 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 21926 | lr 0.00021356 | gnorm 1.032 | loss_scale 8 | train_wall 193 | wall 3544\n",
      "2021-10-07 18:46:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17\n",
      "2021-10-07 18:46:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18\n",
      "2021-10-07 18:46:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:46:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005007\n",
      "2021-10-07 18:46:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.118732\n",
      "2021-10-07 18:46:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:07 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[274051]\n",
      "2021-10-07 18:46:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011512\n",
      "2021-10-07 18:46:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.012938\n",
      "2021-10-07 18:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.144238\n",
      "2021-10-07 18:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085612\n",
      "2021-10-07 18:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:08 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[274051]\n",
      "2021-10-07 18:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009034\n",
      "2021-10-07 18:46:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.974031\n",
      "2021-10-07 18:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.069671\n",
      "2021-10-07 18:46:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 018:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:46:09 | INFO | fairseq.trainer | begin training epoch 18\n",
      "epoch 018: 100%|9| 1289/1290 [03:19<00:00,  6.50it/s, loss=4.216, nll_loss=2.5812021-10-07 18:49:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001996\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061127\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040150\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104125\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001573\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059840\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040177\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102388\n",
      "2021-10-07 18:49:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 018 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.06it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.82it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.62it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.33it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.73it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.96it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.80it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.08it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.43it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.70it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:49:30 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.919 | nll_loss 3.269 | ppl 9.64 | wps 52297.2 | wpb 2691.4 | bsz 201.4 | num_updates 23216 | best_loss 4.919\n",
      "2021-10-07 18:49:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:49:35 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint18.pt (epoch 18 @ 23216 updates, score 4.919) (writing took 4.526904806960374 seconds)\n",
      "2021-10-07 18:49:35 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2021-10-07 18:49:35 | INFO | train | epoch 018 | loss 4.097 | nll_loss 2.445 | ppl 5.45 | wps 22925 | ups 6.2 | wpb 3695.9 | bsz 249.8 | num_updates 23216 | lr 0.000207542 | gnorm 1.026 | loss_scale 8 | train_wall 193 | wall 3751\n",
      "2021-10-07 18:49:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18\n",
      "2021-10-07 18:49:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19\n",
      "2021-10-07 18:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:49:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004763\n",
      "2021-10-07 18:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110561\n",
      "2021-10-07 18:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:35 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[297749]\n",
      "2021-10-07 18:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009542\n",
      "2021-10-07 18:49:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.020857\n",
      "2021-10-07 18:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.141951\n",
      "2021-10-07 18:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086117\n",
      "2021-10-07 18:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:36 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[297749]\n",
      "2021-10-07 18:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007956\n",
      "2021-10-07 18:49:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.130477\n",
      "2021-10-07 18:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.225489\n",
      "2021-10-07 18:49:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 019:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:49:37 | INFO | fairseq.trainer | begin training epoch 19\n",
      "epoch 019: 100%|9| 1289/1290 [03:19<00:00,  6.56it/s, loss=4.074, nll_loss=2.4212021-10-07 18:52:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001789\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063021\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041232\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107081\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001865\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068639\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045614\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117031\n",
      "2021-10-07 18:52:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 019 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.48it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.18it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.93it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.64it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.20it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.54it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.35it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.76it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.87it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.33it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:52:58 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.89 | nll_loss 3.239 | ppl 9.44 | wps 51979.2 | wpb 2691.4 | bsz 201.4 | num_updates 24506 | best_loss 4.89\n",
      "2021-10-07 18:52:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:53:03 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint19.pt (epoch 19 @ 24506 updates, score 4.89) (writing took 4.620127499103546 seconds)\n",
      "2021-10-07 18:53:03 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2021-10-07 18:53:03 | INFO | train | epoch 019 | loss 4.046 | nll_loss 2.388 | ppl 5.23 | wps 22896.2 | ups 6.2 | wpb 3695.9 | bsz 249.8 | num_updates 24506 | lr 0.000202006 | gnorm 1.031 | loss_scale 8 | train_wall 193 | wall 3960\n",
      "2021-10-07 18:53:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19\n",
      "2021-10-07 18:53:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20\n",
      "2021-10-07 18:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:53:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006316\n",
      "2021-10-07 18:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113022\n",
      "2021-10-07 18:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:53:03 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[141443]\n",
      "2021-10-07 18:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010338\n",
      "2021-10-07 18:53:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.049531\n",
      "2021-10-07 18:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.173825\n",
      "2021-10-07 18:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088259\n",
      "2021-10-07 18:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:53:04 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[141443]\n",
      "2021-10-07 18:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009446\n",
      "2021-10-07 18:53:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.998569\n",
      "2021-10-07 18:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.097237\n",
      "2021-10-07 18:53:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 020:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:53:05 | INFO | fairseq.trainer | begin training epoch 20\n",
      "epoch 020: 100%|9| 1289/1290 [03:19<00:00,  6.65it/s, loss=3.927, nll_loss=2.2552021-10-07 18:56:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001722\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060675\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042266\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105438\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001705\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063908\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041484\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108022\n",
      "2021-10-07 18:56:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 020 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.35it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.03it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.72it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.38it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.90it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.33it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.47it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.25it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.46it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.70it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:56:27 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.865 | nll_loss 3.214 | ppl 9.28 | wps 51837.2 | wpb 2691.4 | bsz 201.4 | num_updates 25796 | best_loss 4.865\n",
      "2021-10-07 18:56:27 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 18:56:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint20.pt (epoch 20 @ 25796 updates, score 4.865) (writing took 4.6751000001095235 seconds)\n",
      "2021-10-07 18:56:31 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2021-10-07 18:56:31 | INFO | train | epoch 020 | loss 3.999 | nll_loss 2.334 | ppl 5.04 | wps 22881.2 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 25796 | lr 0.00019689 | gnorm 1.04 | loss_scale 8 | train_wall 193 | wall 4168\n",
      "2021-10-07 18:56:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20\n",
      "2021-10-07 18:56:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21\n",
      "2021-10-07 18:56:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:56:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005320\n",
      "2021-10-07 18:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.113456\n",
      "2021-10-07 18:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[11089]\n",
      "2021-10-07 18:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010078\n",
      "2021-10-07 18:56:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.998009\n",
      "2021-10-07 18:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.122583\n",
      "2021-10-07 18:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092680\n",
      "2021-10-07 18:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:33 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[11089]\n",
      "2021-10-07 18:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010532\n",
      "2021-10-07 18:56:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:56:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.034925\n",
      "2021-10-07 18:56:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.139217\n",
      "2021-10-07 18:56:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 021:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 18:56:34 | INFO | fairseq.trainer | begin training epoch 21\n",
      "epoch 021: 100%|9| 1289/1290 [03:19<00:00,  6.40it/s, loss=3.972, nll_loss=2.3062021-10-07 18:59:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001673\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064672\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045396\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112754\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001822\n",
      "2021-10-07 18:59:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060657\n",
      "2021-10-07 18:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 18:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041601\n",
      "2021-10-07 18:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105038\n",
      "2021-10-07 18:59:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 021 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.65it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.36it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.12it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.83it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.39it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.74it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.81it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.73it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.56it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.05it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.19it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.81it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 18:59:55 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.861 | nll_loss 3.195 | ppl 9.16 | wps 52772.5 | wpb 2691.4 | bsz 201.4 | num_updates 27086 | best_loss 4.861\n",
      "2021-10-07 18:59:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:00:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint21.pt (epoch 21 @ 27086 updates, score 4.861) (writing took 4.68818313581869 seconds)\n",
      "2021-10-07 19:00:00 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
      "2021-10-07 19:00:00 | INFO | train | epoch 021 | loss 3.957 | nll_loss 2.287 | ppl 4.88 | wps 22879.7 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 27086 | lr 0.000192144 | gnorm 1.051 | loss_scale 8 | train_wall 193 | wall 4376\n",
      "2021-10-07 19:00:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21\n",
      "2021-10-07 19:00:00 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22\n",
      "2021-10-07 19:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:00:00 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005596\n",
      "2021-10-07 19:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117889\n",
      "2021-10-07 19:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:00:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[264166]\n",
      "2021-10-07 19:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011160\n",
      "2021-10-07 19:00:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.023073\n",
      "2021-10-07 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.153155\n",
      "2021-10-07 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088153\n",
      "2021-10-07 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:00:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[264166]\n",
      "2021-10-07 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009115\n",
      "2021-10-07 19:00:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.015127\n",
      "2021-10-07 19:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.113448\n",
      "2021-10-07 19:00:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 022:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:00:02 | INFO | fairseq.trainer | begin training epoch 22\n",
      "epoch 022: 100%|9| 1289/1290 [03:19<00:00,  6.27it/s, loss=3.974, nll_loss=2.3052021-10-07 19:03:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001820\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062737\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043645\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109220\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001603\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.070086\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041461\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114035\n",
      "2021-10-07 19:03:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 022 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.68it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  11%|7      | 3/28 [00:00<00:03,  8.33it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  18%|#2     | 5/28 [00:00<00:02, 10.05it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.71it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 13.30it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 14.57it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.24it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.39it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.74it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.77it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:03:24 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.809 | nll_loss 3.147 | ppl 8.86 | wps 51321.3 | wpb 2691.4 | bsz 201.4 | num_updates 28376 | best_loss 4.809\n",
      "2021-10-07 19:03:24 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:03:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint22.pt (epoch 22 @ 28376 updates, score 4.809) (writing took 4.597877118270844 seconds)\n",
      "2021-10-07 19:03:28 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
      "2021-10-07 19:03:28 | INFO | train | epoch 022 | loss 3.914 | nll_loss 2.238 | ppl 4.72 | wps 22866.7 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 28376 | lr 0.000187726 | gnorm 1.046 | loss_scale 8 | train_wall 193 | wall 4585\n",
      "2021-10-07 19:03:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22\n",
      "2021-10-07 19:03:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23\n",
      "2021-10-07 19:03:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:03:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006285\n",
      "2021-10-07 19:03:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.127830\n",
      "2021-10-07 19:03:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[184965]\n",
      "2021-10-07 19:03:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010862\n",
      "2021-10-07 19:03:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.004758\n",
      "2021-10-07 19:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.144604\n",
      "2021-10-07 19:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085083\n",
      "2021-10-07 19:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[184965]\n",
      "2021-10-07 19:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007976\n",
      "2021-10-07 19:03:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.032913\n",
      "2021-10-07 19:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.127015\n",
      "2021-10-07 19:03:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 023:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:03:31 | INFO | fairseq.trainer | begin training epoch 23\n",
      "epoch 023: 100%|9| 1289/1290 [03:19<00:00,  6.57it/s, loss=3.841, nll_loss=2.16,2021-10-07 19:06:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001731\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059914\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041208\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103760\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001801\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063406\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041109\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107240\n",
      "2021-10-07 19:06:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 023 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.36it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  11%|7      | 3/28 [00:00<00:03,  7.99it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  18%|#2     | 5/28 [00:00<00:02,  9.72it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.41it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 13.05it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 14.45it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.62it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.56it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.33it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.89it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.07it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.08it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:06:52 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.829 | nll_loss 3.167 | ppl 8.98 | wps 52048 | wpb 2691.4 | bsz 201.4 | num_updates 29666 | best_loss 4.809\n",
      "2021-10-07 19:06:52 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:06:55 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint23.pt (epoch 23 @ 29666 updates, score 4.829) (writing took 3.452956798952073 seconds)\n",
      "2021-10-07 19:06:56 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
      "2021-10-07 19:06:56 | INFO | train | epoch 023 | loss 3.878 | nll_loss 2.197 | ppl 4.58 | wps 23015 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 29666 | lr 0.000183599 | gnorm 1.056 | loss_scale 8 | train_wall 193 | wall 4792\n",
      "2021-10-07 19:06:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23\n",
      "2021-10-07 19:06:56 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24\n",
      "2021-10-07 19:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:06:56 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004641\n",
      "2021-10-07 19:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109793\n",
      "2021-10-07 19:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:56 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[49249]\n",
      "2021-10-07 19:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008074\n",
      "2021-10-07 19:06:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.096421\n",
      "2021-10-07 19:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.215257\n",
      "2021-10-07 19:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087277\n",
      "2021-10-07 19:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:57 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[49249]\n",
      "2021-10-07 19:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009400\n",
      "2021-10-07 19:06:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:06:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.921096\n",
      "2021-10-07 19:06:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.018699\n",
      "2021-10-07 19:06:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 024:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:06:58 | INFO | fairseq.trainer | begin training epoch 24\n",
      "epoch 024: 100%|9| 1289/1290 [03:19<00:00,  6.27it/s, loss=3.78, nll_loss=2.088,2021-10-07 19:10:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001947\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064486\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040997\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108358\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001808\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061905\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041372\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106029\n",
      "2021-10-07 19:10:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 024 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.95it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.58it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.28it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:02,  8.81it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 10.53it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 12.17it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 13.63it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 14.92it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 17.59it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.15it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.53it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:10:20 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.802 | nll_loss 3.146 | ppl 8.85 | wps 46887.7 | wpb 2691.4 | bsz 201.4 | num_updates 30956 | best_loss 4.802\n",
      "2021-10-07 19:10:20 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:10:24 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint24.pt (epoch 24 @ 30956 updates, score 4.802) (writing took 4.716984604019672 seconds)\n",
      "2021-10-07 19:10:24 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
      "2021-10-07 19:10:24 | INFO | train | epoch 024 | loss 3.845 | nll_loss 2.159 | ppl 4.46 | wps 22823 | ups 6.18 | wpb 3695.9 | bsz 249.8 | num_updates 30956 | lr 0.000179733 | gnorm 1.058 | loss_scale 8 | train_wall 194 | wall 5001\n",
      "2021-10-07 19:10:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24\n",
      "2021-10-07 19:10:24 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25\n",
      "2021-10-07 19:10:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:10:24 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006418\n",
      "2021-10-07 19:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.119467\n",
      "2021-10-07 19:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:25 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[35215]\n",
      "2021-10-07 19:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009440\n",
      "2021-10-07 19:10:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.019212\n",
      "2021-10-07 19:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.149168\n",
      "2021-10-07 19:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092822\n",
      "2021-10-07 19:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:26 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[35215]\n",
      "2021-10-07 19:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010987\n",
      "2021-10-07 19:10:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.078795\n",
      "2021-10-07 19:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.184674\n",
      "2021-10-07 19:10:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 025:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:10:27 | INFO | fairseq.trainer | begin training epoch 25\n",
      "epoch 025: 100%|9| 1289/1290 [03:19<00:00,  6.56it/s, loss=3.94, nll_loss=2.268,2021-10-07 19:13:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:13:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:13:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001586\n",
      "2021-10-07 19:13:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061063\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040988\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104290\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001444\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059784\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040021\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.101943\n",
      "2021-10-07 19:13:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 025 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.92it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.66it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.43it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.16it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.73it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 15.06it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.67it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.46it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.67it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.85it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.97it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:13:48 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.759 | nll_loss 3.092 | ppl 8.53 | wps 52323.6 | wpb 2691.4 | bsz 201.4 | num_updates 32246 | best_loss 4.759\n",
      "2021-10-07 19:13:48 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:13:53 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint25.pt (epoch 25 @ 32246 updates, score 4.759) (writing took 4.582192046102136 seconds)\n",
      "2021-10-07 19:13:53 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
      "2021-10-07 19:13:53 | INFO | train | epoch 025 | loss 3.811 | nll_loss 2.121 | ppl 4.35 | wps 22872.6 | ups 6.19 | wpb 3695.9 | bsz 249.8 | num_updates 32246 | lr 0.000176101 | gnorm 1.069 | loss_scale 8 | train_wall 193 | wall 5209\n",
      "2021-10-07 19:13:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25\n",
      "2021-10-07 19:13:53 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26\n",
      "2021-10-07 19:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:13:53 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004894\n",
      "2021-10-07 19:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.119894\n",
      "2021-10-07 19:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:53 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[279338]\n",
      "2021-10-07 19:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010012\n",
      "2021-10-07 19:13:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.957106\n",
      "2021-10-07 19:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.087997\n",
      "2021-10-07 19:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087408\n",
      "2021-10-07 19:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:54 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[279338]\n",
      "2021-10-07 19:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009213\n",
      "2021-10-07 19:13:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.029595\n",
      "2021-10-07 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.127141\n",
      "2021-10-07 19:13:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 026:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:13:55 | INFO | fairseq.trainer | begin training epoch 26\n",
      "epoch 026: 100%|9| 1289/1290 [03:22<00:00,  6.55it/s, loss=3.82, nll_loss=2.131,2021-10-07 19:17:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002191\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060734\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040014\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103886\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001672\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060631\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.039691\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102934\n",
      "2021-10-07 19:17:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 026 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.34it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.03it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.78it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.48it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.08it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.50it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.65it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.22it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.51it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.68it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.77it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:17:19 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.786 | nll_loss 3.125 | ppl 8.72 | wps 52121.6 | wpb 2691.4 | bsz 201.4 | num_updates 33536 | best_loss 4.759\n",
      "2021-10-07 19:17:19 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:17:23 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint26.pt (epoch 26 @ 33536 updates, score 4.786) (writing took 3.4555229512043297 seconds)\n",
      "2021-10-07 19:17:23 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
      "2021-10-07 19:17:23 | INFO | train | epoch 026 | loss 3.781 | nll_loss 2.086 | ppl 4.25 | wps 22689.3 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 33536 | lr 0.000172681 | gnorm 1.075 | loss_scale 8 | train_wall 196 | wall 5420\n",
      "2021-10-07 19:17:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26\n",
      "2021-10-07 19:17:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27\n",
      "2021-10-07 19:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:17:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004726\n",
      "2021-10-07 19:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108630\n",
      "2021-10-07 19:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:23 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[149946]\n",
      "2021-10-07 19:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008498\n",
      "2021-10-07 19:17:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.950731\n",
      "2021-10-07 19:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.068809\n",
      "2021-10-07 19:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090507\n",
      "2021-10-07 19:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:24 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[149946]\n",
      "2021-10-07 19:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011050\n",
      "2021-10-07 19:17:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.983647\n",
      "2021-10-07 19:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.086210\n",
      "2021-10-07 19:17:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 027:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:17:25 | INFO | fairseq.trainer | begin training epoch 27\n",
      "epoch 027: 100%|9| 1289/1290 [03:21<00:00,  6.55it/s, loss=3.795, nll_loss=2.1022021-10-07 19:20:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001880\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060613\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041260\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104702\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001908\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060087\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040110\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102968\n",
      "2021-10-07 19:20:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 027 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.78it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.50it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.25it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.99it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.57it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.87it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.84it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.63it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.86it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.07it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.12it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:20:48 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.79 | nll_loss 3.135 | ppl 8.79 | wps 52767.2 | wpb 2691.4 | bsz 201.4 | num_updates 34826 | best_loss 4.759\n",
      "2021-10-07 19:20:48 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:20:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint27.pt (epoch 27 @ 34826 updates, score 4.79) (writing took 3.5562071041204035 seconds)\n",
      "2021-10-07 19:20:52 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
      "2021-10-07 19:20:52 | INFO | train | epoch 027 | loss 3.752 | nll_loss 2.053 | ppl 4.15 | wps 22810 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 34826 | lr 0.000169453 | gnorm 1.08 | loss_scale 8 | train_wall 195 | wall 5629\n",
      "2021-10-07 19:20:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27\n",
      "2021-10-07 19:20:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28\n",
      "2021-10-07 19:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:20:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005802\n",
      "2021-10-07 19:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.116102\n",
      "2021-10-07 19:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[281063]\n",
      "2021-10-07 19:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011380\n",
      "2021-10-07 19:20:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.956694\n",
      "2021-10-07 19:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.085306\n",
      "2021-10-07 19:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.092093\n",
      "2021-10-07 19:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:53 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[281063]\n",
      "2021-10-07 19:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010836\n",
      "2021-10-07 19:20:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.964143\n",
      "2021-10-07 19:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.068073\n",
      "2021-10-07 19:20:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 028:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:20:54 | INFO | fairseq.trainer | begin training epoch 28\n",
      "epoch 028: 100%|9| 1289/1290 [03:20<00:00,  6.34it/s, loss=3.831, nll_loss=2.1422021-10-07 19:24:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001760\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.069388\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043631\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116573\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001741\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060520\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041662\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104794\n",
      "2021-10-07 19:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 028 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.50it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.15it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.87it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.57it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.13it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.48it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.45it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.30it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.45it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.65it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:24:17 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.749 | nll_loss 3.088 | ppl 8.5 | wps 51588.1 | wpb 2691.4 | bsz 201.4 | num_updates 36116 | best_loss 4.749\n",
      "2021-10-07 19:24:17 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:24:21 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint28.pt (epoch 28 @ 36116 updates, score 4.749) (writing took 4.684335523750633 seconds)\n",
      "2021-10-07 19:24:22 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
      "2021-10-07 19:24:22 | INFO | train | epoch 028 | loss 3.724 | nll_loss 2.02 | ppl 4.06 | wps 22752.1 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 36116 | lr 0.000166399 | gnorm 1.087 | loss_scale 8 | train_wall 194 | wall 5838\n",
      "2021-10-07 19:24:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28\n",
      "2021-10-07 19:24:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29\n",
      "2021-10-07 19:24:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:24:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005400\n",
      "2021-10-07 19:24:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.118327\n",
      "2021-10-07 19:24:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:22 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[27302]\n",
      "2021-10-07 19:24:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011143\n",
      "2021-10-07 19:24:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.008389\n",
      "2021-10-07 19:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.138916\n",
      "2021-10-07 19:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089717\n",
      "2021-10-07 19:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:23 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[27302]\n",
      "2021-10-07 19:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008725\n",
      "2021-10-07 19:24:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.003982\n",
      "2021-10-07 19:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.103448\n",
      "2021-10-07 19:24:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 029:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:24:24 | INFO | fairseq.trainer | begin training epoch 29\n",
      "epoch 029: 100%|9| 1289/1290 [03:21<00:00,  6.49it/s, loss=3.705, nll_loss=2.0012021-10-07 19:27:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001701\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060714\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041194\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104359\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001628\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065121\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052910\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120593\n",
      "2021-10-07 19:27:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 029 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.43it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.10it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.86it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.39it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.88it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.28it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.41it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.36it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.20it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.36it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.58it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.78it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.30it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:27:47 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.75 | nll_loss 3.088 | ppl 8.5 | wps 51483.7 | wpb 2691.4 | bsz 201.4 | num_updates 37406 | best_loss 4.749\n",
      "2021-10-07 19:27:47 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:27:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint29.pt (epoch 29 @ 37406 updates, score 4.75) (writing took 3.574658603873104 seconds)\n",
      "2021-10-07 19:27:51 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
      "2021-10-07 19:27:51 | INFO | train | epoch 029 | loss 3.698 | nll_loss 1.991 | ppl 3.98 | wps 22813.9 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 37406 | lr 0.000163504 | gnorm 1.097 | loss_scale 16 | train_wall 195 | wall 6047\n",
      "2021-10-07 19:27:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29\n",
      "2021-10-07 19:27:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30\n",
      "2021-10-07 19:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:27:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005285\n",
      "2021-10-07 19:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109000\n",
      "2021-10-07 19:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:51 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[116057]\n",
      "2021-10-07 19:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008846\n",
      "2021-10-07 19:27:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.957771\n",
      "2021-10-07 19:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.076651\n",
      "2021-10-07 19:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083497\n",
      "2021-10-07 19:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[116057]\n",
      "2021-10-07 19:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008444\n",
      "2021-10-07 19:27:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.937195\n",
      "2021-10-07 19:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.030029\n",
      "2021-10-07 19:27:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 030:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:27:53 | INFO | fairseq.trainer | begin training epoch 30\n",
      "epoch 030: 100%|9| 1289/1290 [03:20<00:00,  6.46it/s, loss=3.838, nll_loss=2.1472021-10-07 19:31:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001732\n",
      "2021-10-07 19:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068209\n",
      "2021-10-07 19:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.058170\n",
      "2021-10-07 19:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.129146\n",
      "2021-10-07 19:31:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.004333\n",
      "2021-10-07 19:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064327\n",
      "2021-10-07 19:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044504\n",
      "2021-10-07 19:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114263\n",
      "2021-10-07 19:31:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 030 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.97it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.59it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.32it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.05it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.61it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.08it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.23it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.18it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.04it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.28it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.28it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:31:15 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.752 | nll_loss 3.097 | ppl 8.55 | wps 51955.5 | wpb 2691.4 | bsz 201.4 | num_updates 38696 | best_loss 4.749\n",
      "2021-10-07 19:31:15 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:31:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint30.pt (epoch 30 @ 38696 updates, score 4.752) (writing took 3.502921781037003 seconds)\n",
      "2021-10-07 19:31:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
      "2021-10-07 19:31:19 | INFO | train | epoch 030 | loss 3.673 | nll_loss 1.963 | ppl 3.9 | wps 22901.4 | ups 6.2 | wpb 3695.9 | bsz 249.8 | num_updates 38696 | lr 0.000160756 | gnorm 1.1 | loss_scale 16 | train_wall 194 | wall 6255\n",
      "2021-10-07 19:31:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30\n",
      "2021-10-07 19:31:19 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31\n",
      "2021-10-07 19:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:31:19 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004882\n",
      "2021-10-07 19:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108123\n",
      "2021-10-07 19:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[15510]\n",
      "2021-10-07 19:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009839\n",
      "2021-10-07 19:31:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.946886\n",
      "2021-10-07 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.065813\n",
      "2021-10-07 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087340\n",
      "2021-10-07 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[15510]\n",
      "2021-10-07 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010745\n",
      "2021-10-07 19:31:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:31:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.970213\n",
      "2021-10-07 19:31:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.069244\n",
      "2021-10-07 19:31:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 031:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:31:21 | INFO | fairseq.trainer | begin training epoch 31\n",
      "epoch 031: 100%|9| 1289/1290 [03:21<00:00,  6.48it/s, loss=3.697, nll_loss=1.9912021-10-07 19:34:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001806\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061563\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042987\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107406\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001774\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061687\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045157\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109381\n",
      "2021-10-07 19:34:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 031 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.47it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.14it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.87it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.54it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.13it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.48it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.59it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.34it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.54it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.69it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.83it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:34:45 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.752 | nll_loss 3.094 | ppl 8.54 | wps 51574.5 | wpb 2691.4 | bsz 201.4 | num_updates 39986 | best_loss 4.749\n",
      "2021-10-07 19:34:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:34:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint31.pt (epoch 31 @ 39986 updates, score 4.752) (writing took 3.560062790289521 seconds)\n",
      "2021-10-07 19:34:48 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
      "2021-10-07 19:34:48 | INFO | train | epoch 031 | loss 3.649 | nll_loss 1.935 | ppl 3.82 | wps 22767 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 39986 | lr 0.000158142 | gnorm 1.107 | loss_scale 16 | train_wall 195 | wall 6465\n",
      "2021-10-07 19:34:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31\n",
      "2021-10-07 19:34:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32\n",
      "2021-10-07 19:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:34:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005133\n",
      "2021-10-07 19:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.110698\n",
      "2021-10-07 19:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[3638]\n",
      "2021-10-07 19:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.014573\n",
      "2021-10-07 19:34:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.961651\n",
      "2021-10-07 19:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.087897\n",
      "2021-10-07 19:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086486\n",
      "2021-10-07 19:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[3638]\n",
      "2021-10-07 19:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008641\n",
      "2021-10-07 19:34:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.931102\n",
      "2021-10-07 19:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.027188\n",
      "2021-10-07 19:34:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 032:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:34:50 | INFO | fairseq.trainer | begin training epoch 32\n",
      "epoch 032: 100%|9| 1289/1290 [03:21<00:00,  6.26it/s, loss=3.559, nll_loss=1.8372021-10-07 19:38:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001742\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060850\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040921\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104397\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001947\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061170\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040992\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105076\n",
      "2021-10-07 19:38:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 032 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.35it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.02it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.76it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.47it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.89it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.24it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.32it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.97it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.22it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.48it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.70it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:38:13 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.702 | nll_loss 3.043 | ppl 8.24 | wps 51404.3 | wpb 2691.4 | bsz 201.4 | num_updates 41276 | best_loss 4.702\n",
      "2021-10-07 19:38:13 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:38:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint32.pt (epoch 32 @ 41276 updates, score 4.702) (writing took 4.648561770096421 seconds)\n",
      "2021-10-07 19:38:18 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
      "2021-10-07 19:38:18 | INFO | train | epoch 032 | loss 3.626 | nll_loss 1.909 | ppl 3.75 | wps 22696.5 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 41276 | lr 0.000155651 | gnorm 1.117 | loss_scale 16 | train_wall 195 | wall 6675\n",
      "2021-10-07 19:38:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32\n",
      "2021-10-07 19:38:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33\n",
      "2021-10-07 19:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:38:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005445\n",
      "2021-10-07 19:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106932\n",
      "2021-10-07 19:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[282093]\n",
      "2021-10-07 19:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009218\n",
      "2021-10-07 19:38:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.951703\n",
      "2021-10-07 19:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.068799\n",
      "2021-10-07 19:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088038\n",
      "2021-10-07 19:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[282093]\n",
      "2021-10-07 19:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007707\n",
      "2021-10-07 19:38:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:38:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.982372\n",
      "2021-10-07 19:38:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.079151\n",
      "2021-10-07 19:38:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 033:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:38:20 | INFO | fairseq.trainer | begin training epoch 33\n",
      "epoch 033: 100%|9| 1289/1290 [03:21<00:00,  6.40it/s, loss=3.558, nll_loss=1.8352021-10-07 19:41:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001605\n",
      "2021-10-07 19:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060926\n",
      "2021-10-07 19:41:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040551\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103952\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001517\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060254\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040197\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102876\n",
      "2021-10-07 19:41:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 033 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.13it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.78it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.48it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.18it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.81it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.26it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.46it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.20it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.37it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:41:44 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.734 | nll_loss 3.069 | ppl 8.39 | wps 52161.9 | wpb 2691.4 | bsz 201.4 | num_updates 42566 | best_loss 4.702\n",
      "2021-10-07 19:41:44 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:41:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint33.pt (epoch 33 @ 42566 updates, score 4.734) (writing took 3.4667914756573737 seconds)\n",
      "2021-10-07 19:41:48 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
      "2021-10-07 19:41:48 | INFO | train | epoch 033 | loss 3.604 | nll_loss 1.884 | ppl 3.69 | wps 22759.4 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 42566 | lr 0.000153274 | gnorm 1.122 | loss_scale 16 | train_wall 195 | wall 6884\n",
      "2021-10-07 19:41:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33\n",
      "2021-10-07 19:41:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34\n",
      "2021-10-07 19:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:41:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004802\n",
      "2021-10-07 19:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107560\n",
      "2021-10-07 19:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[263938]\n",
      "2021-10-07 19:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008812\n",
      "2021-10-07 19:41:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.924676\n",
      "2021-10-07 19:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.042009\n",
      "2021-10-07 19:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087315\n",
      "2021-10-07 19:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[263938]\n",
      "2021-10-07 19:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008083\n",
      "2021-10-07 19:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.920224\n",
      "2021-10-07 19:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.016560\n",
      "2021-10-07 19:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 034:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:41:50 | INFO | fairseq.trainer | begin training epoch 34\n",
      "epoch 034: 100%|9| 1289/1290 [03:22<00:00,  6.57it/s, loss=3.676, nll_loss=1.9662021-10-07 19:45:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002472\n",
      "2021-10-07 19:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062211\n",
      "2021-10-07 19:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.054835\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.120754\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001603\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066784\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052130\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121185\n",
      "2021-10-07 19:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 034 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.15it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.79it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.52it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.25it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.85it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.30it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.45it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.41it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.27it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.23it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.52it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.83it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.38it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:45:14 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.734 | nll_loss 3.071 | ppl 8.41 | wps 51832.5 | wpb 2691.4 | bsz 201.4 | num_updates 43856 | best_loss 4.702\n",
      "2021-10-07 19:45:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:45:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint34.pt (epoch 34 @ 43856 updates, score 4.734) (writing took 3.5794499460607767 seconds)\n",
      "2021-10-07 19:45:18 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
      "2021-10-07 19:45:18 | INFO | train | epoch 034 | loss 3.585 | nll_loss 1.862 | ppl 3.64 | wps 22685 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 43856 | lr 0.000151003 | gnorm 1.133 | loss_scale 16 | train_wall 196 | wall 7094\n",
      "2021-10-07 19:45:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34\n",
      "2021-10-07 19:45:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35\n",
      "2021-10-07 19:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:45:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004794\n",
      "2021-10-07 19:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.112765\n",
      "2021-10-07 19:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[46587]\n",
      "2021-10-07 19:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010652\n",
      "2021-10-07 19:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.934709\n",
      "2021-10-07 19:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.059134\n",
      "2021-10-07 19:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089718\n",
      "2021-10-07 19:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[46587]\n",
      "2021-10-07 19:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008260\n",
      "2021-10-07 19:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.947368\n",
      "2021-10-07 19:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.046354\n",
      "2021-10-07 19:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 035:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:45:20 | INFO | fairseq.trainer | begin training epoch 35\n",
      "epoch 035: 100%|9| 1289/1290 [03:21<00:00,  6.70it/s, loss=3.636, nll_loss=1.9192021-10-07 19:48:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001748\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061446\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044040\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108269\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001589\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060198\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041412\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104179\n",
      "2021-10-07 19:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 035 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.53it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.10it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  8.79it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.52it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.18it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.68it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.94it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.97it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.87it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.20it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.46it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.70it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:48:43 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.721 | nll_loss 3.057 | ppl 8.32 | wps 52105.8 | wpb 2691.4 | bsz 201.4 | num_updates 45146 | best_loss 4.702\n",
      "2021-10-07 19:48:43 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:48:47 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint35.pt (epoch 35 @ 45146 updates, score 4.721) (writing took 3.5721908202394843 seconds)\n",
      "2021-10-07 19:48:47 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
      "2021-10-07 19:48:47 | INFO | train | epoch 035 | loss 3.563 | nll_loss 1.837 | ppl 3.57 | wps 22796.3 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 45146 | lr 0.00014883 | gnorm 1.134 | loss_scale 16 | train_wall 195 | wall 7304\n",
      "2021-10-07 19:48:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35\n",
      "2021-10-07 19:48:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36\n",
      "2021-10-07 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:48:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.007403\n",
      "2021-10-07 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.114189\n",
      "2021-10-07 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:47 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[198399]\n",
      "2021-10-07 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009769\n",
      "2021-10-07 19:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.962044\n",
      "2021-10-07 19:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.087058\n",
      "2021-10-07 19:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088606\n",
      "2021-10-07 19:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[198399]\n",
      "2021-10-07 19:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008401\n",
      "2021-10-07 19:48:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.947145\n",
      "2021-10-07 19:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.045089\n",
      "2021-10-07 19:48:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 036:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:48:49 | INFO | fairseq.trainer | begin training epoch 36\n",
      "epoch 036: 100%|9| 1289/1290 [03:21<00:00,  6.35it/s, loss=3.529, nll_loss=1.8022021-10-07 19:52:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001727\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063013\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043539\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109185\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002171\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066613\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040469\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110218\n",
      "2021-10-07 19:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 036 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.53it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.21it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.95it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.67it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.20it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.57it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.29it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.48it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.88it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.33it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:52:13 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.736 | nll_loss 3.08 | ppl 8.46 | wps 51767.1 | wpb 2691.4 | bsz 201.4 | num_updates 46436 | best_loss 4.702\n",
      "2021-10-07 19:52:13 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:52:16 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint36.pt (epoch 36 @ 46436 updates, score 4.736) (writing took 3.4637176478281617 seconds)\n",
      "2021-10-07 19:52:16 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
      "2021-10-07 19:52:16 | INFO | train | epoch 036 | loss 3.543 | nll_loss 1.814 | ppl 3.52 | wps 22790.6 | ups 6.17 | wpb 3695.9 | bsz 249.8 | num_updates 46436 | lr 0.000146748 | gnorm 1.145 | loss_scale 16 | train_wall 195 | wall 7513\n",
      "2021-10-07 19:52:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36\n",
      "2021-10-07 19:52:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37\n",
      "2021-10-07 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:52:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005181\n",
      "2021-10-07 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111119\n",
      "2021-10-07 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:16 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[32581]\n",
      "2021-10-07 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009545\n",
      "2021-10-07 19:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.957127\n",
      "2021-10-07 19:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.078751\n",
      "2021-10-07 19:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086474\n",
      "2021-10-07 19:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:17 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[32581]\n",
      "2021-10-07 19:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008947\n",
      "2021-10-07 19:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:52:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.946681\n",
      "2021-10-07 19:52:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.043036\n",
      "2021-10-07 19:52:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 037:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:52:18 | INFO | fairseq.trainer | begin training epoch 37\n",
      "epoch 037: 100%|9| 1289/1290 [03:22<00:00,  6.46it/s, loss=3.468, nll_loss=1.7332021-10-07 19:55:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001727\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060823\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041305\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104769\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001583\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059983\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040492\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102978\n",
      "2021-10-07 19:55:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 037 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.27it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.93it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.66it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.35it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.97it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.39it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.46it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.25it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.49it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.59it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.85it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.38it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:55:43 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.701 | nll_loss 3.038 | ppl 8.21 | wps 51857.7 | wpb 2691.4 | bsz 201.4 | num_updates 47726 | best_loss 4.701\n",
      "2021-10-07 19:55:43 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:55:47 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint37.pt (epoch 37 @ 47726 updates, score 4.701) (writing took 4.681162662804127 seconds)\n",
      "2021-10-07 19:55:47 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
      "2021-10-07 19:55:47 | INFO | train | epoch 037 | loss 3.526 | nll_loss 1.795 | ppl 3.47 | wps 22575 | ups 6.11 | wpb 3695.9 | bsz 249.8 | num_updates 47726 | lr 0.000144751 | gnorm 1.153 | loss_scale 16 | train_wall 196 | wall 7724\n",
      "2021-10-07 19:55:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37\n",
      "2021-10-07 19:55:47 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38\n",
      "2021-10-07 19:55:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:55:47 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005180\n",
      "2021-10-07 19:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.126116\n",
      "2021-10-07 19:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[309529]\n",
      "2021-10-07 19:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010432\n",
      "2021-10-07 19:55:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.005407\n",
      "2021-10-07 19:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.142965\n",
      "2021-10-07 19:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083329\n",
      "2021-10-07 19:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[309529]\n",
      "2021-10-07 19:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008881\n",
      "2021-10-07 19:55:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.015334\n",
      "2021-10-07 19:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.108547\n",
      "2021-10-07 19:55:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 038:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:55:50 | INFO | fairseq.trainer | begin training epoch 38\n",
      "epoch 038: 100%|9| 1289/1290 [03:22<00:00,  6.29it/s, loss=3.605, nll_loss=1.8832021-10-07 19:59:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001765\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061889\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041659\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106169\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001783\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061312\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041263\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105269\n",
      "2021-10-07 19:59:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 038 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.81it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.43it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.11it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 10.81it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.36it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.81it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.01it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.02it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 16.96it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.14it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.50it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  96%|#####7| 27/28 [00:01<00:00, 19.03it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 19:59:14 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.716 | nll_loss 3.051 | ppl 8.29 | wps 51495.2 | wpb 2691.4 | bsz 201.4 | num_updates 49016 | best_loss 4.701\n",
      "2021-10-07 19:59:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 19:59:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint38.pt (epoch 38 @ 49016 updates, score 4.716) (writing took 3.5340180108323693 seconds)\n",
      "2021-10-07 19:59:18 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
      "2021-10-07 19:59:18 | INFO | train | epoch 038 | loss 3.509 | nll_loss 1.775 | ppl 3.42 | wps 22649 | ups 6.13 | wpb 3695.9 | bsz 249.8 | num_updates 49016 | lr 0.000142834 | gnorm 1.161 | loss_scale 16 | train_wall 196 | wall 7935\n",
      "2021-10-07 19:59:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38\n",
      "2021-10-07 19:59:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39\n",
      "2021-10-07 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:59:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005187\n",
      "2021-10-07 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111020\n",
      "2021-10-07 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[218051]\n",
      "2021-10-07 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.017543\n",
      "2021-10-07 19:59:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.952471\n",
      "2021-10-07 19:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.082091\n",
      "2021-10-07 19:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 19:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088372\n",
      "2021-10-07 19:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[218051]\n",
      "2021-10-07 19:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008852\n",
      "2021-10-07 19:59:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 19:59:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.937854\n",
      "2021-10-07 19:59:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.036056\n",
      "2021-10-07 19:59:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 039:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 19:59:20 | INFO | fairseq.trainer | begin training epoch 39\n",
      "epoch 039: 100%|9| 1289/1290 [03:21<00:00,  6.51it/s, loss=3.43, nll_loss=1.689,2021-10-07 20:02:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 20:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001712\n",
      "2021-10-07 20:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060372\n",
      "2021-10-07 20:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.049210\n",
      "2021-10-07 20:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112031\n",
      "2021-10-07 20:02:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001866\n",
      "2021-10-07 20:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062465\n",
      "2021-10-07 20:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.039882\n",
      "2021-10-07 20:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104998\n",
      "2021-10-07 20:02:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 039 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.22it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.89it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.61it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.36it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.97it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.31it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.43it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.27it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.12it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.35it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.57it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.82it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 20:02:43 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.689 | nll_loss 3.026 | ppl 8.14 | wps 51856.3 | wpb 2691.4 | bsz 201.4 | num_updates 50306 | best_loss 4.689\n",
      "2021-10-07 20:02:43 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 20:02:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint39.pt (epoch 39 @ 50306 updates, score 4.689) (writing took 4.724755809176713 seconds)\n",
      "2021-10-07 20:02:48 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
      "2021-10-07 20:02:48 | INFO | train | epoch 039 | loss 3.492 | nll_loss 1.755 | ppl 3.38 | wps 22697.5 | ups 6.14 | wpb 3695.9 | bsz 249.8 | num_updates 50306 | lr 0.000140991 | gnorm 1.162 | loss_scale 16 | train_wall 195 | wall 8145\n",
      "2021-10-07 20:02:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39\n",
      "2021-10-07 20:02:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40\n",
      "2021-10-07 20:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:02:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005090\n",
      "2021-10-07 20:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.115524\n",
      "2021-10-07 20:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[154612]\n",
      "2021-10-07 20:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010187\n",
      "2021-10-07 20:02:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.988349\n",
      "2021-10-07 20:02:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.115069\n",
      "2021-10-07 20:02:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:02:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.134733\n",
      "2021-10-07 20:02:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[154612]\n",
      "2021-10-07 20:02:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.022381\n",
      "2021-10-07 20:02:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.091319\n",
      "2021-10-07 20:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.250338\n",
      "2021-10-07 20:02:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 040:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 20:02:50 | INFO | fairseq.trainer | begin training epoch 40\n",
      "epoch 040: 100%|9| 1289/1290 [03:21<00:00,  6.40it/s, loss=3.425, nll_loss=1.6822021-10-07 20:06:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001654\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062088\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040361\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104848\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001534\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062068\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040660\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104928\n",
      "2021-10-07 20:06:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 040 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.68it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.40it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.16it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.85it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.74it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 11.60it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 13.17it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 14.61it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 16.27it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 17.04it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 17.69it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 20:06:14 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.717 | nll_loss 3.058 | ppl 8.33 | wps 46982.3 | wpb 2691.4 | bsz 201.4 | num_updates 51596 | best_loss 4.689\n",
      "2021-10-07 20:06:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 20:06:17 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint40.pt (epoch 40 @ 51596 updates, score 4.717) (writing took 3.4588696430437267 seconds)\n",
      "2021-10-07 20:06:17 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
      "2021-10-07 20:06:17 | INFO | train | epoch 040 | loss 3.475 | nll_loss 1.736 | ppl 3.33 | wps 22756.2 | ups 6.16 | wpb 3695.9 | bsz 249.8 | num_updates 51596 | lr 0.000139217 | gnorm 1.171 | loss_scale 16 | train_wall 195 | wall 8354\n",
      "2021-10-07 20:06:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40\n",
      "2021-10-07 20:06:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41\n",
      "2021-10-07 20:06:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:06:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004897\n",
      "2021-10-07 20:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108700\n",
      "2021-10-07 20:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[280484]\n",
      "2021-10-07 20:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008660\n",
      "2021-10-07 20:06:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.983241\n",
      "2021-10-07 20:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.101503\n",
      "2021-10-07 20:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.094677\n",
      "2021-10-07 20:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[280484]\n",
      "2021-10-07 20:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008543\n",
      "2021-10-07 20:06:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.947899\n",
      "2021-10-07 20:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.052045\n",
      "2021-10-07 20:06:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 041:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 20:06:20 | INFO | fairseq.trainer | begin training epoch 41\n",
      "epoch 041: 100%|9| 1289/1290 [03:22<00:00,  6.73it/s, loss=3.505, nll_loss=1.7692021-10-07 20:09:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001628\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060687\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040093\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103261\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001550\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061878\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041158\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105440\n",
      "2021-10-07 20:09:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 041 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.98it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.60it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.34it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.08it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.62it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.06it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.25it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.67it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.14it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.47it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 20:09:45 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.705 | nll_loss 3.04 | ppl 8.22 | wps 51885.3 | wpb 2691.4 | bsz 201.4 | num_updates 52886 | best_loss 4.689\n",
      "2021-10-07 20:09:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 20:09:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint41.pt (epoch 41 @ 52886 updates, score 4.705) (writing took 3.4525631312280893 seconds)\n",
      "2021-10-07 20:09:48 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
      "2021-10-07 20:09:48 | INFO | train | epoch 041 | loss 3.459 | nll_loss 1.718 | ppl 3.29 | wps 22640.3 | ups 6.13 | wpb 3695.9 | bsz 249.8 | num_updates 52886 | lr 0.000137509 | gnorm 1.179 | loss_scale 16 | train_wall 196 | wall 8565\n",
      "2021-10-07 20:09:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=42/shard_epoch=41\n",
      "2021-10-07 20:09:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=42/shard_epoch=42\n",
      "2021-10-07 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:09:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005055\n",
      "2021-10-07 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108083\n",
      "2021-10-07 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[1215]\n",
      "2021-10-07 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010528\n",
      "2021-10-07 20:09:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.941264\n",
      "2021-10-07 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.060922\n",
      "2021-10-07 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086575\n",
      "2021-10-07 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[1215]\n",
      "2021-10-07 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009008\n",
      "2021-10-07 20:09:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:09:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.933162\n",
      "2021-10-07 20:09:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.029882\n",
      "2021-10-07 20:09:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 042:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 20:09:50 | INFO | fairseq.trainer | begin training epoch 42\n",
      "epoch 042:  28%|2| 360/1290 [00:56<02:24,  6.43it/s, loss=3.563, nll_loss=1.83, 2021-10-07 20:10:47 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 042: 100%|9| 1289/1290 [03:20<00:00,  6.34it/s, loss=3.534, nll_loss=1.8032021-10-07 20:13:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001977\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061001\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045326\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109253\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001617\n",
      "2021-10-07 20:13:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066739\n",
      "2021-10-07 20:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044944\n",
      "2021-10-07 20:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.114705\n",
      "2021-10-07 20:13:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 042 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.05it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.69it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.41it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.14it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.76it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.22it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.44it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.18it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.31it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.53it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.22it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 20:13:13 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.704 | nll_loss 3.043 | ppl 8.24 | wps 51895.4 | wpb 2691.4 | bsz 201.4 | num_updates 54175 | best_loss 4.689\n",
      "2021-10-07 20:13:13 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 20:13:17 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint42.pt (epoch 42 @ 54175 updates, score 4.704) (writing took 3.469880964141339 seconds)\n",
      "2021-10-07 20:13:17 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
      "2021-10-07 20:13:17 | INFO | train | epoch 042 | loss 3.441 | nll_loss 1.698 | ppl 3.24 | wps 22842.3 | ups 6.18 | wpb 3696.4 | bsz 249.8 | num_updates 54175 | lr 0.000135863 | gnorm 1.183 | loss_scale 16 | train_wall 195 | wall 8773\n",
      "2021-10-07 20:13:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=43/shard_epoch=42\n",
      "2021-10-07 20:13:17 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=43/shard_epoch=43\n",
      "2021-10-07 20:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:13:17 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005926\n",
      "2021-10-07 20:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.118081\n",
      "2021-10-07 20:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:17 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[240951]\n",
      "2021-10-07 20:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009518\n",
      "2021-10-07 20:13:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.925301\n",
      "2021-10-07 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.053857\n",
      "2021-10-07 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.089064\n",
      "2021-10-07 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[240951]\n",
      "2021-10-07 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008236\n",
      "2021-10-07 20:13:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.916804\n",
      "2021-10-07 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.015048\n",
      "2021-10-07 20:13:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 043:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 20:13:19 | INFO | fairseq.trainer | begin training epoch 43\n",
      "epoch 043: 100%|9| 1289/1290 [03:20<00:00,  6.44it/s, loss=3.442, nll_loss=1.6992021-10-07 20:16:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 20:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001733\n",
      "2021-10-07 20:16:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063379\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042064\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108076\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001765\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060565\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040806\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104032\n",
      "2021-10-07 20:16:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 043 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.37it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.04it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.79it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.39it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.98it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.28it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.39it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.38it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.22it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.38it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.63it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.86it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 20:16:41 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.714 | nll_loss 3.05 | ppl 8.28 | wps 51570.5 | wpb 2691.4 | bsz 201.4 | num_updates 55465 | best_loss 4.689\n",
      "2021-10-07 20:16:41 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-07 20:16:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/quy-es+es-en/checkpoint43.pt (epoch 43 @ 55465 updates, score 4.714) (writing took 3.4935607658699155 seconds)\n",
      "2021-10-07 20:16:45 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
      "2021-10-07 20:16:45 | INFO | train | epoch 043 | loss 3.429 | nll_loss 1.684 | ppl 3.21 | wps 22903.6 | ups 6.2 | wpb 3695.9 | bsz 249.8 | num_updates 55465 | lr 0.000134274 | gnorm 1.189 | loss_scale 16 | train_wall 194 | wall 8981\n",
      "2021-10-07 20:16:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=44/shard_epoch=43\n",
      "2021-10-07 20:16:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=44/shard_epoch=44\n",
      "2021-10-07 20:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:16:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004972\n",
      "2021-10-07 20:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105990\n",
      "2021-10-07 20:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:45 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[3846]\n",
      "2021-10-07 20:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009608\n",
      "2021-10-07 20:16:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.949837\n",
      "2021-10-07 20:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.066408\n",
      "2021-10-07 20:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086643\n",
      "2021-10-07 20:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:46 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[3846]\n",
      "2021-10-07 20:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008902\n",
      "2021-10-07 20:16:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.022466\n",
      "2021-10-07 20:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.118963\n",
      "2021-10-07 20:16:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 044:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-07 20:16:47 | INFO | fairseq.trainer | begin training epoch 44\n",
      "epoch 044: 100%|9| 1289/1290 [03:21<00:00,  6.39it/s, loss=3.547, nll_loss=1.8142021-10-07 20:20:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-07 20:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001760\n",
      "2021-10-07 20:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062465\n",
      "2021-10-07 20:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042497\n",
      "2021-10-07 20:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107607\n",
      "2021-10-07 20:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:20:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-07 20:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001625\n",
      "2021-10-07 20:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063434\n",
      "2021-10-07 20:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-07 20:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041241\n",
      "2021-10-07 20:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107165\n",
      "2021-10-07 20:20:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 044 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.30it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.95it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.63it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.34it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.94it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.33it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  64%|###8  | 18/28 [00:01<00:00, 17.06it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.35it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.50it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-07 20:20:10 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.714 | nll_loss 3.053 | ppl 8.3 | wps 51284.9 | wpb 2691.4 | bsz 201.4 | num_updates 56755 | best_loss 4.689\n",
      "2021-10-07 20:20:10 | INFO | fairseq_cli.train | begin save checkpoint\n"
     ]
    }
   ],
   "source": [
    "! fairseq-train $BIN_DIR \\\n",
    "    --arch=transformer --share-all-embeddings \\\n",
    "    --task translation_multi_simple_epoch --lang-pairs quy-es,es-en \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --restore-file $MODEL_DIR/checkpoint_last.pt \\\n",
    "    --save-dir $MODEL_DIR/ \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --reset-optimizer \\\n",
    "    --encoder-langtok \"src\" \\\n",
    "    --decoder-langtok \\\n",
    "    --fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "! echo $MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "cd /storage/master-thesis/models/quy-es+es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm checkpoint108.pt\n",
    "! rm checkpoint109.pt\n",
    "! rm checkpoint_best.pt\n",
    "! rm checkpoint_last.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm dict.en.txt\n",
    "! rm dict.es.txt\n",
    "! rm dict.quy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5K\t/notebooks/CITATION.cff\n",
      "5.5K\t/notebooks/CODE_OF_CONDUCT.md\n",
      "15K\t/notebooks/CONTRIBUTING.md\n",
      "19K\t/notebooks/ISSUES.md\n",
      "12K\t/notebooks/LICENSE\n",
      "512\t/notebooks/MANIFEST.in\n",
      "3.5K\t/notebooks/Makefile\n",
      "41K\t/notebooks/README.md\n",
      "41K\t/notebooks/README_zh-hans.md\n",
      "42K\t/notebooks/README_zh-hant.md\n",
      "12K\t/notebooks/docker\n",
      "4.6M\t/notebooks/docs\n",
      "5.0M\t/notebooks/examples\n",
      "8.5K\t/notebooks/hubconf.py\n",
      "7.8G\t/notebooks/master-thesis\n",
      "1.5K\t/notebooks/model_cards\n",
      "9.5K\t/notebooks/notebooks\n",
      "512\t/notebooks/pyproject.toml\n",
      "64K\t/notebooks/scripts\n",
      "1.0K\t/notebooks/setup.cfg\n",
      "13K\t/notebooks/setup.py\n",
      "13M\t/notebooks/src\n",
      "731K\t/notebooks/templates\n",
      "4.5K\t/notebooks/test quy-es -> es-en model.ipynb\n",
      "6.8M\t/notebooks/tests\n",
      "147K\t/notebooks/train es-en model.ipynb\n",
      "158K\t/notebooks/train quy-es + es-en model.ipynb\n",
      "160K\t/notebooks/utils\n",
      "3.5K\t/notebooks/valohai.yaml\n",
      "7.9G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\t/notebooks/master-thesis/Untitled.ipynb\n",
      "362M\t/notebooks/master-thesis/corpora\n",
      "7.5G\t/notebooks/master-thesis/models\n",
      "7.8G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6G\t/notebooks/master-thesis/models/es-en\n",
      "802M\t/notebooks/master-thesis/models/quy-es\n",
      "5.2G\t/notebooks/master-thesis/models/quy-es+es-en\n",
      "7.5G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19M\t/apex\n",
      "5.0M\t/bin\n",
      "4.0K\t/boot\n",
      "24K\t/content\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! du -shc /*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CITATION.cff         \u001b[0m\u001b[01;34mdocker\u001b[0m/          setup.py\n",
      " CODE_OF_CONDUCT.md   \u001b[01;34mdocs\u001b[0m/            \u001b[01;34msrc\u001b[0m/\n",
      " CONTRIBUTING.md      \u001b[01;34mexamples\u001b[0m/        \u001b[01;34mtemplates\u001b[0m/\n",
      " ISSUES.md            hubconf.py      'test quy-es -> es-en model.ipynb'\n",
      " LICENSE              \u001b[01;34mmaster-thesis\u001b[0m/   \u001b[01;34mtests\u001b[0m/\n",
      " MANIFEST.in          \u001b[01;34mmodel_cards\u001b[0m/    'train es-en model.ipynb'\n",
      " Makefile             \u001b[01;34mnotebooks\u001b[0m/      'train quy-es + es-en model.ipynb'\n",
      " README.md            pyproject.toml   \u001b[01;34mutils\u001b[0m/\n",
      " README_zh-hans.md    \u001b[01;34mscripts\u001b[0m/         valohai.yaml\n",
      " README_zh-hant.md    setup.cfg\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
