{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tatoeba format to standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(EN_ES_CORPUS_DIR + \"original/valid.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"dev.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"dev.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "    en.close()\n",
    "    es.close()\n",
    "    \n",
    "with open(EN_ES_CORPUS_DIR + \"original/dev.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"train.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"train.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        if \"\\n\" in line[2] or \"\\n\" in line[3]:\n",
    "            continue\n",
    "        if len(line) != 4:\n",
    "            continue\n",
    "            \n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "        \n",
    "    en.close()\n",
    "    es.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.10.3 in /usr/local/lib/python3.6/dist-packages (0.10.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers==0.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_path = \"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "en_es_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "en_es_trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "en_es_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "en_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.en\" for split in [\"dev\", \"train\"]]\n",
    "es_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "files = en_files + es_files\n",
    "en_es_tokenizer.train(files= files, trainer=en_es_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_files(tokenizer, files, extension):\n",
    "    for file in files:\n",
    "        print(f\"Reading file {file}\")\n",
    "        with open(file, encoding='utf8') as f:\n",
    "          lines = f.readlines()\n",
    "          tokenized_lines = tokenizer.encode_batch(lines)\n",
    "          tokenized_name = Path(file).stem\n",
    "          tokenized_name = tokenized_path + tokenized_name + \".\" + extension\n",
    "          print(tokenized_name)\n",
    "          with open(tokenized_name, 'w', encoding='utf8') as wf:\n",
    "\n",
    "            wf.writelines([\" \".join(t.tokens) + \"\\n\" for t in tokenized_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_tokenizer.save(tokenized_path + \"en-es-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.Tokenizer object at 0x2cf5440>\n"
     ]
    }
   ],
   "source": [
    "print(en_es_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['En', 'mis', 'sueños', 'tengo', 'mi', 'propio', 'idioma', '.', '¿', 'Será', 'qué', 'me', 'comunic', '##o', 'con', 'mi', 'planeta', 'mientras', 'duermo', '?']\n"
     ]
    }
   ],
   "source": [
    "tok = en_es_tokenizer.encode(\"En mis sueños tengo mi propio idioma. ¿Será qué me comunico con mi planeta mientras duermo?\")\n",
    "print(tok.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.en\n",
      "/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/dev.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.en\n",
      "/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/train.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.es\n",
      "/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.es\n",
      "/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/train.es\n"
     ]
    }
   ],
   "source": [
    "tokenize_files(en_es_tokenizer, en_files, \"en\")\n",
    "tokenize_files(en_es_tokenizer, es_files, \"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEST_DIR=/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es\n"
     ]
    }
   ],
   "source": [
    "%env DEST_DIR = /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (0.10.2)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.24)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.6.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.51.0)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.3)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2020.11.13)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (2.0.0)\n",
      "Requirement already satisfied: hydra-core in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.1.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.18.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.3.2)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.4)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (5.2.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (4.8)\n",
      "Requirement already satisfied: omegaconf==2.1.* in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (2.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq) (5.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 17:40:54 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/bin/', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='es', srcdict=None, target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/train', user_dir=None, validpref='/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/dev', workers=20)\n",
      "2021-09-29 17:40:59 | INFO | fairseq_cli.preprocess | [es] Dictionary: 20112 types\n",
      "2021-09-29 17:41:05 | INFO | fairseq_cli.preprocess | [es] /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/train.es: 197297 sents, 1840182 tokens, 0.0% replaced by <unk>\n",
      "2021-09-29 17:41:05 | INFO | fairseq_cli.preprocess | [es] Dictionary: 20112 types\n",
      "2021-09-29 17:41:07 | INFO | fairseq_cli.preprocess | [es] /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/dev.es: 4643 sents, 50022 tokens, 0.384% replaced by <unk>\n",
      "2021-09-29 17:41:07 | INFO | fairseq_cli.preprocess | [en] Dictionary: 14800 types\n",
      "2021-09-29 17:41:10 | INFO | fairseq_cli.preprocess | [en] /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/train.en: 197297 sents, 1928819 tokens, 0.0% replaced by <unk>\n",
      "2021-09-29 17:41:10 | INFO | fairseq_cli.preprocess | [en] Dictionary: 14800 types\n",
      "2021-09-29 17:41:11 | INFO | fairseq_cli.preprocess | [en] /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/dev.en: 4643 sents, 52152 tokens, 0.286% replaced by <unk>\n",
      "2021-09-29 17:41:11 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/bin/\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang es --target-lang en \\\n",
    "    --trainpref $DEST_DIR/train --validpref $DEST_DIR/dev \\\n",
    "    --destdir $DEST_DIR/bin/ \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_DIR=/notebooks/master-thesis/models/es-en\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_DIR = /notebooks/master-thesis/models/es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 21:18:31 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/bin/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=3, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='/notebooks/master-thesis/models/es-en/checkpoint_last.pt', save_dir='/notebooks/master-thesis/models/es-en', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='es', stop_time_hours=0, target_lang='en', task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')\n",
      "2021-09-29 21:18:31 | INFO | fairseq.tasks.translation | [es] dictionary: 20112 types\n",
      "2021-09-29 21:18:31 | INFO | fairseq.tasks.translation | [en] dictionary: 14800 types\n",
      "2021-09-29 21:18:31 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/bin/valid.es-en.es\n",
      "2021-09-29 21:18:31 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/bin/valid.es-en.en\n",
      "2021-09-29 21:18:31 | INFO | fairseq.tasks.translation | /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/bin/ valid es-en 4643 examples\n",
      "2021-09-29 21:18:32 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(20112, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(14800, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=14800, bias=False)\n",
      "  )\n",
      ")\n",
      "2021-09-29 21:18:32 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n",
      "2021-09-29 21:18:32 | INFO | fairseq_cli.train | model: transformer (TransformerModel)\n",
      "2021-09-29 21:18:32 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
      "2021-09-29 21:18:32 | INFO | fairseq_cli.train | num. model params: 69591040 (num. trained: 69591040)\n",
      "2021-09-29 21:18:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-09-29 21:18:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 7.795 GB ; name = Quadro RTX 4000                         \n",
      "2021-09-29 21:18:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-09-29 21:18:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2021-09-29 21:18:36 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None\n",
      "2021-09-29 21:18:38 | INFO | fairseq.trainer | loaded checkpoint /notebooks/master-thesis/models/es-en/checkpoint_last.pt (epoch 76 @ 0 updates)\n",
      "2021-09-29 21:18:38 | INFO | fairseq.optim.adam | using FusedAdam\n",
      "2021-09-29 21:18:38 | INFO | fairseq.trainer | loading train data for epoch 76\n",
      "2021-09-29 21:18:38 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/bin/train.es-en.es\n",
      "2021-09-29 21:18:38 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/bin/train.es-en.en\n",
      "2021-09-29 21:18:38 | INFO | fairseq.tasks.translation | /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/tokenizers/en-es/bin/ train es-en 197297 examples\n",
      "epoch 076:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:18:38 | INFO | fairseq.trainer | begin training epoch 76\n",
      "epoch 076:  30%|2| 153/516 [00:15<00:35, 10.15it/s, loss=2.038, nll_loss=0.301, 2021-09-29 21:18:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
      "epoch 076: 100%|9| 514/516 [00:50<00:00, 10.14it/s, loss=2.037, nll_loss=0.303, 2021-09-29 21:19:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 076 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 076 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:01,  9.99it/s]\u001b[A\n",
      "epoch 076 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 12.68it/s]\u001b[A\n",
      "epoch 076 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 15.53it/s]\u001b[A\n",
      "epoch 076 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 18.48it/s]\u001b[A\n",
      "epoch 076 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:19:30 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 4.008 | nll_loss 2.468 | ppl 5.53 | wps 89027.9 | wpb 2607.6 | bsz 232.2 | num_updates 515\n",
      "2021-09-29 21:19:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 21:19:37 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint76.pt (epoch 76 @ 515 updates, score 4.008) (writing took 7.4990707950200886 seconds)\n",
      "2021-09-29 21:19:39 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
      "2021-09-29 21:19:39 | INFO | train | epoch 076 | loss 2.046 | nll_loss 0.311 | ppl 1.24 | wps 18434.1 | ups 4.93 | wpb 3737.7 | bsz 381.9 | num_updates 515 | lr 6.4375e-05 | gnorm 0.491 | loss_scale 64 | train_wall 195 | wall 0\n",
      "epoch 077:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:19:39 | INFO | fairseq.trainer | begin training epoch 77\n",
      "epoch 077: 100%|9| 515/516 [00:52<00:00, 10.04it/s, loss=2.032, nll_loss=0.3, pp2021-09-29 21:20:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 077 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 077 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.28it/s]\u001b[A\n",
      "epoch 077 | valid on 'valid' subset:  20%|#4     | 4/20 [00:00<00:01, 10.48it/s]\u001b[A\n",
      "epoch 077 | valid on 'valid' subset:  40%|##8    | 8/20 [00:00<00:00, 13.22it/s]\u001b[A\n",
      "epoch 077 | valid on 'valid' subset:  60%|###6  | 12/20 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "epoch 077 | valid on 'valid' subset:  80%|####8 | 16/20 [00:00<00:00, 18.83it/s]\u001b[A\n",
      "epoch 077 | valid on 'valid' subset: 100%|######| 20/20 [00:00<00:00, 22.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:20:32 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 4.028 | nll_loss 2.483 | ppl 5.59 | wps 84444.9 | wpb 2607.6 | bsz 232.2 | num_updates 1031 | best_loss 4.008\n",
      "2021-09-29 21:20:32 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 077: 100%|9| 515/516 [01:09<00:00, 10.04it/s, loss=2.032, nll_loss=0.3, pp2021-09-29 21:22:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint77.pt (epoch 77 @ 1031 updates, score 4.028) (writing took 132.5335880900966 seconds)\n",
      "2021-09-29 21:22:45 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
      "2021-09-29 21:22:45 | INFO | train | epoch 077 | loss 2.033 | nll_loss 0.3 | ppl 1.23 | wps 10363.6 | ups 2.77 | wpb 3738 | bsz 382.4 | num_updates 1031 | lr 0.000128875 | gnorm 0.46 | loss_scale 64 | train_wall 52 | wall 0\n",
      "epoch 078:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:22:45 | INFO | fairseq.trainer | begin training epoch 78\n",
      "epoch 078: 100%|9| 514/516 [00:52<00:00,  9.97it/s, loss=2.067, nll_loss=0.338, 2021-09-29 21:23:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 078 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 078 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.26it/s]\u001b[A\n",
      "epoch 078 | valid on 'valid' subset:  20%|#4     | 4/20 [00:00<00:01, 11.44it/s]\u001b[A\n",
      "epoch 078 | valid on 'valid' subset:  40%|##8    | 8/20 [00:00<00:00, 14.33it/s]\u001b[A\n",
      "epoch 078 | valid on 'valid' subset:  60%|###6  | 12/20 [00:00<00:00, 17.14it/s]\u001b[A\n",
      "epoch 078 | valid on 'valid' subset:  80%|####8 | 16/20 [00:00<00:00, 20.12it/s]\u001b[A\n",
      "epoch 078 | valid on 'valid' subset: 100%|######| 20/20 [00:00<00:00, 23.43it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:23:38 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 4.057 | nll_loss 2.512 | ppl 5.7 | wps 84740.2 | wpb 2607.6 | bsz 232.2 | num_updates 1547 | best_loss 4.008\n",
      "2021-09-29 21:23:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 21:24:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint78.pt (epoch 78 @ 1547 updates, score 4.057) (writing took 22.14871007797774 seconds)\n",
      "2021-09-29 21:24:00 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
      "2021-09-29 21:24:00 | INFO | train | epoch 078 | loss 2.051 | nll_loss 0.319 | ppl 1.25 | wps 25535.1 | ups 6.83 | wpb 3738 | bsz 382.4 | num_updates 1547 | lr 0.000193375 | gnorm 0.509 | loss_scale 64 | train_wall 51 | wall 0\n",
      "epoch 079:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:24:01 | INFO | fairseq.trainer | begin training epoch 79\n",
      "epoch 079:  45%|4| 233/516 [00:23<00:30,  9.40it/s, loss=2.054, nll_loss=0.321, 2021-09-29 21:24:24 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
      "epoch 079: 100%|9| 514/516 [00:52<00:00, 10.31it/s, loss=2.101, nll_loss=0.372, 2021-09-29 21:24:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 079 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 079 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.32it/s]\u001b[A\n",
      "epoch 079 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 10.80it/s]\u001b[A\n",
      "epoch 079 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "epoch 079 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "epoch 079 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 19.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:24:54 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 4.078 | nll_loss 2.528 | ppl 5.77 | wps 89264.1 | wpb 2607.6 | bsz 232.2 | num_updates 2062 | best_loss 4.008\n",
      "2021-09-29 21:24:54 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 079: 100%|9| 514/516 [01:07<00:00, 10.31it/s, loss=2.101, nll_loss=0.372, 2021-09-29 21:26:08 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint79.pt (epoch 79 @ 2062 updates, score 4.078) (writing took 74.26432995905634 seconds)\n",
      "2021-09-29 21:26:10 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
      "2021-09-29 21:26:10 | INFO | train | epoch 079 | loss 2.08 | nll_loss 0.35 | ppl 1.27 | wps 14900.1 | ups 3.99 | wpb 3738.8 | bsz 382.9 | num_updates 2062 | lr 0.00025775 | gnorm 0.593 | loss_scale 32 | train_wall 52 | wall 0\n",
      "epoch 080:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:26:10 | INFO | fairseq.trainer | begin training epoch 80\n",
      "epoch 080: 100%|9| 514/516 [00:52<00:00, 10.01it/s, loss=2.157, nll_loss=0.43, p2021-09-29 21:27:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 080 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 080 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.44it/s]\u001b[A\n",
      "epoch 080 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 12.09it/s]\u001b[A\n",
      "epoch 080 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.99it/s]\u001b[A\n",
      "epoch 080 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 17.98it/s]\u001b[A\n",
      "epoch 080 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 20.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:27:03 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 4.098 | nll_loss 2.544 | ppl 5.83 | wps 90161.2 | wpb 2607.6 | bsz 232.2 | num_updates 2578 | best_loss 4.008\n",
      "2021-09-29 21:27:03 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 080: 100%|9| 514/516 [01:08<00:00, 10.01it/s, loss=2.157, nll_loss=0.43, p2021-09-29 21:27:24 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint80.pt (epoch 80 @ 2578 updates, score 4.098) (writing took 21.037493698997423 seconds)\n",
      "2021-09-29 21:27:25 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
      "2021-09-29 21:27:25 | INFO | train | epoch 080 | loss 2.113 | nll_loss 0.382 | ppl 1.3 | wps 25668.9 | ups 6.87 | wpb 3738 | bsz 382.4 | num_updates 2578 | lr 0.00032225 | gnorm 0.658 | loss_scale 32 | train_wall 51 | wall 0\n",
      "epoch 081:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:27:25 | INFO | fairseq.trainer | begin training epoch 81\n",
      "epoch 081:  15%|1| 79/516 [00:07<00:43, 10.00it/s, loss=2.133, nll_loss=0.404, p2021-09-29 21:27:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 081: 100%|9| 514/516 [00:52<00:00,  9.39it/s, loss=2.187, nll_loss=0.461, 2021-09-29 21:28:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 081 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 081 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.20it/s]\u001b[A\n",
      "epoch 081 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.77it/s]\u001b[A\n",
      "epoch 081 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.64it/s]\u001b[A\n",
      "epoch 081 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 17.48it/s]\u001b[A\n",
      "epoch 081 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 20.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:28:18 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 4.109 | nll_loss 2.554 | ppl 5.87 | wps 88392.4 | wpb 2607.6 | bsz 232.2 | num_updates 3093 | best_loss 4.008\n",
      "2021-09-29 21:28:18 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 21:29:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint81.pt (epoch 81 @ 3093 updates, score 4.109) (writing took 98.01532284694258 seconds)\n",
      "2021-09-29 21:29:56 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n",
      "2021-09-29 21:29:56 | INFO | train | epoch 081 | loss 2.162 | nll_loss 0.433 | ppl 1.35 | wps 12706.6 | ups 3.4 | wpb 3739.1 | bsz 382.9 | num_updates 3093 | lr 0.000386625 | gnorm 0.794 | loss_scale 16 | train_wall 52 | wall 0\n",
      "epoch 082:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:29:56 | INFO | fairseq.trainer | begin training epoch 82\n",
      "epoch 082: 100%|9| 514/516 [00:52<00:00, 10.40it/s, loss=2.268, nll_loss=0.545, 2021-09-29 21:30:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 082 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 082 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.76it/s]\u001b[A\n",
      "epoch 082 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.33it/s]\u001b[A\n",
      "epoch 082 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.28it/s]\u001b[A\n",
      "epoch 082 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 17.34it/s]\u001b[A\n",
      "epoch 082 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 20.15it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:30:50 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 4.143 | nll_loss 2.579 | ppl 5.97 | wps 91261.7 | wpb 2607.6 | bsz 232.2 | num_updates 3609 | best_loss 4.008\n",
      "2021-09-29 21:30:50 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 082: 100%|9| 514/516 [01:11<00:00, 10.40it/s, loss=2.268, nll_loss=0.545, 2021-09-29 21:31:15 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint82.pt (epoch 82 @ 3609 updates, score 4.143) (writing took 25.29557479301002 seconds)\n",
      "2021-09-29 21:31:15 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n",
      "2021-09-29 21:31:15 | INFO | train | epoch 082 | loss 2.207 | nll_loss 0.479 | ppl 1.39 | wps 24432.1 | ups 6.54 | wpb 3738 | bsz 382.4 | num_updates 3609 | lr 0.000451125 | gnorm 0.844 | loss_scale 16 | train_wall 51 | wall 0\n",
      "epoch 083:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:31:15 | INFO | fairseq.trainer | begin training epoch 83\n",
      "epoch 083:  15%|####6                          | 77/516 [00:07<00:44,  9.94it/s]2021-09-29 21:31:23 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n",
      "epoch 083: 100%|9| 514/516 [00:52<00:00, 10.08it/s, loss=2.333, nll_loss=0.614, 2021-09-29 21:32:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 083 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 083 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:01,  9.81it/s]\u001b[A\n",
      "epoch 083 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 12.43it/s]\u001b[A\n",
      "epoch 083 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 15.31it/s]\u001b[A\n",
      "epoch 083 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 18.15it/s]\u001b[A\n",
      "epoch 083 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:32:09 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 4.14 | nll_loss 2.551 | ppl 5.86 | wps 87825 | wpb 2607.6 | bsz 232.2 | num_updates 4124 | best_loss 4.008\n",
      "2021-09-29 21:32:09 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 083: 100%|9| 514/516 [01:02<00:00, 10.08it/s, loss=2.333, nll_loss=0.614, 2021-09-29 21:33:06 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint83.pt (epoch 83 @ 4124 updates, score 4.14) (writing took 57.273222832009196 seconds)\n",
      "2021-09-29 21:33:06 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n",
      "2021-09-29 21:33:06 | INFO | train | epoch 083 | loss 2.276 | nll_loss 0.552 | ppl 1.47 | wps 17386 | ups 4.65 | wpb 3739.3 | bsz 382.9 | num_updates 4124 | lr 0.000492426 | gnorm 0.951 | loss_scale 8 | train_wall 51 | wall 0\n",
      "epoch 084:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:33:06 | INFO | fairseq.trainer | begin training epoch 84\n",
      "epoch 084: 100%|9| 514/516 [00:52<00:00, 10.27it/s, loss=2.315, nll_loss=0.593, 2021-09-29 21:33:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 084 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 084 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.68it/s]\u001b[A\n",
      "epoch 084 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.19it/s]\u001b[A\n",
      "epoch 084 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.02it/s]\u001b[A\n",
      "epoch 084 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 16.81it/s]\u001b[A\n",
      "epoch 084 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 19.82it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:34:00 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 4.122 | nll_loss 2.543 | ppl 5.83 | wps 88810 | wpb 2607.6 | bsz 232.2 | num_updates 4640 | best_loss 4.008\n",
      "2021-09-29 21:34:00 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 21:34:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint84.pt (epoch 84 @ 4640 updates, score 4.122) (writing took 18.53379924199544 seconds)\n",
      "2021-09-29 21:34:18 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n",
      "2021-09-29 21:34:18 | INFO | train | epoch 084 | loss 2.282 | nll_loss 0.555 | ppl 1.47 | wps 26794 | ups 7.17 | wpb 3738 | bsz 382.4 | num_updates 4640 | lr 0.000464238 | gnorm 0.923 | loss_scale 8 | train_wall 51 | wall 0\n",
      "epoch 085:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:34:18 | INFO | fairseq.trainer | begin training epoch 85\n",
      "epoch 085:  27%|2| 141/516 [00:14<00:38,  9.71it/s, loss=2.277, nll_loss=0.547, 2021-09-29 21:34:33 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4.0\n",
      "epoch 085: 100%|9| 515/516 [00:52<00:00, 10.25it/s, loss=2.27, nll_loss=0.543, p2021-09-29 21:35:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 085 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 085 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.88it/s]\u001b[A\n",
      "epoch 085 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.33it/s]\u001b[A\n",
      "epoch 085 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.23it/s]\u001b[A\n",
      "epoch 085 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 17.10it/s]\u001b[A\n",
      "epoch 085 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 20.08it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:35:11 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 4.086 | nll_loss 2.497 | ppl 5.65 | wps 87929.4 | wpb 2607.6 | bsz 232.2 | num_updates 5155 | best_loss 4.008\n",
      "2021-09-29 21:35:11 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 085: 100%|9| 515/516 [01:10<00:00, 10.25it/s, loss=2.27, nll_loss=0.543, p2021-09-29 21:36:33 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint85.pt (epoch 85 @ 5155 updates, score 4.086) (writing took 81.62184824503493 seconds)\n",
      "2021-09-29 21:36:33 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n",
      "2021-09-29 21:36:33 | INFO | train | epoch 085 | loss 2.266 | nll_loss 0.536 | ppl 1.45 | wps 14284.4 | ups 3.82 | wpb 3737.5 | bsz 382.2 | num_updates 5155 | lr 0.000440439 | gnorm 0.879 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 086:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:36:33 | INFO | fairseq.trainer | begin training epoch 86\n",
      "epoch 086: 100%|9| 514/516 [00:51<00:00, 10.25it/s, loss=2.265, nll_loss=0.54, p2021-09-29 21:37:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 086 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 086 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.83it/s]\u001b[A\n",
      "epoch 086 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.32it/s]\u001b[A\n",
      "epoch 086 | valid on 'valid' subset:  40%|##8    | 8/20 [00:00<00:00, 13.58it/s]\u001b[A\n",
      "epoch 086 | valid on 'valid' subset:  55%|###3  | 11/20 [00:00<00:00, 15.53it/s]\u001b[A\n",
      "epoch 086 | valid on 'valid' subset:  70%|####1 | 14/20 [00:00<00:00, 17.73it/s]\u001b[A\n",
      "epoch 086 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 19.74it/s]\u001b[A\n",
      "epoch 086 | valid on 'valid' subset: 100%|######| 20/20 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:37:26 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 4.082 | nll_loss 2.493 | ppl 5.63 | wps 71195.6 | wpb 2607.6 | bsz 232.2 | num_updates 5671 | best_loss 4.008\n",
      "2021-09-29 21:37:26 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 086: 100%|9| 514/516 [01:05<00:00, 10.25it/s, loss=2.265, nll_loss=0.54, p2021-09-29 21:37:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint86.pt (epoch 86 @ 5671 updates, score 4.082) (writing took 18.863064098986797 seconds)\n",
      "2021-09-29 21:37:46 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n",
      "2021-09-29 21:37:46 | INFO | train | epoch 086 | loss 2.25 | nll_loss 0.52 | ppl 1.43 | wps 26275.4 | ups 7.03 | wpb 3738 | bsz 382.4 | num_updates 5671 | lr 0.000419923 | gnorm 0.841 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 087:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:37:46 | INFO | fairseq.trainer | begin training epoch 87\n",
      "epoch 087: 100%|9| 515/516 [00:52<00:00,  9.99it/s, loss=2.255, nll_loss=0.527, 2021-09-29 21:38:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 087 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 087 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.34it/s]\u001b[A\n",
      "epoch 087 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 12.00it/s]\u001b[A\n",
      "epoch 087 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.95it/s]\u001b[A\n",
      "epoch 087 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 17.81it/s]\u001b[A\n",
      "epoch 087 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 20.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:38:39 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 4.063 | nll_loss 2.477 | ppl 5.57 | wps 90625.5 | wpb 2607.6 | bsz 232.2 | num_updates 6187 | best_loss 4.008\n",
      "2021-09-29 21:38:39 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 087: 100%|9| 515/516 [01:11<00:00,  9.99it/s, loss=2.255, nll_loss=0.527, 2021-09-29 21:39:51 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint87.pt (epoch 87 @ 6187 updates, score 4.063) (writing took 71.87314744305331 seconds)\n",
      "2021-09-29 21:39:51 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n",
      "2021-09-29 21:39:51 | INFO | train | epoch 087 | loss 2.238 | nll_loss 0.507 | ppl 1.42 | wps 15403.8 | ups 4.12 | wpb 3738 | bsz 382.4 | num_updates 6187 | lr 0.000402031 | gnorm 0.828 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 088:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:39:52 | INFO | fairseq.trainer | begin training epoch 88\n",
      "epoch 088: 100%|9| 514/516 [00:51<00:00,  9.93it/s, loss=2.23, nll_loss=0.502, p2021-09-29 21:40:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 088 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 088 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.34it/s]\u001b[A\n",
      "epoch 088 | valid on 'valid' subset:  20%|#4     | 4/20 [00:00<00:01, 11.49it/s]\u001b[A\n",
      "epoch 088 | valid on 'valid' subset:  40%|##8    | 8/20 [00:00<00:00, 14.45it/s]\u001b[A\n",
      "epoch 088 | valid on 'valid' subset:  60%|###6  | 12/20 [00:00<00:00, 17.39it/s]\u001b[A\n",
      "epoch 088 | valid on 'valid' subset:  80%|####8 | 16/20 [00:00<00:00, 20.22it/s]\u001b[A\n",
      "epoch 088 | valid on 'valid' subset: 100%|######| 20/20 [00:00<00:00, 23.53it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:40:44 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 4.043 | nll_loss 2.471 | ppl 5.54 | wps 85004.2 | wpb 2607.6 | bsz 232.2 | num_updates 6703 | best_loss 4.008\n",
      "2021-09-29 21:40:44 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 21:41:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint88.pt (epoch 88 @ 6703 updates, score 4.043) (writing took 17.04048386006616 seconds)\n",
      "2021-09-29 21:41:02 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n",
      "2021-09-29 21:41:02 | INFO | train | epoch 088 | loss 2.216 | nll_loss 0.483 | ppl 1.4 | wps 27475.5 | ups 7.35 | wpb 3738 | bsz 382.4 | num_updates 6703 | lr 0.000386247 | gnorm 0.78 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 089:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:41:02 | INFO | fairseq.trainer | begin training epoch 89\n",
      "epoch 089: 100%|9| 515/516 [00:52<00:00,  9.76it/s, loss=2.216, nll_loss=0.487, 2021-09-29 21:41:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 089 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 089 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.53it/s]\u001b[A\n",
      "epoch 089 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.05it/s]\u001b[A\n",
      "epoch 089 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 13.85it/s]\u001b[A\n",
      "epoch 089 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 16.74it/s]\u001b[A\n",
      "epoch 089 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 19.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:41:55 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 4.099 | nll_loss 2.53 | ppl 5.78 | wps 88379.4 | wpb 2607.6 | bsz 232.2 | num_updates 7219 | best_loss 4.008\n",
      "2021-09-29 21:41:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 21:43:09 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint89.pt (epoch 89 @ 7219 updates, score 4.099) (writing took 73.85007873305585 seconds)\n",
      "2021-09-29 21:43:09 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n",
      "2021-09-29 21:43:09 | INFO | train | epoch 089 | loss 2.205 | nll_loss 0.472 | ppl 1.39 | wps 15154.9 | ups 4.05 | wpb 3738 | bsz 382.4 | num_updates 7219 | lr 0.000372187 | gnorm 0.769 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 090:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:43:09 | INFO | fairseq.trainer | begin training epoch 90\n",
      "epoch 090: 100%|9| 514/516 [00:51<00:00, 10.32it/s, loss=2.201, nll_loss=0.471, 2021-09-29 21:44:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 090 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 090 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.91it/s]\u001b[A\n",
      "epoch 090 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.48it/s]\u001b[A\n",
      "epoch 090 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.41it/s]\u001b[A\n",
      "epoch 090 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 17.30it/s]\u001b[A\n",
      "epoch 090 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 20.32it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:44:01 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 4.104 | nll_loss 2.536 | ppl 5.8 | wps 90874.1 | wpb 2607.6 | bsz 232.2 | num_updates 7735 | best_loss 4.008\n",
      "2021-09-29 21:44:01 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 090: 100%|9| 514/516 [01:09<00:00, 10.32it/s, loss=2.201, nll_loss=0.471, 2021-09-29 21:45:30 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint90.pt (epoch 90 @ 7735 updates, score 4.104) (writing took 88.28068394691218 seconds)\n",
      "2021-09-29 21:45:30 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n",
      "2021-09-29 21:45:30 | INFO | train | epoch 090 | loss 2.194 | nll_loss 0.461 | ppl 1.38 | wps 13713 | ups 3.67 | wpb 3738 | bsz 382.4 | num_updates 7735 | lr 0.000359559 | gnorm 0.777 | loss_scale 4 | train_wall 50 | wall 0\n",
      "epoch 091:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:45:30 | INFO | fairseq.trainer | begin training epoch 91\n",
      "epoch 091: 100%|9| 515/516 [00:51<00:00, 10.07it/s, loss=2.191, nll_loss=0.46, p2021-09-29 21:46:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 091 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 091 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.09it/s]\u001b[A\n",
      "epoch 091 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 10.45it/s]\u001b[A\n",
      "epoch 091 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 13.22it/s]\u001b[A\n",
      "epoch 091 | valid on 'valid' subset:  60%|###6  | 12/20 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "epoch 091 | valid on 'valid' subset:  80%|####8 | 16/20 [00:00<00:00, 18.75it/s]\u001b[A\n",
      "epoch 091 | valid on 'valid' subset: 100%|######| 20/20 [00:00<00:00, 22.08it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:46:22 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 4.045 | nll_loss 2.466 | ppl 5.53 | wps 86809.7 | wpb 2607.6 | bsz 232.2 | num_updates 8251 | best_loss 4.008\n",
      "2021-09-29 21:46:22 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 091: 100%|9| 515/516 [01:08<00:00, 10.07it/s, loss=2.191, nll_loss=0.46, p2021-09-29 21:46:39 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint91.pt (epoch 91 @ 8251 updates, score 4.045) (writing took 17.31183288199827 seconds)\n",
      "2021-09-29 21:46:39 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n",
      "2021-09-29 21:46:39 | INFO | train | epoch 091 | loss 2.181 | nll_loss 0.447 | ppl 1.36 | wps 27635.5 | ups 7.39 | wpb 3738 | bsz 382.4 | num_updates 8251 | lr 0.000348134 | gnorm 0.729 | loss_scale 4 | train_wall 50 | wall 0\n",
      "epoch 092:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:46:39 | INFO | fairseq.trainer | begin training epoch 92\n",
      "epoch 092: 100%|9| 514/516 [00:52<00:00,  9.92it/s, loss=2.187, nll_loss=0.456, 2021-09-29 21:47:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 092 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 092 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.22it/s]\u001b[A\n",
      "epoch 092 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.77it/s]\u001b[A\n",
      "epoch 092 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.62it/s]\u001b[A\n",
      "epoch 092 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 17.53it/s]\u001b[A\n",
      "epoch 092 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 20.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:47:33 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 4.032 | nll_loss 2.457 | ppl 5.49 | wps 87744.2 | wpb 2607.6 | bsz 232.2 | num_updates 8767 | best_loss 4.008\n",
      "2021-09-29 21:47:33 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 21:48:44 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint92.pt (epoch 92 @ 8767 updates, score 4.032) (writing took 70.65572621999308 seconds)\n",
      "2021-09-29 21:48:44 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n",
      "2021-09-29 21:48:44 | INFO | train | epoch 092 | loss 2.172 | nll_loss 0.438 | ppl 1.35 | wps 15507.1 | ups 4.15 | wpb 3738 | bsz 382.4 | num_updates 8767 | lr 0.000337734 | gnorm 0.747 | loss_scale 4 | train_wall 52 | wall 0\n",
      "epoch 093:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:48:44 | INFO | fairseq.trainer | begin training epoch 93\n",
      "epoch 093: 100%|9| 514/516 [00:51<00:00,  9.84it/s, loss=2.186, nll_loss=0.456, 2021-09-29 21:49:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 093 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.33it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  20%|#4     | 4/20 [00:00<00:01, 11.58it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  35%|##4    | 7/20 [00:00<00:00, 13.90it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  55%|###3  | 11/20 [00:00<00:00, 16.66it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  75%|####5 | 15/20 [00:00<00:00, 19.59it/s]\u001b[A\n",
      "epoch 093 | valid on 'valid' subset:  95%|#####6| 19/20 [00:00<00:00, 22.45it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:49:37 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 4.032 | nll_loss 2.458 | ppl 5.5 | wps 80136.5 | wpb 2607.6 | bsz 232.2 | num_updates 9283 | best_loss 4.008\n",
      "2021-09-29 21:49:37 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 21:50:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint93.pt (epoch 93 @ 9283 updates, score 4.032) (writing took 23.742149965954013 seconds)\n",
      "2021-09-29 21:50:00 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n",
      "2021-09-29 21:50:00 | INFO | train | epoch 093 | loss 2.165 | nll_loss 0.431 | ppl 1.35 | wps 25170.1 | ups 6.73 | wpb 3738 | bsz 382.4 | num_updates 9283 | lr 0.000328213 | gnorm 0.72 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 094:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:50:00 | INFO | fairseq.trainer | begin training epoch 94\n",
      "epoch 094: 100%|9| 514/516 [00:53<00:00, 10.19it/s, loss=2.161, nll_loss=0.431, 2021-09-29 21:50:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 094 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.15it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  20%|#4     | 4/20 [00:00<00:01, 10.38it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  40%|##8    | 8/20 [00:00<00:00, 13.13it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  60%|###6  | 12/20 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset:  80%|####8 | 16/20 [00:00<00:00, 19.00it/s]\u001b[A\n",
      "epoch 094 | valid on 'valid' subset: 100%|######| 20/20 [00:00<00:00, 22.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:50:55 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 4.057 | nll_loss 2.496 | ppl 5.64 | wps 87559.7 | wpb 2607.6 | bsz 232.2 | num_updates 9799 | best_loss 4.008\n",
      "2021-09-29 21:50:55 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 094: 100%|9| 514/516 [01:08<00:00, 10.19it/s, loss=2.161, nll_loss=0.431, 2021-09-29 21:51:27 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint94.pt (epoch 94 @ 9799 updates, score 4.057) (writing took 32.10178414999973 seconds)\n",
      "2021-09-29 21:51:29 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n",
      "2021-09-29 21:51:29 | INFO | train | epoch 094 | loss 2.153 | nll_loss 0.42 | ppl 1.34 | wps 21890.6 | ups 5.86 | wpb 3738 | bsz 382.4 | num_updates 9799 | lr 0.000319455 | gnorm 0.699 | loss_scale 4 | train_wall 53 | wall 0\n",
      "epoch 095:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:51:29 | INFO | fairseq.trainer | begin training epoch 95\n",
      "epoch 095: 100%|9| 515/516 [00:52<00:00,  9.84it/s, loss=2.163, nll_loss=0.435, 2021-09-29 21:52:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 095 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  10%|7      | 2/20 [00:00<00:01, 15.39it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  30%|##1    | 6/20 [00:00<00:00, 18.47it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  50%|###   | 10/20 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  70%|####1 | 14/20 [00:00<00:00, 23.84it/s]\u001b[A\n",
      "epoch 095 | valid on 'valid' subset:  90%|#####4| 18/20 [00:00<00:00, 26.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:52:22 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 4.069 | nll_loss 2.503 | ppl 5.67 | wps 89113.4 | wpb 2607.6 | bsz 232.2 | num_updates 10315 | best_loss 4.008\n",
      "2021-09-29 21:52:22 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 21:53:51 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint95.pt (epoch 95 @ 10315 updates, score 4.069) (writing took 88.98445799795445 seconds)\n",
      "2021-09-29 21:53:51 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n",
      "2021-09-29 21:53:51 | INFO | train | epoch 095 | loss 2.145 | nll_loss 0.411 | ppl 1.33 | wps 13568.4 | ups 3.63 | wpb 3738 | bsz 382.4 | num_updates 10315 | lr 0.000311362 | gnorm 0.682 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 096:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:53:51 | INFO | fairseq.trainer | begin training epoch 96\n",
      "epoch 096: 100%|9| 515/516 [00:51<00:00, 10.14it/s, loss=2.151, nll_loss=0.42, p2021-09-29 21:54:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 096 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.27it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.87it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.81it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 17.74it/s]\u001b[A\n",
      "epoch 096 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 20.66it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:54:43 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 4.057 | nll_loss 2.486 | ppl 5.6 | wps 90122.3 | wpb 2607.6 | bsz 232.2 | num_updates 10831 | best_loss 4.008\n",
      "2021-09-29 21:54:43 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 096: 100%|9| 515/516 [01:08<00:00, 10.14it/s, loss=2.151, nll_loss=0.42, p2021-09-29 21:55:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint96.pt (epoch 96 @ 10831 updates, score 4.057) (writing took 16.86113229498733 seconds)\n",
      "2021-09-29 21:55:00 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n",
      "2021-09-29 21:55:00 | INFO | train | epoch 096 | loss 2.139 | nll_loss 0.406 | ppl 1.32 | wps 27757.2 | ups 7.43 | wpb 3738 | bsz 382.4 | num_updates 10831 | lr 0.000303855 | gnorm 0.68 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 097:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:55:00 | INFO | fairseq.trainer | begin training epoch 97\n",
      "epoch 097: 100%|9| 514/516 [00:52<00:00, 10.26it/s, loss=2.163, nll_loss=0.434, 2021-09-29 21:55:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 097 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.49it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 12.12it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  35%|##4    | 7/20 [00:00<00:00, 13.23it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  55%|###3  | 11/20 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  75%|####5 | 15/20 [00:00<00:00, 18.84it/s]\u001b[A\n",
      "epoch 097 | valid on 'valid' subset:  95%|#####6| 19/20 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:55:53 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 4.046 | nll_loss 2.485 | ppl 5.6 | wps 79035.6 | wpb 2607.6 | bsz 232.2 | num_updates 11347 | best_loss 4.008\n",
      "2021-09-29 21:55:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 097: 100%|9| 514/516 [01:08<00:00, 10.26it/s, loss=2.163, nll_loss=0.434, 2021-09-29 21:57:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint97.pt (epoch 97 @ 11347 updates, score 4.046) (writing took 102.20519738597795 seconds)\n",
      "2021-09-29 21:57:36 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n",
      "2021-09-29 21:57:36 | INFO | train | epoch 097 | loss 2.132 | nll_loss 0.398 | ppl 1.32 | wps 12408.4 | ups 3.32 | wpb 3738 | bsz 382.4 | num_updates 11347 | lr 0.000296865 | gnorm 0.665 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 098:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:57:36 | INFO | fairseq.trainer | begin training epoch 98\n",
      "epoch 098: 100%|9| 515/516 [00:51<00:00, 10.46it/s, loss=2.138, nll_loss=0.407, 2021-09-29 21:58:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 098 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:01,  9.53it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 12.16it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  35%|##4    | 7/20 [00:00<00:00, 13.47it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  55%|###3  | 11/20 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  75%|####5 | 15/20 [00:00<00:00, 19.17it/s]\u001b[A\n",
      "epoch 098 | valid on 'valid' subset:  95%|#####6| 19/20 [00:00<00:00, 22.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 21:58:28 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 4.079 | nll_loss 2.515 | ppl 5.72 | wps 80743.3 | wpb 2607.6 | bsz 232.2 | num_updates 11863 | best_loss 4.008\n",
      "2021-09-29 21:58:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "epoch 098: 100%|9| 515/516 [01:03<00:00, 10.46it/s, loss=2.138, nll_loss=0.407, 2021-09-29 21:59:40 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint98.pt (epoch 98 @ 11863 updates, score 4.079) (writing took 71.4197247789707 seconds)\n",
      "2021-09-29 21:59:40 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n",
      "2021-09-29 21:59:40 | INFO | train | epoch 098 | loss 2.122 | nll_loss 0.389 | ppl 1.31 | wps 15541.5 | ups 4.16 | wpb 3738 | bsz 382.4 | num_updates 11863 | lr 0.000290337 | gnorm 0.647 | loss_scale 4 | train_wall 51 | wall 0\n",
      "epoch 099:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 21:59:40 | INFO | fairseq.trainer | begin training epoch 99\n",
      "epoch 099: 100%|9| 514/516 [00:53<00:00,  9.62it/s, loss=2.119, nll_loss=0.389, 2021-09-29 22:00:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 099 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  9.04it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.54it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.48it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  60%|###6  | 12/20 [00:00<00:00, 17.11it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset:  80%|####8 | 16/20 [00:00<00:00, 19.91it/s]\u001b[A\n",
      "epoch 099 | valid on 'valid' subset: 100%|######| 20/20 [00:00<00:00, 23.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 22:00:34 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 4.096 | nll_loss 2.543 | ppl 5.83 | wps 86888.4 | wpb 2607.6 | bsz 232.2 | num_updates 12379 | best_loss 4.008\n",
      "2021-09-29 22:00:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-09-29 22:00:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /notebooks/master-thesis/models/es-en/checkpoint99.pt (epoch 99 @ 12379 updates, score 4.096) (writing took 16.430941144004464 seconds)\n",
      "2021-09-29 22:00:50 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n",
      "2021-09-29 22:00:50 | INFO | train | epoch 099 | loss 2.117 | nll_loss 0.383 | ppl 1.3 | wps 27323.9 | ups 7.31 | wpb 3738 | bsz 382.4 | num_updates 12379 | lr 0.000284222 | gnorm 0.644 | loss_scale 4 | train_wall 52 | wall 0\n",
      "epoch 100:   0%|                                        | 0/516 [00:00<?, ?it/s]2021-09-29 22:00:50 | INFO | fairseq.trainer | begin training epoch 100\n",
      "epoch 100: 100%|9| 515/516 [00:54<00:00,  9.69it/s, loss=2.128, nll_loss=0.399, 2021-09-29 22:01:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "\n",
      "epoch 100 | valid on 'valid' subset:   0%|               | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:   5%|3      | 1/20 [00:00<00:02,  8.76it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  25%|#7     | 5/20 [00:00<00:01, 11.24it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  45%|###1   | 9/20 [00:00<00:00, 14.07it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  65%|###9  | 13/20 [00:00<00:00, 16.89it/s]\u001b[A\n",
      "epoch 100 | valid on 'valid' subset:  85%|#####1| 17/20 [00:00<00:00, 19.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-09-29 22:01:45 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 4.071 | nll_loss 2.511 | ppl 5.7 | wps 87458.1 | wpb 2607.6 | bsz 232.2 | num_updates 12895 | best_loss 4.008\n",
      "2021-09-29 22:01:45 | INFO | fairseq_cli.train | begin save checkpoint\n"
     ]
    }
   ],
   "source": [
    "! fairseq-train $DEST_DIR/bin/ \\\n",
    "    --source-lang es --target-lang en \\\n",
    "    --arch=transformer \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --restore-file $MODEL_DIR/checkpoint_last.pt \\\n",
    "    --save-dir $MODEL_DIR \\\n",
    "    --keep-last-epochs 3 \\\n",
    "    --reset-optimizer \\\n",
    "    --fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd master-thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip models-20210929T123614Z-003.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm models-20210929T123614Z-003.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
