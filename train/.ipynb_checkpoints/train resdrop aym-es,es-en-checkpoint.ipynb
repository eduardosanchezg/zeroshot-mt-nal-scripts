{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## ENVIRONMENT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "EN_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "NAH_ES_CORPUS_DIR = \"/notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_path = \"/storage/master-thesis/models/resdrop__nah-es_es-en/tokenizers/nah-es-en/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZER_PATH=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZER_PATH = /storage/master-thesis/models/resdrop__nah-es_es-en/tokenizers/nah-es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "tokenized_path_es_en = \"/storage/master-thesis/models/resdrop__nah-es_es-en/tokenizers/nah-es-en/es-en/\"\n",
    "tokenized_path_nah_es = \"/storage/master-thesis/models/resdrop__nah-es_es-en/tokenizers/nah-es-en/nah-es/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZED_PATH_ES_EN=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en\n",
      "env: TOKENIZED_PATH_QUY_ES=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZED_PATH_ES_EN = /storage/master-thesis/models/resdrop__nah-es_es-en/tokenizers/nah-es-en/es-en\n",
    "%env TOKENIZED_PATH_NAH_ES = /storage/master-thesis/models/resdrop__nah-es_es-en/tokenizers/nah-es-en/nah-es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BIN_DIR=/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "%env BIN_DIR = /storage/master-thesis/models/resdrop__nah-es_es-en/tokenizers/nah-es-en/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $TOKENIZED_PATH_ES_EN\n",
    "! mkdir -p $TOKENIZED_PATH_NAH_ES\n",
    "! mkdir -p $BIN_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_DIR=/storage/master-thesis/models/resdrop_quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_DIR = /storage/master-thesis/models/resdrop__nah-es_es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CODE_STORAGE=/storage/code/\n"
     ]
    }
   ],
   "source": [
    "%env CODE_STORAGE = /storage/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locale: Cannot set LC_CTYPE to default locale: No such file or directory\n",
      "locale: Cannot set LC_MESSAGES to default locale: No such file or directory\n",
      "locale: Cannot set LC_ALL to default locale: No such file or directory\n",
      "LANG=en_US.UTF-8\n",
      "LANGUAGE=\n",
      "LC_CTYPE=\"en_US.UTF-8\"\n",
      "LC_NUMERIC=\"en_US.UTF-8\"\n",
      "LC_TIME=\"en_US.UTF-8\"\n",
      "LC_COLLATE=\"en_US.UTF-8\"\n",
      "LC_MONETARY=\"en_US.UTF-8\"\n",
      "LC_MESSAGES=\"en_US.UTF-8\"\n",
      "LC_PAPER=\"en_US.UTF-8\"\n",
      "LC_NAME=\"en_US.UTF-8\"\n",
      "LC_ADDRESS=\"en_US.UTF-8\"\n",
      "LC_TELEPHONE=\"en_US.UTF-8\"\n",
      "LC_MEASUREMENT=\"en_US.UTF-8\"\n",
      "LC_IDENTIFICATION=\"en_US.UTF-8\"\n",
      "LC_ALL=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "! locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: update-locale: not found\n",
      "env: LANG=en_US.UTF-8\n",
      "env: LC_CTYPE=en_US.UTF-8\n",
      "env: LC_ALL=en_US.UTF-8\n"
     ]
    }
   ],
   "source": [
    "! update-locale LANG=en_US.UTF-8 LANGUAGE=en.UTF-8\n",
    "\n",
    "%env LANG=en_US.UTF-8\n",
    "%env LC_CTYPE=en_US.UTF-8\n",
    "%env LC_ALL=en_US.UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /etc/rc.conf: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! cat /etc/rc.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.10.3 in /usr/local/lib/python3.6/dist-packages (0.10.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers==0.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall torch torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall fairseq -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (0.10.2)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.8)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (2.0.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.6.0)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.24)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.51.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2020.11.13)\n",
      "Requirement already satisfied: hydra-core in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.4)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.3.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.18.2)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (5.2.2)\n",
      "Requirement already satisfied: omegaconf==2.1.* in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (2.1.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (4.8)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq) (5.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/NVIDIA/apex\n",
    "\n",
    "! pip install -v --disable-pip-version-check --no-cache-dir apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tatoeba format to standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(EN_ES_CORPUS_DIR + \"original/valid.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"dev.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"dev.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "    en.close()\n",
    "    es.close()\n",
    "    \n",
    "with open(EN_ES_CORPUS_DIR + \"original/dev.txt\", encoding='utf8') as f:\n",
    "    es = open(EN_ES_CORPUS_DIR + \"train.es\", 'w', encoding='utf8')\n",
    "    en = open(EN_ES_CORPUS_DIR + \"train.en\", 'w', encoding='utf8')\n",
    "    for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "        if \"\\n\" in line[2] or \"\\n\" in line[3]:\n",
    "            continue\n",
    "        if len(line) != 4:\n",
    "            continue\n",
    "            \n",
    "        es.write(line[3] + \"\\n\")\n",
    "        en.write(line[2] + \"\\n\")\n",
    "        \n",
    "    en.close()\n",
    "    es.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "en_es_nah_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "en_es_nah_trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "en_es_nah_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "en_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.en\" for split in [\"dev\", \"train\"]]\n",
    "es_files = [f\"/notebooks/master-thesis/corpora/tatoeba-challenge/en-es/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "nah_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish/{split}.nah\" for split in [\"dev\", \"train\"]]\n",
    "es2_files = [f\"/notebooks/master-thesis/corpora/americasnlp2021/data/nahuatl-spanish/{split}.es\" for split in [\"dev\", \"train\"]]\n",
    "\n",
    "files = en_files + es_files + es2_files + nah_files\n",
    "#print(files)\n",
    "en_es_nah_tokenizer.train(files= files, trainer=en_es_nah_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_files(tokenizer, files, extension, output_path):\n",
    "    for file in files:\n",
    "        print(f\"Reading file {file}\")\n",
    "        with open(file, encoding='utf8') as f:\n",
    "          lines = f.readlines()\n",
    "          tokenized_lines = tokenizer.encode_batch(lines)\n",
    "          tokenized_name = Path(file).stem\n",
    "          tokenized_name = output_path + tokenized_name + \".\" + extension\n",
    "          print(tokenized_name)\n",
    "          with open(tokenized_name, 'w', encoding='utf8') as wf:\n",
    "\n",
    "            wf.writelines([\" \".join(t.tokens) + \"\\n\" for t in tokenized_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "en_es_nah_tokenizer.save(tokenizer_path + \"nah-es-en-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tokenizers.Tokenizer object at 0x28a09a0>\n"
     ]
    }
   ],
   "source": [
    "print(en_es_nah_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'hold', 'this', 'truth', 'to', 'be', 'self', 'evid', '##ent', 'that', 'everyone', 'is', 'created', 'equal', '?']\n"
     ]
    }
   ],
   "source": [
    "tok = en_es_nah_tokenizer.encode(\"we hold this truth to be self evident that everyone is created equal?\")\n",
    "print(tok.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.en\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.en\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.en\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/dev.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/tatoeba-challenge/en-es/train.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.quy\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dict.quy\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dict.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.quy\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.quy\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dev.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/dict.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dict.es\n",
      "Reading file /notebooks/master-thesis/corpora/americasnlp2021/data/quechua-spanish/train.es\n",
      "/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.es\n"
     ]
    }
   ],
   "source": [
    "tokenize_files(en_es_nah_tokenizer, en_files, \"en\", tokenized_path_es_en)\n",
    "tokenize_files(en_es_nah_tokenizer, es_files, \"es\", tokenized_path_es_en)\n",
    "\n",
    "tokenize_files(en_es_nah_tokenizer, nah_files, \"nah\", tokenized_path_nah_es)\n",
    "tokenize_files(en_es_nah_tokenizer, es2_files, \"es\", tokenized_path_nah_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.quy.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.es.txt': No such file or directory\n",
      "rm: cannot remove '/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.en.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "### Removing previous dict files\n",
    "! rm $BIN_DIR/dict.nah.txt\n",
    "! rm $BIN_DIR/dict.es.txt\n",
    "! rm $BIN_DIR/dict.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "## Concatenating all training data\n",
    "! cat $TOKENIZED_PATH_NAH_ES/train.nah $TOKENIZED_PATH_NAH_ES/train.es $TOKENIZED_PATH_ES_EN/train.es $TOKENIZED_PATH_ES_EN/train.en > $BIN_DIR/train.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:44:27 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='all', srcdict=None, target_lang=None, task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train', user_dir=None, validpref=None, workers=20)\n",
      "2021-10-07 17:44:34 | INFO | fairseq_cli.preprocess | [all] Dictionary: 28976 types\n",
      "2021-10-07 17:44:48 | INFO | fairseq_cli.preprocess | [all] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.all: 644610 sents, 8263690 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:44:48 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang all \\\n",
    "    --trainpref $BIN_DIR/train \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --workers 20 \\\n",
    "    --only-source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:46:10 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='quy', srcdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', target_lang='es', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train', user_dir=None, validpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev', workers=20)\n",
      "2021-10-07 17:46:10 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 28976 types\n",
      "2021-10-07 17:46:13 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.quy: 125008 sents, 1922581 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:13 | INFO | fairseq_cli.preprocess | [quy] Dictionary: 28976 types\n",
      "2021-10-07 17:46:14 | INFO | fairseq_cli.preprocess | [quy] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.quy: 996 sents, 14488 tokens, 0.456% replaced by <unk>\n",
      "2021-10-07 17:46:14 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:18 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/train.es: 125008 sents, 2452298 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:18 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:19 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/quy-es/dev.es: 996 sents, 15303 tokens, 0.242% replaced by <unk>\n",
      "2021-10-07 17:46:19 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang nah --target-lang es \\\n",
    "    --trainpref $TOKENIZED_PATH_NAH_ES/train --validpref $TOKENIZED_PATH_NAH_ES/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing es dict from quy-es preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-07 17:46:28 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='es', srcdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/dict.all.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train', user_dir=None, validpref='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev', workers=20)\n",
      "2021-10-07 17:46:28 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:33 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.es: 197297 sents, 1894674 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:33 | INFO | fairseq_cli.preprocess | [es] Dictionary: 28976 types\n",
      "2021-10-07 17:46:34 | INFO | fairseq_cli.preprocess | [es] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.es: 4643 sents, 51839 tokens, 0.0405% replaced by <unk>\n",
      "2021-10-07 17:46:34 | INFO | fairseq_cli.preprocess | [en] Dictionary: 28976 types\n",
      "2021-10-07 17:46:38 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/train.en: 197297 sents, 1994137 tokens, 0.0% replaced by <unk>\n",
      "2021-10-07 17:46:38 | INFO | fairseq_cli.preprocess | [en] Dictionary: 28976 types\n",
      "2021-10-07 17:46:39 | INFO | fairseq_cli.preprocess | [en] /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/es-en/dev.en: 4643 sents, 54416 tokens, 0.0404% replaced by <unk>\n",
      "2021-10-07 17:46:39 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin\n"
     ]
    }
   ],
   "source": [
    "! fairseq-preprocess --source-lang es --target-lang en \\\n",
    "    --trainpref $TOKENIZED_PATH_ES_EN/train --validpref $TOKENIZED_PATH_ES_EN/dev \\\n",
    "    --destdir $BIN_DIR \\\n",
    "    --srcdict $BIN_DIR/dict.all.txt \\\n",
    "    --tgtdict $BIN_DIR/dict.all.txt \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage\n"
     ]
    }
   ],
   "source": [
    "cd $CODE_STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fairseq'...\n",
      "remote: Enumerating objects: 21955, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 21955 (delta 0), reused 4 (delta 0), pack-reused 21948\u001b[K\n",
      "Receiving objects: 100% (21955/21955), 9.90 MiB | 22.14 MiB/s, done.\n",
      "Resolving deltas: 100% (16366/16366), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/dannigt/fairseq.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-08 10:24:07 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='residual_drop_transformer', attention_dropout=0.0, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_langtok=True, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_lang_ids=False, enable_reservsed_directions_shared_datasets=False, encoder_attention_heads=8, encoder_drop_residual=2, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_langtok='src', encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, extra_data=None, extra_lang_pairs=None, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_dictionary=None, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_inference_langtok=False, keep_interval_updates=-1, keep_last_epochs=2, label_smoothing=0.1, lang_dict=None, lang_pairs='quy-es,es-en', lang_tok_replacing_bos_eos=False, lang_tok_style='multilingual', langs=None, langtoks=None, langtoks_specs=['main'], layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='/storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint_last.pt', sampling_method='concat', sampling_temperature=1.5, sampling_weights=None, sampling_weights_from_file=None, save_dir='/storage/master-thesis/models/resdrop_quy-es+es-en/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation_multi_simple_epoch', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir='/storage/code//fairseq/examples/residual_drop/residual_drop_src/', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, virtual_data_size=None, virtual_epoch_size=1000000, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')\n",
      "2021-10-08 10:24:07 | WARNING | fairseq.data.multilingual.multilingual_data_manager | External language dictionary is not provided; use lang-pairs to infer the set of supported languages. The language ordering is not stable which might cause misalignment in pretraining and finetuning.\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | inferred language list: ['en', 'es', 'quy']\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | [en] dictionary: 28979 types\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | [es] dictionary: 28979 types\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | [quy] dictionary: 28979 types\n",
      "2021-10-08 10:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for valid epoch=1/None\n",
      "2021-10-08 10:24:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | [valid] num of shards: {'main:quy-es': 1, 'main:es-en': 1}\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 28978; tgt_langtok: 28977\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.quy-es.quy\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.data_utils | loaded 996 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.quy-es.es\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin valid quy-es 996 examples\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 28977; tgt_langtok: 28976\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.es-en.es\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.data_utils | loaded 4643 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/valid.es-en.en\n",
      "2021-10-08 10:24:07 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin valid es-en 4643 examples\n",
      "**************************** False\n",
      "**************************** False\n",
      "**************************** True\n",
      "**************************** False\n",
      "**************************** False\n",
      "**************************** False\n",
      "2021-10-08 10:24:08 | INFO | fairseq_cli.train | ResidualDropTransformerModel(\n",
      "  (encoder): ResidualDropTransformerEncoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(28979, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): ResidualDropTransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ResidualDropTransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ResidualDropTransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ResidualDropTransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): ResidualDropTransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): ResidualDropTransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(28979, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=28979, bias=False)\n",
      "  )\n",
      ")\n",
      "2021-10-08 10:24:08 | INFO | fairseq_cli.train | task: translation_multi_simple_epoch (TranslationMultiSimpleEpochTask)\n",
      "2021-10-08 10:24:08 | INFO | fairseq_cli.train | model: residual_drop_transformer (ResidualDropTransformerModel)\n",
      "2021-10-08 10:24:08 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)\n",
      "2021-10-08 10:24:08 | INFO | fairseq_cli.train | num. model params: 58975744 (num. trained: 58975744)\n",
      "2021-10-08 10:24:12 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
      "2021-10-08 10:24:12 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2021-10-08 10:24:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-10-08 10:24:12 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 7.795 GB ; name = Quadro RTX 4000                         \n",
      "2021-10-08 10:24:12 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2021-10-08 10:24:12 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2021-10-08 10:24:12 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None\n",
      "2021-10-08 10:24:12 | INFO | fairseq.trainer | no existing checkpoint found /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint_last.pt\n",
      "2021-10-08 10:24:12 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2021-10-08 10:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | loading data for train epoch=1/None\n",
      "2021-10-08 10:24:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:24:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | langtoks settings: {'main': ('src', 'tgt')}\n",
      "2021-10-08 10:24:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | [train] num of shards: {'main:quy-es': 1, 'main:es-en': 1}\n",
      "2021-10-08 10:24:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:quy-es src_langtok: 28978; tgt_langtok: 28977\n",
      "2021-10-08 10:24:12 | INFO | fairseq.data.data_utils | loaded 125008 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.quy-es.quy\n",
      "2021-10-08 10:24:12 | INFO | fairseq.data.data_utils | loaded 125008 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.quy-es.es\n",
      "2021-10-08 10:24:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin train quy-es 125008 examples\n",
      "2021-10-08 10:24:12 | INFO | fairseq.data.multilingual.multilingual_data_manager | main:es-en src_langtok: 28977; tgt_langtok: 28976\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.es-en.es\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.data_utils | loaded 197297 examples from: /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin/train.es-en.en\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | /storage/master-thesis/models/quy-es+es-en/tokenizers/quy-es-en/bin train es-en 197297 examples\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.multilingual_data_manager | estimated total data sizes of all shards used in sampling ratios: [('main:quy-es', 125008), ('main:es-en', 197297)]. Note that if the data a shard has not been loaded yet, use the max known data size to approximate\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.sampling_method | selected sampler: concat\n",
      "2021-10-08 10:24:13 | WARNING | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 1000000 is greater than virtual dataset size 322305\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | virtual epoch size 322305; virtual dataset size 322305\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=1/shard_epoch=1\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Raw sizes: {'main:quy-es': 125008, 'main:es-en': 197297}; raw total size: 322305\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] Resampled sizes: {'main:quy-es': 125008, 'main:es-en': 197297}; resampled total size: 322305\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] A concat dataset\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | [train] virtual dataset established time: 0:00:00.034238\n",
      "2021-10-08 10:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:24:13 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006599\n",
      "2021-10-08 10:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111850\n",
      "2021-10-08 10:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:24:13 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[176689]\n",
      "2021-10-08 10:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008949\n",
      "2021-10-08 10:24:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.894550\n",
      "2021-10-08 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.016339\n",
      "2021-10-08 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:24:14 | INFO | fairseq.optim.adam | using FusedAdam\n",
      "2021-10-08 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086390\n",
      "2021-10-08 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:24:14 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[176689]\n",
      "2021-10-08 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008640\n",
      "2021-10-08 10:24:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.928919\n",
      "2021-10-08 10:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.024920\n",
      "2021-10-08 10:24:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 001:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:24:15 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2021-10-08 10:24:15 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0\n",
      "epoch 001:   1%| | 8/1290 [00:01<03:33,  6.02it/s, loss_scale=64, train_wall=0, 2021-10-08 10:24:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0\n",
      "epoch 001: 100%|9| 1289/1290 [03:13<00:00,  6.59it/s, loss=8.495, nll_loss=7.5012021-10-08 10:27:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001575\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059775\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041901\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103988\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001505\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061583\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041916\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105851\n",
      "2021-10-08 10:27:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.58it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  11%|7      | 3/28 [00:00<00:02,  9.30it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:01, 11.15it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.87it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 14.40it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.71it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.55it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.15it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:27:30 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.21 | nll_loss 7.114 | ppl 138.55 | wps 54268.5 | wpb 2691.4 | bsz 201.4 | num_updates 1288\n",
      "2021-10-08 10:27:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:27:35 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint1.pt (epoch 1 @ 1288 updates, score 8.21) (writing took 4.380234686000222 seconds)\n",
      "2021-10-08 10:27:35 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2021-10-08 10:27:35 | INFO | train | epoch 001 | loss 9.629 | nll_loss 8.825 | ppl 453.61 | wps 23836.9 | ups 6.45 | wpb 3695.3 | bsz 249.7 | num_updates 1288 | lr 0.000161 | gnorm 2.077 | loss_scale 32 | train_wall 188 | wall 202\n",
      "2021-10-08 10:27:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=2/shard_epoch=1\n",
      "2021-10-08 10:27:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=2/shard_epoch=2\n",
      "2021-10-08 10:27:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:27:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005044\n",
      "2021-10-08 10:27:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.104728\n",
      "2021-10-08 10:27:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:35 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[315437]\n",
      "2021-10-08 10:27:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008038\n",
      "2021-10-08 10:27:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.884457\n",
      "2021-10-08 10:27:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.998147\n",
      "2021-10-08 10:27:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:27:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084127\n",
      "2021-10-08 10:27:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:36 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[315437]\n",
      "2021-10-08 10:27:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007506\n",
      "2021-10-08 10:27:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:27:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.890169\n",
      "2021-10-08 10:27:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.982809\n",
      "2021-10-08 10:27:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 002:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:27:37 | INFO | fairseq.trainer | begin training epoch 2\n",
      "epoch 002: 100%|9| 1289/1290 [03:14<00:00,  6.47it/s, loss=7.118, nll_loss=5.9272021-10-08 10:30:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001677\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061096\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041021\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104488\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001685\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065703\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041503\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109855\n",
      "2021-10-08 10:30:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.79it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.66it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.46it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.16it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.63it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.82it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.79it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 18.01it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.80it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.14it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.33it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:30:53 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.148 | nll_loss 5.897 | ppl 59.59 | wps 54343.6 | wpb 2691.4 | bsz 201.4 | num_updates 2578 | best_loss 7.148\n",
      "2021-10-08 10:30:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:30:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint2.pt (epoch 2 @ 2578 updates, score 7.148) (writing took 4.434934172999419 seconds)\n",
      "2021-10-08 10:30:58 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2021-10-08 10:30:58 | INFO | train | epoch 002 | loss 7.533 | nll_loss 6.403 | ppl 84.61 | wps 23462 | ups 6.35 | wpb 3695.9 | bsz 249.8 | num_updates 2578 | lr 0.00032225 | gnorm 1.274 | loss_scale 32 | train_wall 189 | wall 406\n",
      "2021-10-08 10:30:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=3/shard_epoch=2\n",
      "2021-10-08 10:30:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=3/shard_epoch=3\n",
      "2021-10-08 10:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:30:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004969\n",
      "2021-10-08 10:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103365\n",
      "2021-10-08 10:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:30:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[282912]\n",
      "2021-10-08 10:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008920\n",
      "2021-10-08 10:30:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.877344\n",
      "2021-10-08 10:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.990583\n",
      "2021-10-08 10:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086888\n",
      "2021-10-08 10:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:30:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[282912]\n",
      "2021-10-08 10:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008240\n",
      "2021-10-08 10:30:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.900644\n",
      "2021-10-08 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.996787\n",
      "2021-10-08 10:31:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 003:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:31:00 | INFO | fairseq.trainer | begin training epoch 3\n",
      "epoch 003: 100%|9| 1289/1290 [03:16<00:00,  6.59it/s, loss=6.676, nll_loss=5.4172021-10-08 10:34:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001762\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063008\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042142\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107678\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001516\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061962\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042015\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106096\n",
      "2021-10-08 10:34:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.69it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.52it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.34it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 13.02it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 14.45it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.65it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.41it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.06it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:34:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.681 | nll_loss 5.319 | ppl 39.91 | wps 54183.9 | wpb 2691.4 | bsz 201.4 | num_updates 3868 | best_loss 6.681\n",
      "2021-10-08 10:34:19 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:34:23 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint3.pt (epoch 3 @ 3868 updates, score 6.681) (writing took 4.581430771000669 seconds)\n",
      "2021-10-08 10:34:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2021-10-08 10:34:23 | INFO | train | epoch 003 | loss 6.705 | nll_loss 5.454 | ppl 43.85 | wps 23234.8 | ups 6.29 | wpb 3695.9 | bsz 249.8 | num_updates 3868 | lr 0.0004835 | gnorm 1.116 | loss_scale 32 | train_wall 191 | wall 611\n",
      "2021-10-08 10:34:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=4/shard_epoch=3\n",
      "2021-10-08 10:34:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=4/shard_epoch=4\n",
      "2021-10-08 10:34:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:34:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005003\n",
      "2021-10-08 10:34:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.111246\n",
      "2021-10-08 10:34:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:23 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[250240]\n",
      "2021-10-08 10:34:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008814\n",
      "2021-10-08 10:34:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.892636\n",
      "2021-10-08 10:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.013698\n",
      "2021-10-08 10:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088411\n",
      "2021-10-08 10:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:24 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[250240]\n",
      "2021-10-08 10:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008668\n",
      "2021-10-08 10:34:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.901302\n",
      "2021-10-08 10:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.999373\n",
      "2021-10-08 10:34:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 004:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:34:25 | INFO | fairseq.trainer | begin training epoch 4\n",
      "epoch 004: 100%|9| 1289/1290 [03:17<00:00,  6.56it/s, loss=5.866, nll_loss=4.4912021-10-08 10:37:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001712\n",
      "2021-10-08 10:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059402\n",
      "2021-10-08 10:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043336\n",
      "2021-10-08 10:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105179\n",
      "2021-10-08 10:37:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001497\n",
      "2021-10-08 10:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061195\n",
      "2021-10-08 10:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043651\n",
      "2021-10-08 10:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107005\n",
      "2021-10-08 10:37:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.86it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.71it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.52it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 13.15it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 14.63it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.75it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.54it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 19.17it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.35it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:37:44 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.245 | nll_loss 4.811 | ppl 28.07 | wps 54159.9 | wpb 2691.4 | bsz 201.4 | num_updates 5158 | best_loss 6.245\n",
      "2021-10-08 10:37:44 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:37:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint4.pt (epoch 4 @ 5158 updates, score 6.245) (writing took 4.403934718000528 seconds)\n",
      "2021-10-08 10:37:49 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2021-10-08 10:37:49 | INFO | train | epoch 004 | loss 6.153 | nll_loss 4.819 | ppl 28.23 | wps 23211 | ups 6.28 | wpb 3695.9 | bsz 249.8 | num_updates 5158 | lr 0.000440311 | gnorm 1.01 | loss_scale 32 | train_wall 191 | wall 816\n",
      "2021-10-08 10:37:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=5/shard_epoch=4\n",
      "2021-10-08 10:37:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=5/shard_epoch=5\n",
      "2021-10-08 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:37:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005183\n",
      "2021-10-08 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106206\n",
      "2021-10-08 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[90416]\n",
      "2021-10-08 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008794\n",
      "2021-10-08 10:37:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.945571\n",
      "2021-10-08 10:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.061568\n",
      "2021-10-08 10:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.081873\n",
      "2021-10-08 10:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:50 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[90416]\n",
      "2021-10-08 10:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008148\n",
      "2021-10-08 10:37:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.988126\n",
      "2021-10-08 10:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.079138\n",
      "2021-10-08 10:37:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 005:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:37:51 | INFO | fairseq.trainer | begin training epoch 5\n",
      "epoch 005: 100%|9| 1289/1290 [03:17<00:00,  6.48it/s, loss=5.697, nll_loss=4.2942021-10-08 10:41:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001817\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061186\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042204\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105875\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001878\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059651\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040781\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102944\n",
      "2021-10-08 10:41:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.03it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.81it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.63it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.35it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.20it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.21it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  64%|###8  | 18/28 [00:00<00:00, 17.17it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.60it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.28it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:41:10 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.954 | nll_loss 4.473 | ppl 22.2 | wps 54288.5 | wpb 2691.4 | bsz 201.4 | num_updates 6448 | best_loss 5.954\n",
      "2021-10-08 10:41:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:41:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint5.pt (epoch 5 @ 6448 updates, score 5.954) (writing took 4.514513899999656 seconds)\n",
      "2021-10-08 10:41:14 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2021-10-08 10:41:14 | INFO | train | epoch 005 | loss 5.73 | nll_loss 4.333 | ppl 20.15 | wps 23156 | ups 6.27 | wpb 3695.9 | bsz 249.8 | num_updates 6448 | lr 0.000393811 | gnorm 0.972 | loss_scale 32 | train_wall 192 | wall 1022\n",
      "2021-10-08 10:41:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=6/shard_epoch=5\n",
      "2021-10-08 10:41:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=6/shard_epoch=6\n",
      "2021-10-08 10:41:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:41:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004784\n",
      "2021-10-08 10:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105336\n",
      "2021-10-08 10:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:15 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[115279]\n",
      "2021-10-08 10:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008720\n",
      "2021-10-08 10:41:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.957773\n",
      "2021-10-08 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.072836\n",
      "2021-10-08 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087780\n",
      "2021-10-08 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:16 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[115279]\n",
      "2021-10-08 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010649\n",
      "2021-10-08 10:41:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.897075\n",
      "2021-10-08 10:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.996544\n",
      "2021-10-08 10:41:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 006:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:41:17 | INFO | fairseq.trainer | begin training epoch 6\n",
      "epoch 006: 100%|9| 1289/1290 [03:16<00:00,  7.18it/s, loss=5.288, nll_loss=3.8242021-10-08 10:44:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002090\n",
      "2021-10-08 10:44:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060129\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041729\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104655\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001599\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066060\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043828\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112083\n",
      "2021-10-08 10:44:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.36it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.16it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.98it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.69it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 14.21it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.54it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.35it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.03it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:44:35 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.789 | nll_loss 4.295 | ppl 19.62 | wps 54223.6 | wpb 2691.4 | bsz 201.4 | num_updates 7738 | best_loss 5.789\n",
      "2021-10-08 10:44:35 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:45:08 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint6.pt (epoch 6 @ 7738 updates, score 5.789) (writing took 33.272430646999965 seconds)\n",
      "2021-10-08 10:45:08 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2021-10-08 10:45:09 | INFO | train | epoch 006 | loss 5.436 | nll_loss 3.993 | ppl 15.92 | wps 20368.1 | ups 5.51 | wpb 3695.9 | bsz 249.8 | num_updates 7738 | lr 0.000359489 | gnorm 0.951 | loss_scale 32 | train_wall 191 | wall 1256\n",
      "2021-10-08 10:45:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=7/shard_epoch=6\n",
      "2021-10-08 10:45:09 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=7/shard_epoch=7\n",
      "2021-10-08 10:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:45:09 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005079\n",
      "2021-10-08 10:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108690\n",
      "2021-10-08 10:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:45:09 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[227703]\n",
      "2021-10-08 10:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009325\n",
      "2021-10-08 10:45:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.926365\n",
      "2021-10-08 10:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.045394\n",
      "2021-10-08 10:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084438\n",
      "2021-10-08 10:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:45:10 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[227703]\n",
      "2021-10-08 10:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008665\n",
      "2021-10-08 10:45:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:45:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.912862\n",
      "2021-10-08 10:45:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.006942\n",
      "2021-10-08 10:45:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 007:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:45:11 | INFO | fairseq.trainer | begin training epoch 7\n",
      "epoch 007:  22%|2| 281/1290 [00:42<02:26,  6.90it/s, loss=5.162, nll_loss=3.678,2021-10-08 10:45:53 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 007: 100%|9| 1289/1290 [03:15<00:00,  6.60it/s, loss=5.329, nll_loss=3.8662021-10-08 10:48:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001680\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061302\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.047925\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.111737\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001670\n",
      "2021-10-08 10:48:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061576\n",
      "2021-10-08 10:48:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042264\n",
      "2021-10-08 10:48:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106232\n",
      "2021-10-08 10:48:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.95it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.72it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.52it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.25it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.82it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.13it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.27it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.63it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.98it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:48:28 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.675 | nll_loss 4.161 | ppl 17.89 | wps 54472.3 | wpb 2691.4 | bsz 201.4 | num_updates 9027 | best_loss 5.675\n",
      "2021-10-08 10:48:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:48:32 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint7.pt (epoch 7 @ 9027 updates, score 5.675) (writing took 4.1633090069999525 seconds)\n",
      "2021-10-08 10:48:32 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2021-10-08 10:48:32 | INFO | train | epoch 007 | loss 5.222 | nll_loss 3.745 | ppl 13.41 | wps 23383.8 | ups 6.33 | wpb 3696.1 | bsz 250 | num_updates 9027 | lr 0.000332834 | gnorm 0.946 | loss_scale 16 | train_wall 190 | wall 1460\n",
      "2021-10-08 10:48:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=8/shard_epoch=7\n",
      "2021-10-08 10:48:32 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=8/shard_epoch=8\n",
      "2021-10-08 10:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:48:32 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005050\n",
      "2021-10-08 10:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.102546\n",
      "2021-10-08 10:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[241793]\n",
      "2021-10-08 10:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007843\n",
      "2021-10-08 10:48:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.935688\n",
      "2021-10-08 10:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.047085\n",
      "2021-10-08 10:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087416\n",
      "2021-10-08 10:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:33 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[241793]\n",
      "2021-10-08 10:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008035\n",
      "2021-10-08 10:48:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.892961\n",
      "2021-10-08 10:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.989332\n",
      "2021-10-08 10:48:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 008:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:48:34 | INFO | fairseq.trainer | begin training epoch 8\n",
      "epoch 008: 100%|9| 1289/1290 [03:17<00:00,  6.38it/s, loss=5.107, nll_loss=3.61,2021-10-08 10:51:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001743\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060162\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043421\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106046\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001577\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061284\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040690\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104179\n",
      "2021-10-08 10:51:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.77it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.53it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.33it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.06it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.64it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 11.46it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 13.09it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 14.91it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 16.35it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 17.25it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 17.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:51:54 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.583 | nll_loss 4.046 | ppl 16.52 | wps 49300.4 | wpb 2691.4 | bsz 201.4 | num_updates 10317 | best_loss 5.583\n",
      "2021-10-08 10:51:54 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:51:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint8.pt (epoch 8 @ 10317 updates, score 5.583) (writing took 4.395914024000376 seconds)\n",
      "2021-10-08 10:51:58 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2021-10-08 10:51:58 | INFO | train | epoch 008 | loss 5.059 | nll_loss 3.556 | ppl 11.76 | wps 23139.6 | ups 6.26 | wpb 3695.9 | bsz 249.8 | num_updates 10317 | lr 0.000311332 | gnorm 0.951 | loss_scale 16 | train_wall 192 | wall 1666\n",
      "2021-10-08 10:51:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=9/shard_epoch=8\n",
      "2021-10-08 10:51:58 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=9/shard_epoch=9\n",
      "2021-10-08 10:51:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:51:58 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004868\n",
      "2021-10-08 10:51:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105152\n",
      "2021-10-08 10:51:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:51:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[322092]\n",
      "2021-10-08 10:51:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008633\n",
      "2021-10-08 10:51:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:51:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.884271\n",
      "2021-10-08 10:51:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.998996\n",
      "2021-10-08 10:51:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:51:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:51:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088164\n",
      "2021-10-08 10:51:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:51:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[322092]\n",
      "2021-10-08 10:51:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008893\n",
      "2021-10-08 10:51:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.893316\n",
      "2021-10-08 10:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.991398\n",
      "2021-10-08 10:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 009:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:52:00 | INFO | fairseq.trainer | begin training epoch 9\n",
      "epoch 009: 100%|9| 1289/1290 [03:16<00:00,  6.71it/s, loss=4.921, nll_loss=3.3972021-10-08 10:55:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001760\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061452\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041597\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105658\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001452\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062877\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041343\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106441\n",
      "2021-10-08 10:55:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.97it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.75it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.56it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.28it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.16it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.27it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.63it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.49it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.79it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.04it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:55:19 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.575 | nll_loss 4.044 | ppl 16.49 | wps 54022.2 | wpb 2691.4 | bsz 201.4 | num_updates 11607 | best_loss 5.575\n",
      "2021-10-08 10:55:19 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:55:23 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint9.pt (epoch 9 @ 11607 updates, score 5.575) (writing took 4.708255714999723 seconds)\n",
      "2021-10-08 10:55:23 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2021-10-08 10:55:23 | INFO | train | epoch 009 | loss 4.927 | nll_loss 3.403 | ppl 10.58 | wps 23235.6 | ups 6.29 | wpb 3695.9 | bsz 249.8 | num_updates 11607 | lr 0.000293522 | gnorm 0.95 | loss_scale 16 | train_wall 191 | wall 1871\n",
      "2021-10-08 10:55:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=10/shard_epoch=9\n",
      "2021-10-08 10:55:23 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=10/shard_epoch=10\n",
      "2021-10-08 10:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:55:23 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005165\n",
      "2021-10-08 10:55:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109944\n",
      "2021-10-08 10:55:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:24 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[144740]\n",
      "2021-10-08 10:55:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009296\n",
      "2021-10-08 10:55:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.203682\n",
      "2021-10-08 10:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.324070\n",
      "2021-10-08 10:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086133\n",
      "2021-10-08 10:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:25 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[144740]\n",
      "2021-10-08 10:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008515\n",
      "2021-10-08 10:55:25 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.908704\n",
      "2021-10-08 10:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.004418\n",
      "2021-10-08 10:55:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 010:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:55:26 | INFO | fairseq.trainer | begin training epoch 10\n",
      "epoch 010: 100%|9| 1289/1290 [03:17<00:00,  6.61it/s, loss=4.815, nll_loss=3.2732021-10-08 10:58:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001858\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059610\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041603\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103852\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001498\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064924\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041595\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108767\n",
      "2021-10-08 10:58:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.01it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.79it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.61it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.35it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.93it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.23it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.31it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.65it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.56it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.96it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 10:58:45 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.543 | nll_loss 3.998 | ppl 15.98 | wps 54347.1 | wpb 2691.4 | bsz 201.4 | num_updates 12897 | best_loss 5.543\n",
      "2021-10-08 10:58:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 10:58:50 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint10.pt (epoch 10 @ 12897 updates, score 5.543) (writing took 4.5714155739997295 seconds)\n",
      "2021-10-08 10:58:50 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2021-10-08 10:58:50 | INFO | train | epoch 010 | loss 4.817 | nll_loss 3.275 | ppl 9.68 | wps 23102.7 | ups 6.25 | wpb 3695.9 | bsz 249.8 | num_updates 12897 | lr 0.000278455 | gnorm 0.95 | loss_scale 16 | train_wall 192 | wall 2077\n",
      "2021-10-08 10:58:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=11/shard_epoch=10\n",
      "2021-10-08 10:58:50 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=11/shard_epoch=11\n",
      "2021-10-08 10:58:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:58:50 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004950\n",
      "2021-10-08 10:58:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107349\n",
      "2021-10-08 10:58:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:50 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[143301]\n",
      "2021-10-08 10:58:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008398\n",
      "2021-10-08 10:58:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.899039\n",
      "2021-10-08 10:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.015760\n",
      "2021-10-08 10:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 10:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083323\n",
      "2021-10-08 10:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:51 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[143301]\n",
      "2021-10-08 10:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008401\n",
      "2021-10-08 10:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 10:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.891548\n",
      "2021-10-08 10:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.984255\n",
      "2021-10-08 10:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 011:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 10:58:52 | INFO | fairseq.trainer | begin training epoch 11\n",
      "epoch 011: 100%|9| 1289/1290 [03:16<00:00,  7.14it/s, loss=4.816, nll_loss=3.2732021-10-08 11:02:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001828\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061604\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042023\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106120\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001300\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063115\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042141\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107884\n",
      "2021-10-08 11:02:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 011 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.98it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.74it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.46it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.19it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 15.16it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 16.22it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 17.08it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  64%|###8  | 18/28 [00:00<00:00, 17.85it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 19.15it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.31it/s]\u001b[A\n",
      "epoch 011 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.38it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:02:10 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.51 | nll_loss 3.964 | ppl 15.6 | wps 53788.1 | wpb 2691.4 | bsz 201.4 | num_updates 14187 | best_loss 5.51\n",
      "2021-10-08 11:02:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:04:29 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint11.pt (epoch 11 @ 14187 updates, score 5.51) (writing took 138.24384929100052 seconds)\n",
      "2021-10-08 11:04:29 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
      "2021-10-08 11:04:29 | INFO | train | epoch 011 | loss 4.724 | nll_loss 3.168 | ppl 8.98 | wps 14071 | ups 3.81 | wpb 3695.9 | bsz 249.8 | num_updates 14187 | lr 0.000265494 | gnorm 0.959 | loss_scale 16 | train_wall 191 | wall 2416\n",
      "2021-10-08 11:04:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=12/shard_epoch=11\n",
      "2021-10-08 11:04:29 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=12/shard_epoch=12\n",
      "2021-10-08 11:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:04:29 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004853\n",
      "2021-10-08 11:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107914\n",
      "2021-10-08 11:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:04:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[310675]\n",
      "2021-10-08 11:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007751\n",
      "2021-10-08 11:04:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.897169\n",
      "2021-10-08 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.013775\n",
      "2021-10-08 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087416\n",
      "2021-10-08 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:04:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[310675]\n",
      "2021-10-08 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007814\n",
      "2021-10-08 11:04:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.908373\n",
      "2021-10-08 11:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.004540\n",
      "2021-10-08 11:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 012:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:04:31 | INFO | fairseq.trainer | begin training epoch 12\n",
      "epoch 012: 100%|9| 1289/1290 [03:15<00:00,  6.57it/s, loss=4.569, nll_loss=2.99,2021-10-08 11:07:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001739\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059375\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042564\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104318\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001415\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060358\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040818\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103206\n",
      "2021-10-08 11:07:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 012 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.86it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.62it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.44it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.19it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.74it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.05it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.54it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.53it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "epoch 012 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.66it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:07:48 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.484 | nll_loss 3.935 | ppl 15.3 | wps 54399.9 | wpb 2691.4 | bsz 201.4 | num_updates 15477 | best_loss 5.484\n",
      "2021-10-08 11:07:48 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:07:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint12.pt (epoch 12 @ 15477 updates, score 5.484) (writing took 4.450026759999673 seconds)\n",
      "2021-10-08 11:07:52 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
      "2021-10-08 11:07:52 | INFO | train | epoch 012 | loss 4.644 | nll_loss 3.075 | ppl 8.43 | wps 23420.4 | ups 6.34 | wpb 3695.9 | bsz 249.8 | num_updates 15477 | lr 0.000254189 | gnorm 0.963 | loss_scale 16 | train_wall 190 | wall 2620\n",
      "2021-10-08 11:07:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=13/shard_epoch=12\n",
      "2021-10-08 11:07:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=13/shard_epoch=13\n",
      "2021-10-08 11:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:07:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004986\n",
      "2021-10-08 11:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.104501\n",
      "2021-10-08 11:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[254515]\n",
      "2021-10-08 11:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007882\n",
      "2021-10-08 11:07:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.890477\n",
      "2021-10-08 11:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.003867\n",
      "2021-10-08 11:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085106\n",
      "2021-10-08 11:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:53 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[254515]\n",
      "2021-10-08 11:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007777\n",
      "2021-10-08 11:07:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:07:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.908041\n",
      "2021-10-08 11:07:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.001877\n",
      "2021-10-08 11:07:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 013:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:07:54 | INFO | fairseq.trainer | begin training epoch 13\n",
      "epoch 013: 100%|9| 1289/1290 [03:18<00:00,  6.44it/s, loss=4.546, nll_loss=2.9632021-10-08 11:11:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001719\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061179\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041506\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105129\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001479\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060464\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041357\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103962\n",
      "2021-10-08 11:11:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 013 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.63it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.46it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.26it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.94it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.42it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.59it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.87it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.06it/s]\u001b[A\n",
      "epoch 013 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.24it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:11:14 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.494 | nll_loss 3.943 | ppl 15.38 | wps 54025.2 | wpb 2691.4 | bsz 201.4 | num_updates 16767 | best_loss 5.484\n",
      "2021-10-08 11:11:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:11:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint13.pt (epoch 13 @ 16767 updates, score 5.494) (writing took 3.497728459999962 seconds)\n",
      "2021-10-08 11:11:18 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
      "2021-10-08 11:11:18 | INFO | train | epoch 013 | loss 4.572 | nll_loss 2.991 | ppl 7.95 | wps 23175.2 | ups 6.27 | wpb 3695.9 | bsz 249.8 | num_updates 16767 | lr 0.000244215 | gnorm 0.97 | loss_scale 16 | train_wall 192 | wall 2826\n",
      "2021-10-08 11:11:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=14/shard_epoch=13\n",
      "2021-10-08 11:11:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=14/shard_epoch=14\n",
      "2021-10-08 11:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:11:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005556\n",
      "2021-10-08 11:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.109559\n",
      "2021-10-08 11:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[159220]\n",
      "2021-10-08 11:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008616\n",
      "2021-10-08 11:11:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.888281\n",
      "2021-10-08 11:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.007397\n",
      "2021-10-08 11:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087589\n",
      "2021-10-08 11:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[159220]\n",
      "2021-10-08 11:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007788\n",
      "2021-10-08 11:11:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.885657\n",
      "2021-10-08 11:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.982000\n",
      "2021-10-08 11:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 014:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:11:20 | INFO | fairseq.trainer | begin training epoch 14\n",
      "epoch 014: 100%|9| 1289/1290 [03:15<00:00,  6.62it/s, loss=4.528, nll_loss=2.94,2021-10-08 11:14:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001783\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063038\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041183\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106877\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001718\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060102\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041225\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103812\n",
      "2021-10-08 11:14:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 014 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.63it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.46it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.26it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.95it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 14.46it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.74it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.69it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.53it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.90it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.18it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.42it/s]\u001b[A\n",
      "epoch 014 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 21.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:14:38 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.508 | nll_loss 3.962 | ppl 15.58 | wps 54379.8 | wpb 2691.4 | bsz 201.4 | num_updates 18057 | best_loss 5.484\n",
      "2021-10-08 11:14:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:14:41 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint14.pt (epoch 14 @ 18057 updates, score 5.508) (writing took 3.368127359999562 seconds)\n",
      "2021-10-08 11:14:41 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
      "2021-10-08 11:14:41 | INFO | train | epoch 014 | loss 4.508 | nll_loss 2.918 | ppl 7.56 | wps 23487.6 | ups 6.36 | wpb 3695.9 | bsz 249.8 | num_updates 18057 | lr 0.00023533 | gnorm 0.977 | loss_scale 16 | train_wall 190 | wall 3029\n",
      "2021-10-08 11:14:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=15/shard_epoch=14\n",
      "2021-10-08 11:14:41 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=15/shard_epoch=15\n",
      "2021-10-08 11:14:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:14:41 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005142\n",
      "2021-10-08 11:14:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.104755\n",
      "2021-10-08 11:14:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:41 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[136366]\n",
      "2021-10-08 11:14:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008530\n",
      "2021-10-08 11:14:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.903795\n",
      "2021-10-08 11:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.018058\n",
      "2021-10-08 11:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.082555\n",
      "2021-10-08 11:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:42 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[136366]\n",
      "2021-10-08 11:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008868\n",
      "2021-10-08 11:14:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.895718\n",
      "2021-10-08 11:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.988873\n",
      "2021-10-08 11:14:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 015:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:14:43 | INFO | fairseq.trainer | begin training epoch 15\n",
      "epoch 015: 100%|9| 1289/1290 [03:19<00:00,  6.67it/s, loss=4.442, nll_loss=2.8432021-10-08 11:18:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001710\n",
      "2021-10-08 11:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060154\n",
      "2021-10-08 11:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041277\n",
      "2021-10-08 11:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103872\n",
      "2021-10-08 11:18:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001532\n",
      "2021-10-08 11:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063251\n",
      "2021-10-08 11:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041753\n",
      "2021-10-08 11:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107249\n",
      "2021-10-08 11:18:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 015 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.09it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.88it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.69it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.42it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.96it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.25it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.73it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.58it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.98it/s]\u001b[A\n",
      "epoch 015 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:18:04 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.45 | nll_loss 3.893 | ppl 14.86 | wps 54393.6 | wpb 2691.4 | bsz 201.4 | num_updates 19347 | best_loss 5.45\n",
      "2021-10-08 11:18:04 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:18:08 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint15.pt (epoch 15 @ 19347 updates, score 5.45) (writing took 4.330630322999241 seconds)\n",
      "2021-10-08 11:18:08 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
      "2021-10-08 11:18:08 | INFO | train | epoch 015 | loss 4.448 | nll_loss 2.848 | ppl 7.2 | wps 22979.5 | ups 6.22 | wpb 3695.9 | bsz 249.8 | num_updates 19347 | lr 0.000227349 | gnorm 0.985 | loss_scale 16 | train_wall 193 | wall 3236\n",
      "2021-10-08 11:18:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=16/shard_epoch=15\n",
      "2021-10-08 11:18:08 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=16/shard_epoch=16\n",
      "2021-10-08 11:18:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:18:08 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004943\n",
      "2021-10-08 11:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103364\n",
      "2021-10-08 11:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:09 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[256488]\n",
      "2021-10-08 11:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008058\n",
      "2021-10-08 11:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.208602\n",
      "2021-10-08 11:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.320991\n",
      "2021-10-08 11:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.081574\n",
      "2021-10-08 11:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:10 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[256488]\n",
      "2021-10-08 11:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008065\n",
      "2021-10-08 11:18:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.026280\n",
      "2021-10-08 11:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.116875\n",
      "2021-10-08 11:18:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 016:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:18:11 | INFO | fairseq.trainer | begin training epoch 16\n",
      "epoch 016:  95%|9| 1228/1290 [03:07<00:09,  6.68it/s, loss=4.494, nll_loss=2.8992021-10-08 11:21:19 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8.0\n",
      "epoch 016: 100%|9| 1289/1290 [03:16<00:00,  6.88it/s, loss=4.473, nll_loss=2.8762021-10-08 11:21:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001712\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059865\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042707\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104973\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001598\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059902\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043062\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105405\n",
      "2021-10-08 11:21:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 016 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.24it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.90it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.67it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01,  9.93it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 11.67it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 13.28it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 14.68it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 16.30it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 17.36it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.04it/s]\u001b[A\n",
      "epoch 016 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:21:30 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.512 | nll_loss 3.972 | ppl 15.7 | wps 49020.8 | wpb 2691.4 | bsz 201.4 | num_updates 20636 | best_loss 5.45\n",
      "2021-10-08 11:21:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:21:33 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint16.pt (epoch 16 @ 20636 updates, score 5.512) (writing took 3.37876343500011 seconds)\n",
      "2021-10-08 11:21:33 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
      "2021-10-08 11:21:33 | INFO | train | epoch 016 | loss 4.399 | nll_loss 2.791 | ppl 6.92 | wps 23284.4 | ups 6.3 | wpb 3695.6 | bsz 249.8 | num_updates 20636 | lr 0.000220134 | gnorm 0.996 | loss_scale 8 | train_wall 191 | wall 3441\n",
      "2021-10-08 11:21:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=17/shard_epoch=16\n",
      "2021-10-08 11:21:33 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=17/shard_epoch=17\n",
      "2021-10-08 11:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:21:33 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005007\n",
      "2021-10-08 11:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106460\n",
      "2021-10-08 11:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:33 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[19119]\n",
      "2021-10-08 11:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008769\n",
      "2021-10-08 11:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.881549\n",
      "2021-10-08 11:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.997811\n",
      "2021-10-08 11:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087107\n",
      "2021-10-08 11:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:34 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[19119]\n",
      "2021-10-08 11:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007752\n",
      "2021-10-08 11:21:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.870465\n",
      "2021-10-08 11:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.966268\n",
      "2021-10-08 11:21:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 017:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:21:35 | INFO | fairseq.trainer | begin training epoch 17\n",
      "epoch 017: 100%|9| 1289/1290 [03:16<00:00,  6.52it/s, loss=4.476, nll_loss=2.8782021-10-08 11:24:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001634\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059954\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040560\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102839\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001389\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061460\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040707\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104277\n",
      "2021-10-08 11:24:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 017 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.75it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.61it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.43it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.12it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.61it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.75it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.63it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.95it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.79it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.13it/s]\u001b[A\n",
      "epoch 017 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.30it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:24:54 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.511 | nll_loss 3.963 | ppl 15.59 | wps 54180 | wpb 2691.4 | bsz 201.4 | num_updates 21926 | best_loss 5.45\n",
      "2021-10-08 11:24:54 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:24:57 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint17.pt (epoch 17 @ 21926 updates, score 5.511) (writing took 3.4044104219992732 seconds)\n",
      "2021-10-08 11:24:57 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
      "2021-10-08 11:24:57 | INFO | train | epoch 017 | loss 4.351 | nll_loss 2.735 | ppl 6.66 | wps 23362.4 | ups 6.32 | wpb 3695.9 | bsz 249.8 | num_updates 21926 | lr 0.00021356 | gnorm 1.011 | loss_scale 8 | train_wall 191 | wall 3645\n",
      "2021-10-08 11:24:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=18/shard_epoch=17\n",
      "2021-10-08 11:24:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=18/shard_epoch=18\n",
      "2021-10-08 11:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:24:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004904\n",
      "2021-10-08 11:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103785\n",
      "2021-10-08 11:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:57 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[274051]\n",
      "2021-10-08 11:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007671\n",
      "2021-10-08 11:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.858480\n",
      "2021-10-08 11:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.970878\n",
      "2021-10-08 11:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087160\n",
      "2021-10-08 11:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[274051]\n",
      "2021-10-08 11:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008165\n",
      "2021-10-08 11:24:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.870085\n",
      "2021-10-08 11:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.966377\n",
      "2021-10-08 11:24:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 018:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:24:59 | INFO | fairseq.trainer | begin training epoch 18\n",
      "epoch 018: 100%|9| 1289/1290 [03:15<00:00,  6.59it/s, loss=4.415, nll_loss=2.8062021-10-08 11:28:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001859\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061291\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041647\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105604\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001458\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060824\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041009\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103947\n",
      "2021-10-08 11:28:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 018 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.29it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.11it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.91it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.63it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.18it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.43it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.75it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.59it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.98it/s]\u001b[A\n",
      "epoch 018 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:28:17 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.532 | nll_loss 3.993 | ppl 15.92 | wps 54011.6 | wpb 2691.4 | bsz 201.4 | num_updates 23216 | best_loss 5.45\n",
      "2021-10-08 11:28:17 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:28:20 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint18.pt (epoch 18 @ 23216 updates, score 5.532) (writing took 3.2940169789999345 seconds)\n",
      "2021-10-08 11:28:20 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
      "2021-10-08 11:28:20 | INFO | train | epoch 018 | loss 4.306 | nll_loss 2.682 | ppl 6.42 | wps 23486.5 | ups 6.35 | wpb 3695.9 | bsz 249.8 | num_updates 23216 | lr 0.000207542 | gnorm 1.018 | loss_scale 8 | train_wall 190 | wall 3848\n",
      "2021-10-08 11:28:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=19/shard_epoch=18\n",
      "2021-10-08 11:28:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=19/shard_epoch=19\n",
      "2021-10-08 11:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:28:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004981\n",
      "2021-10-08 11:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107819\n",
      "2021-10-08 11:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[297749]\n",
      "2021-10-08 11:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008034\n",
      "2021-10-08 11:28:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.918519\n",
      "2021-10-08 11:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.035360\n",
      "2021-10-08 11:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084852\n",
      "2021-10-08 11:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:21 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[297749]\n",
      "2021-10-08 11:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008603\n",
      "2021-10-08 11:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.890677\n",
      "2021-10-08 11:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.985188\n",
      "2021-10-08 11:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 019:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:28:22 | INFO | fairseq.trainer | begin training epoch 19\n",
      "epoch 019: 100%|9| 1289/1290 [03:14<00:00,  6.70it/s, loss=4.285, nll_loss=2.6592021-10-08 11:31:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001759\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061452\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041713\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105590\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001496\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061661\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043890\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108764\n",
      "2021-10-08 11:31:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 019 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.32it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.13it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.89it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.66it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 14.21it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.52it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.53it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.41it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.80it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.16it/s]\u001b[A\n",
      "epoch 019 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:31:39 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.61 | nll_loss 4.087 | ppl 17 | wps 54485.6 | wpb 2691.4 | bsz 201.4 | num_updates 24506 | best_loss 5.45\n",
      "2021-10-08 11:31:39 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:31:43 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint19.pt (epoch 19 @ 24506 updates, score 5.61) (writing took 3.7625950050005486 seconds)\n",
      "2021-10-08 11:31:43 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
      "2021-10-08 11:31:43 | INFO | train | epoch 019 | loss 4.266 | nll_loss 2.637 | ppl 6.22 | wps 23535.6 | ups 6.37 | wpb 3695.9 | bsz 249.8 | num_updates 24506 | lr 0.000202006 | gnorm 1.03 | loss_scale 8 | train_wall 190 | wall 4050\n",
      "2021-10-08 11:31:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=20/shard_epoch=19\n",
      "2021-10-08 11:31:43 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=20/shard_epoch=20\n",
      "2021-10-08 11:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:31:43 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005022\n",
      "2021-10-08 11:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106656\n",
      "2021-10-08 11:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:43 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[141443]\n",
      "2021-10-08 11:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008115\n",
      "2021-10-08 11:31:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.003926\n",
      "2021-10-08 11:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.119719\n",
      "2021-10-08 11:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083213\n",
      "2021-10-08 11:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:44 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[141443]\n",
      "2021-10-08 11:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008019\n",
      "2021-10-08 11:31:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.091857\n",
      "2021-10-08 11:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.184105\n",
      "2021-10-08 11:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 020:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:31:45 | INFO | fairseq.trainer | begin training epoch 20\n",
      "epoch 020: 100%|9| 1289/1290 [03:16<00:00,  6.82it/s, loss=4.152, nll_loss=2.5072021-10-08 11:35:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001698\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064622\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043153\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110477\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001638\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060048\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041853\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104219\n",
      "2021-10-08 11:35:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 020 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.55it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.38it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.20it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.87it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.38it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.66it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.96it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.80it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.12it/s]\u001b[A\n",
      "epoch 020 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:35:04 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.527 | nll_loss 3.985 | ppl 15.83 | wps 54365.3 | wpb 2691.4 | bsz 201.4 | num_updates 25796 | best_loss 5.45\n",
      "2021-10-08 11:35:04 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:35:07 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint20.pt (epoch 20 @ 25796 updates, score 5.527) (writing took 3.338071707999916 seconds)\n",
      "2021-10-08 11:35:07 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
      "2021-10-08 11:35:07 | INFO | train | epoch 020 | loss 4.228 | nll_loss 2.592 | ppl 6.03 | wps 23307.8 | ups 6.31 | wpb 3695.9 | bsz 249.8 | num_updates 25796 | lr 0.00019689 | gnorm 1.04 | loss_scale 8 | train_wall 191 | wall 4255\n",
      "2021-10-08 11:35:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=21/shard_epoch=20\n",
      "2021-10-08 11:35:07 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=21/shard_epoch=21\n",
      "2021-10-08 11:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:35:07 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004926\n",
      "2021-10-08 11:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105410\n",
      "2021-10-08 11:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:07 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[11089]\n",
      "2021-10-08 11:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007542\n",
      "2021-10-08 11:35:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.876404\n",
      "2021-10-08 11:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.990280\n",
      "2021-10-08 11:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088061\n",
      "2021-10-08 11:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:08 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[11089]\n",
      "2021-10-08 11:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008074\n",
      "2021-10-08 11:35:08 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.891043\n",
      "2021-10-08 11:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.988116\n",
      "2021-10-08 11:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 021:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:35:09 | INFO | fairseq.trainer | begin training epoch 21\n",
      "epoch 021: 100%|9| 1289/1290 [03:16<00:00,  6.52it/s, loss=4.216, nll_loss=2.5812021-10-08 11:38:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001748\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.058695\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041191\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102528\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001679\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059623\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040672\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102624\n",
      "2021-10-08 11:38:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 021 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.06it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.83it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.64it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.34it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.17it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.19it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.54it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.99it/s]\u001b[A\n",
      "epoch 021 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:38:28 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.545 | nll_loss 4.001 | ppl 16.01 | wps 54214.8 | wpb 2691.4 | bsz 201.4 | num_updates 27086 | best_loss 5.45\n",
      "2021-10-08 11:38:28 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:38:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint21.pt (epoch 21 @ 27086 updates, score 5.545) (writing took 3.2712542319986824 seconds)\n",
      "2021-10-08 11:38:31 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
      "2021-10-08 11:38:31 | INFO | train | epoch 021 | loss 4.192 | nll_loss 2.551 | ppl 5.86 | wps 23377.6 | ups 6.33 | wpb 3695.9 | bsz 249.8 | num_updates 27086 | lr 0.000192144 | gnorm 1.045 | loss_scale 8 | train_wall 191 | wall 4459\n",
      "2021-10-08 11:38:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=22/shard_epoch=21\n",
      "2021-10-08 11:38:31 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=22/shard_epoch=22\n",
      "2021-10-08 11:38:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:38:31 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004883\n",
      "2021-10-08 11:38:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103981\n",
      "2021-10-08 11:38:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:31 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[264166]\n",
      "2021-10-08 11:38:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008415\n",
      "2021-10-08 11:38:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.891480\n",
      "2021-10-08 11:38:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.004845\n",
      "2021-10-08 11:38:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:38:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.082905\n",
      "2021-10-08 11:38:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:32 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[264166]\n",
      "2021-10-08 11:38:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008013\n",
      "2021-10-08 11:38:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.892256\n",
      "2021-10-08 11:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.984127\n",
      "2021-10-08 11:38:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 022:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:38:33 | INFO | fairseq.trainer | begin training epoch 22\n",
      "epoch 022: 100%|9| 1289/1290 [03:15<00:00,  6.41it/s, loss=4.218, nll_loss=2.5792021-10-08 11:41:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001776\n",
      "2021-10-08 11:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061125\n",
      "2021-10-08 11:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042195\n",
      "2021-10-08 11:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105889\n",
      "2021-10-08 11:41:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001600\n",
      "2021-10-08 11:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060150\n",
      "2021-10-08 11:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040631\n",
      "2021-10-08 11:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103048\n",
      "2021-10-08 11:41:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 022 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.94it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.72it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.55it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.28it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.17it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.25it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.21it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.57it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "epoch 022 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.29it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:41:51 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.501 | nll_loss 3.948 | ppl 15.43 | wps 54313.9 | wpb 2691.4 | bsz 201.4 | num_updates 28376 | best_loss 5.45\n",
      "2021-10-08 11:41:51 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:41:55 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint22.pt (epoch 22 @ 28376 updates, score 5.501) (writing took 3.480272335000336 seconds)\n",
      "2021-10-08 11:41:55 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
      "2021-10-08 11:41:55 | INFO | train | epoch 022 | loss 4.157 | nll_loss 2.51 | ppl 5.7 | wps 23439.2 | ups 6.34 | wpb 3695.9 | bsz 249.8 | num_updates 28376 | lr 0.000187726 | gnorm 1.057 | loss_scale 8 | train_wall 190 | wall 4662\n",
      "2021-10-08 11:41:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=23/shard_epoch=22\n",
      "2021-10-08 11:41:55 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=23/shard_epoch=23\n",
      "2021-10-08 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:41:55 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004615\n",
      "2021-10-08 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103951\n",
      "2021-10-08 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:55 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[184965]\n",
      "2021-10-08 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008653\n",
      "2021-10-08 11:41:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.929399\n",
      "2021-10-08 11:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.042938\n",
      "2021-10-08 11:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084751\n",
      "2021-10-08 11:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:56 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[184965]\n",
      "2021-10-08 11:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008302\n",
      "2021-10-08 11:41:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.914489\n",
      "2021-10-08 11:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.008521\n",
      "2021-10-08 11:41:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 023:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:41:57 | INFO | fairseq.trainer | begin training epoch 23\n",
      "epoch 023: 100%|9| 1289/1290 [03:15<00:00,  6.69it/s, loss=4.075, nll_loss=2.42,2021-10-08 11:45:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:45:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001760\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060590\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.053074\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.116206\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001732\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066246\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052655\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121313\n",
      "2021-10-08 11:45:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 023 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.89it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.63it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.40it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.17it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.69it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.08it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.04it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.37it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.81it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.09it/s]\u001b[A\n",
      "epoch 023 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:45:14 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.655 | nll_loss 4.137 | ppl 17.6 | wps 53378 | wpb 2691.4 | bsz 201.4 | num_updates 29666 | best_loss 5.45\n",
      "2021-10-08 11:45:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:45:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint23.pt (epoch 23 @ 29666 updates, score 5.655) (writing took 3.305337841999062 seconds)\n",
      "2021-10-08 11:45:18 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
      "2021-10-08 11:45:18 | INFO | train | epoch 023 | loss 4.125 | nll_loss 2.474 | ppl 5.55 | wps 23482.2 | ups 6.35 | wpb 3695.9 | bsz 249.8 | num_updates 29666 | lr 0.000183599 | gnorm 1.065 | loss_scale 8 | train_wall 190 | wall 4865\n",
      "2021-10-08 11:45:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=24/shard_epoch=23\n",
      "2021-10-08 11:45:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=24/shard_epoch=24\n",
      "2021-10-08 11:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:45:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004630\n",
      "2021-10-08 11:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.100865\n",
      "2021-10-08 11:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:18 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[49249]\n",
      "2021-10-08 11:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008364\n",
      "2021-10-08 11:45:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.887874\n",
      "2021-10-08 11:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.998069\n",
      "2021-10-08 11:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083318\n",
      "2021-10-08 11:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[49249]\n",
      "2021-10-08 11:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008106\n",
      "2021-10-08 11:45:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.884906\n",
      "2021-10-08 11:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.977304\n",
      "2021-10-08 11:45:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 024:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:45:20 | INFO | fairseq.trainer | begin training epoch 24\n",
      "epoch 024: 100%|9| 1289/1290 [03:16<00:00,  6.40it/s, loss=4.039, nll_loss=2.3772021-10-08 11:48:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:48:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:48:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.002008\n",
      "2021-10-08 11:48:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066856\n",
      "2021-10-08 11:48:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052111\n",
      "2021-10-08 11:48:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121712\n",
      "2021-10-08 11:48:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001805\n",
      "2021-10-08 11:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.071255\n",
      "2021-10-08 11:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051707\n",
      "2021-10-08 11:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.125654\n",
      "2021-10-08 11:48:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 024 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.32it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.10it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.51it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.25it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.94it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.43it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  61%|###6  | 17/28 [00:01<00:00, 16.65it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.17it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.70it/s]\u001b[A\n",
      "epoch 024 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.06it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:48:38 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.552 | nll_loss 4.009 | ppl 16.1 | wps 49147.7 | wpb 2691.4 | bsz 201.4 | num_updates 30956 | best_loss 5.45\n",
      "2021-10-08 11:48:38 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:48:42 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint24.pt (epoch 24 @ 30956 updates, score 5.552) (writing took 3.271259741999529 seconds)\n",
      "2021-10-08 11:48:42 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
      "2021-10-08 11:48:42 | INFO | train | epoch 024 | loss 4.097 | nll_loss 2.441 | ppl 5.43 | wps 23377.2 | ups 6.33 | wpb 3695.9 | bsz 249.8 | num_updates 30956 | lr 0.000179733 | gnorm 1.078 | loss_scale 8 | train_wall 191 | wall 5069\n",
      "2021-10-08 11:48:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=25/shard_epoch=24\n",
      "2021-10-08 11:48:42 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=25/shard_epoch=25\n",
      "2021-10-08 11:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:48:42 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004964\n",
      "2021-10-08 11:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.100938\n",
      "2021-10-08 11:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:42 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[35215]\n",
      "2021-10-08 11:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008987\n",
      "2021-10-08 11:48:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.887213\n",
      "2021-10-08 11:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.998143\n",
      "2021-10-08 11:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.082334\n",
      "2021-10-08 11:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:43 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[35215]\n",
      "2021-10-08 11:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008253\n",
      "2021-10-08 11:48:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:48:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.939234\n",
      "2021-10-08 11:48:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.030877\n",
      "2021-10-08 11:48:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 025:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:48:44 | INFO | fairseq.trainer | begin training epoch 25\n",
      "epoch 025: 100%|9| 1289/1290 [03:15<00:00,  6.71it/s, loss=4.19, nll_loss=2.547,2021-10-08 11:52:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001668\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060563\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043284\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106275\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001502\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060911\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041140\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104306\n",
      "2021-10-08 11:52:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 025 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.29it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.09it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.87it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.59it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 14.09it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 15.42it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 17.22it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 18.39it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 19.05it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.25it/s]\u001b[A\n",
      "epoch 025 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:52:01 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.515 | nll_loss 3.97 | ppl 15.67 | wps 53850.8 | wpb 2691.4 | bsz 201.4 | num_updates 32246 | best_loss 5.45\n",
      "2021-10-08 11:52:01 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:52:05 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint25.pt (epoch 25 @ 32246 updates, score 5.515) (writing took 3.305358150999382 seconds)\n",
      "2021-10-08 11:52:05 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
      "2021-10-08 11:52:05 | INFO | train | epoch 025 | loss 4.068 | nll_loss 2.408 | ppl 5.31 | wps 23480.4 | ups 6.35 | wpb 3695.9 | bsz 249.8 | num_updates 32246 | lr 0.000176101 | gnorm 1.087 | loss_scale 8 | train_wall 190 | wall 5272\n",
      "2021-10-08 11:52:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=26/shard_epoch=25\n",
      "2021-10-08 11:52:05 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=26/shard_epoch=26\n",
      "2021-10-08 11:52:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:52:05 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005084\n",
      "2021-10-08 11:52:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.104655\n",
      "2021-10-08 11:52:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:05 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[279338]\n",
      "2021-10-08 11:52:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007715\n",
      "2021-10-08 11:52:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.860919\n",
      "2021-10-08 11:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.974318\n",
      "2021-10-08 11:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086436\n",
      "2021-10-08 11:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:06 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[279338]\n",
      "2021-10-08 11:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008150\n",
      "2021-10-08 11:52:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:52:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.901815\n",
      "2021-10-08 11:52:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.997401\n",
      "2021-10-08 11:52:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 026:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:52:07 | INFO | fairseq.trainer | begin training epoch 26\n",
      "epoch 026: 100%|9| 1289/1290 [03:16<00:00,  6.74it/s, loss=4.077, nll_loss=2.4172021-10-08 11:55:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001985\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059570\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040592\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102825\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001441\n",
      "2021-10-08 11:55:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061590\n",
      "2021-10-08 11:55:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041256\n",
      "2021-10-08 11:55:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104922\n",
      "2021-10-08 11:55:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 026 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.52it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.24it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.03it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.80it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.45it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.87it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  64%|###8  | 18/28 [00:00<00:00, 17.03it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.42it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "epoch 026 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:55:25 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.583 | nll_loss 4.057 | ppl 16.64 | wps 54416.3 | wpb 2691.4 | bsz 201.4 | num_updates 33536 | best_loss 5.45\n",
      "2021-10-08 11:55:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:55:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint26.pt (epoch 26 @ 33536 updates, score 5.583) (writing took 3.3512113109991333 seconds)\n",
      "2021-10-08 11:55:28 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
      "2021-10-08 11:55:28 | INFO | train | epoch 026 | loss 4.039 | nll_loss 2.374 | ppl 5.18 | wps 23393.7 | ups 6.33 | wpb 3695.9 | bsz 249.8 | num_updates 33536 | lr 0.000172681 | gnorm 1.094 | loss_scale 8 | train_wall 191 | wall 5476\n",
      "2021-10-08 11:55:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=27/shard_epoch=26\n",
      "2021-10-08 11:55:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=27/shard_epoch=27\n",
      "2021-10-08 11:55:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:55:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004530\n",
      "2021-10-08 11:55:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105981\n",
      "2021-10-08 11:55:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[149946]\n",
      "2021-10-08 11:55:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010440\n",
      "2021-10-08 11:55:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.946336\n",
      "2021-10-08 11:55:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.063808\n",
      "2021-10-08 11:55:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085528\n",
      "2021-10-08 11:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[149946]\n",
      "2021-10-08 11:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010260\n",
      "2021-10-08 11:55:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:55:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.913327\n",
      "2021-10-08 11:55:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.010197\n",
      "2021-10-08 11:55:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 027:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:55:31 | INFO | fairseq.trainer | begin training epoch 27\n",
      "epoch 027: 100%|9| 1289/1290 [03:15<00:00,  6.61it/s, loss=4.046, nll_loss=2.3822021-10-08 11:58:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001729\n",
      "2021-10-08 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060817\n",
      "2021-10-08 11:58:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042173\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105411\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001447\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059993\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040713\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102741\n",
      "2021-10-08 11:58:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 027 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.45it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.29it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 11.11it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.83it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.33it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.67it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 18.00it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.80it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.12it/s]\u001b[A\n",
      "epoch 027 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 11:58:48 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.484 | nll_loss 3.936 | ppl 15.31 | wps 54477.6 | wpb 2691.4 | bsz 201.4 | num_updates 34826 | best_loss 5.45\n",
      "2021-10-08 11:58:48 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 11:58:51 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint27.pt (epoch 27 @ 34826 updates, score 5.484) (writing took 3.355011599000136 seconds)\n",
      "2021-10-08 11:58:51 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
      "2021-10-08 11:58:51 | INFO | train | epoch 027 | loss 4.011 | nll_loss 2.341 | ppl 5.07 | wps 23479.4 | ups 6.35 | wpb 3695.9 | bsz 249.8 | num_updates 34826 | lr 0.000169453 | gnorm 1.101 | loss_scale 8 | train_wall 190 | wall 5679\n",
      "2021-10-08 11:58:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=28/shard_epoch=27\n",
      "2021-10-08 11:58:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=28/shard_epoch=28\n",
      "2021-10-08 11:58:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:58:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004997\n",
      "2021-10-08 11:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.102687\n",
      "2021-10-08 11:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[281063]\n",
      "2021-10-08 11:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008036\n",
      "2021-10-08 11:58:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.907533\n",
      "2021-10-08 11:58:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.019257\n",
      "2021-10-08 11:58:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 11:58:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.090577\n",
      "2021-10-08 11:58:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:53 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[281063]\n",
      "2021-10-08 11:58:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.011976\n",
      "2021-10-08 11:58:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.941874\n",
      "2021-10-08 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.045986\n",
      "2021-10-08 11:58:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 028:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 11:58:54 | INFO | fairseq.trainer | begin training epoch 28\n",
      "epoch 028: 100%|9| 1289/1290 [03:15<00:00,  6.50it/s, loss=4.093, nll_loss=2.4342021-10-08 12:02:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001734\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062491\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043270\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108393\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001587\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061034\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041310\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104582\n",
      "2021-10-08 12:02:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 028 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.32it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.03it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.76it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.52it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.63it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.80it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 18.09it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.89it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.19it/s]\u001b[A\n",
      "epoch 028 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:02:11 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.486 | nll_loss 3.937 | ppl 15.32 | wps 53953.6 | wpb 2691.4 | bsz 201.4 | num_updates 36116 | best_loss 5.45\n",
      "2021-10-08 12:02:11 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:02:15 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint28.pt (epoch 28 @ 36116 updates, score 5.486) (writing took 3.5298030900012236 seconds)\n",
      "2021-10-08 12:02:15 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
      "2021-10-08 12:02:15 | INFO | train | epoch 028 | loss 3.983 | nll_loss 2.309 | ppl 4.95 | wps 23438.3 | ups 6.34 | wpb 3695.9 | bsz 249.8 | num_updates 36116 | lr 0.000166399 | gnorm 1.111 | loss_scale 8 | train_wall 190 | wall 5883\n",
      "2021-10-08 12:02:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=29/shard_epoch=28\n",
      "2021-10-08 12:02:15 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=29/shard_epoch=29\n",
      "2021-10-08 12:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:02:15 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004816\n",
      "2021-10-08 12:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.102246\n",
      "2021-10-08 12:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:15 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[27302]\n",
      "2021-10-08 12:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009626\n",
      "2021-10-08 12:02:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.912126\n",
      "2021-10-08 12:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.024984\n",
      "2021-10-08 12:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083964\n",
      "2021-10-08 12:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:16 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[27302]\n",
      "2021-10-08 12:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008717\n",
      "2021-10-08 12:02:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:02:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.939942\n",
      "2021-10-08 12:02:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.033679\n",
      "2021-10-08 12:02:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 029:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:02:17 | INFO | fairseq.trainer | begin training epoch 29\n",
      "epoch 029: 100%|9| 1289/1290 [03:16<00:00,  6.74it/s, loss=3.97, nll_loss=2.295,2021-10-08 12:05:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001715\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059940\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041202\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103633\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001529\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059322\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040764\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102421\n",
      "2021-10-08 12:05:34 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 029 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.03it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.80it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.59it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.31it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.82it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.16it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.25it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.59it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.45it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.88it/s]\u001b[A\n",
      "epoch 029 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:05:36 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.477 | nll_loss 3.918 | ppl 15.11 | wps 53880.2 | wpb 2691.4 | bsz 201.4 | num_updates 37406 | best_loss 5.45\n",
      "2021-10-08 12:05:36 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:05:39 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint29.pt (epoch 29 @ 37406 updates, score 5.477) (writing took 3.261654235999231 seconds)\n",
      "2021-10-08 12:05:39 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
      "2021-10-08 12:05:39 | INFO | train | epoch 029 | loss 3.957 | nll_loss 2.279 | ppl 4.85 | wps 23362.8 | ups 6.32 | wpb 3695.9 | bsz 249.8 | num_updates 37406 | lr 0.000163504 | gnorm 1.12 | loss_scale 16 | train_wall 191 | wall 6087\n",
      "2021-10-08 12:05:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=30/shard_epoch=29\n",
      "2021-10-08 12:05:39 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=30/shard_epoch=30\n",
      "2021-10-08 12:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:05:39 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005079\n",
      "2021-10-08 12:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.102944\n",
      "2021-10-08 12:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:39 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[116057]\n",
      "2021-10-08 12:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008286\n",
      "2021-10-08 12:05:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.924114\n",
      "2021-10-08 12:05:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.036378\n",
      "2021-10-08 12:05:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:05:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083741\n",
      "2021-10-08 12:05:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:40 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[116057]\n",
      "2021-10-08 12:05:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008236\n",
      "2021-10-08 12:05:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:05:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.881053\n",
      "2021-10-08 12:05:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.974066\n",
      "2021-10-08 12:05:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 030:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:05:41 | INFO | fairseq.trainer | begin training epoch 30\n",
      "epoch 030: 100%|9| 1289/1290 [03:16<00:00,  6.71it/s, loss=4.099, nll_loss=2.4382021-10-08 12:08:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001767\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060983\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041772\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105413\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001631\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062958\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.051845\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.117312\n",
      "2021-10-08 12:08:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 030 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.28it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.08it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.89it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.61it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.15it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.42it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  64%|###8  | 18/28 [00:00<00:00, 17.33it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.72it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.04it/s]\u001b[A\n",
      "epoch 030 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:08:59 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.629 | nll_loss 4.116 | ppl 17.34 | wps 53615 | wpb 2691.4 | bsz 201.4 | num_updates 38696 | best_loss 5.45\n",
      "2021-10-08 12:08:59 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:09:03 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint30.pt (epoch 30 @ 38696 updates, score 5.629) (writing took 3.2953533420004533 seconds)\n",
      "2021-10-08 12:09:03 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
      "2021-10-08 12:09:03 | INFO | train | epoch 030 | loss 3.934 | nll_loss 2.252 | ppl 4.76 | wps 23410.8 | ups 6.33 | wpb 3695.9 | bsz 249.8 | num_updates 38696 | lr 0.000160756 | gnorm 1.129 | loss_scale 16 | train_wall 191 | wall 6290\n",
      "2021-10-08 12:09:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=31/shard_epoch=30\n",
      "2021-10-08 12:09:03 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=31/shard_epoch=31\n",
      "2021-10-08 12:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:09:03 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005114\n",
      "2021-10-08 12:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107112\n",
      "2021-10-08 12:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:09:03 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[15510]\n",
      "2021-10-08 12:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008123\n",
      "2021-10-08 12:09:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.903783\n",
      "2021-10-08 12:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.020008\n",
      "2021-10-08 12:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.088389\n",
      "2021-10-08 12:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:09:04 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[15510]\n",
      "2021-10-08 12:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008654\n",
      "2021-10-08 12:09:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.886907\n",
      "2021-10-08 12:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.984958\n",
      "2021-10-08 12:09:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 031:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:09:05 | INFO | fairseq.trainer | begin training epoch 31\n",
      "epoch 031: 100%|9| 1289/1290 [03:18<00:00,  6.55it/s, loss=3.95, nll_loss=2.27, 2021-10-08 12:12:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001764\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061981\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041119\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105712\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001734\n",
      "2021-10-08 12:12:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061001\n",
      "2021-10-08 12:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041612\n",
      "2021-10-08 12:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105088\n",
      "2021-10-08 12:12:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 031 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.21it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.98it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.79it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.49it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 14.02it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.35it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.75it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.08it/s]\u001b[A\n",
      "epoch 031 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:12:25 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.511 | nll_loss 3.964 | ppl 15.6 | wps 54038 | wpb 2691.4 | bsz 201.4 | num_updates 39986 | best_loss 5.45\n",
      "2021-10-08 12:12:25 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:12:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint31.pt (epoch 31 @ 39986 updates, score 5.511) (writing took 3.354624405999857 seconds)\n",
      "2021-10-08 12:12:28 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
      "2021-10-08 12:12:28 | INFO | train | epoch 031 | loss 3.91 | nll_loss 2.225 | ppl 4.67 | wps 23169.5 | ups 6.27 | wpb 3695.9 | bsz 249.8 | num_updates 39986 | lr 0.000158142 | gnorm 1.135 | loss_scale 16 | train_wall 192 | wall 6496\n",
      "2021-10-08 12:12:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=32/shard_epoch=31\n",
      "2021-10-08 12:12:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=32/shard_epoch=32\n",
      "2021-10-08 12:12:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:12:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004995\n",
      "2021-10-08 12:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.102661\n",
      "2021-10-08 12:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[3638]\n",
      "2021-10-08 12:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007974\n",
      "2021-10-08 12:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.885906\n",
      "2021-10-08 12:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.997509\n",
      "2021-10-08 12:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:12:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084627\n",
      "2021-10-08 12:12:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:30 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[3638]\n",
      "2021-10-08 12:12:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008053\n",
      "2021-10-08 12:12:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:12:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.889778\n",
      "2021-10-08 12:12:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.983409\n",
      "2021-10-08 12:12:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 032:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:12:30 | INFO | fairseq.trainer | begin training epoch 32\n",
      "epoch 032: 100%|9| 1289/1290 [03:16<00:00,  6.34it/s, loss=3.816, nll_loss=2.1212021-10-08 12:15:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001923\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059128\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041391\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103071\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001632\n",
      "2021-10-08 12:15:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060620\n",
      "2021-10-08 12:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041282\n",
      "2021-10-08 12:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104157\n",
      "2021-10-08 12:15:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 032 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.92it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.64it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.43it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 12.18it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:00, 15.16it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.21it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.63it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.01it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.28it/s]\u001b[A\n",
      "epoch 032 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.92it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:15:49 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.469 | nll_loss 3.92 | ppl 15.13 | wps 53853.7 | wpb 2691.4 | bsz 201.4 | num_updates 41276 | best_loss 5.45\n",
      "2021-10-08 12:15:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:15:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint32.pt (epoch 32 @ 41276 updates, score 5.469) (writing took 3.236150255999746 seconds)\n",
      "2021-10-08 12:15:52 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
      "2021-10-08 12:15:52 | INFO | train | epoch 032 | loss 3.888 | nll_loss 2.199 | ppl 4.59 | wps 23382.9 | ups 6.33 | wpb 3695.9 | bsz 249.8 | num_updates 41276 | lr 0.000155651 | gnorm 1.145 | loss_scale 16 | train_wall 191 | wall 6700\n",
      "2021-10-08 12:15:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=33/shard_epoch=32\n",
      "2021-10-08 12:15:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=33/shard_epoch=33\n",
      "2021-10-08 12:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:15:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005051\n",
      "2021-10-08 12:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105179\n",
      "2021-10-08 12:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[282093]\n",
      "2021-10-08 12:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008486\n",
      "2021-10-08 12:15:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.234511\n",
      "2021-10-08 12:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.349178\n",
      "2021-10-08 12:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083327\n",
      "2021-10-08 12:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:54 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[282093]\n",
      "2021-10-08 12:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007788\n",
      "2021-10-08 12:15:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.904668\n",
      "2021-10-08 12:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.996850\n",
      "2021-10-08 12:15:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 033:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:15:55 | INFO | fairseq.trainer | begin training epoch 33\n",
      "epoch 033: 100%|9| 1289/1290 [03:18<00:00,  6.50it/s, loss=3.819, nll_loss=2.1232021-10-08 12:19:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001545\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060900\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041962\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105302\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001555\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060922\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043066\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106455\n",
      "2021-10-08 12:19:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 033 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.51it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.23it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.03it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.79it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.44it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.84it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.41it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.33it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.74it/s]\u001b[A\n",
      "epoch 033 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:19:15 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.598 | nll_loss 4.065 | ppl 16.74 | wps 54031 | wpb 2691.4 | bsz 201.4 | num_updates 42566 | best_loss 5.45\n",
      "2021-10-08 12:19:15 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:19:18 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint33.pt (epoch 33 @ 42566 updates, score 5.598) (writing took 3.536219147999873 seconds)\n",
      "2021-10-08 12:19:18 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
      "2021-10-08 12:19:18 | INFO | train | epoch 033 | loss 3.867 | nll_loss 2.175 | ppl 4.52 | wps 23134.2 | ups 6.26 | wpb 3695.9 | bsz 249.8 | num_updates 42566 | lr 0.000153274 | gnorm 1.15 | loss_scale 16 | train_wall 192 | wall 6906\n",
      "2021-10-08 12:19:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=34/shard_epoch=33\n",
      "2021-10-08 12:19:18 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=34/shard_epoch=34\n",
      "2021-10-08 12:19:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:19:18 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004967\n",
      "2021-10-08 12:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.117619\n",
      "2021-10-08 12:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:19 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[263938]\n",
      "2021-10-08 12:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008789\n",
      "2021-10-08 12:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.899164\n",
      "2021-10-08 12:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.026603\n",
      "2021-10-08 12:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:19 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087501\n",
      "2021-10-08 12:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[263938]\n",
      "2021-10-08 12:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008944\n",
      "2021-10-08 12:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.901186\n",
      "2021-10-08 12:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.998788\n",
      "2021-10-08 12:19:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 034:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:19:20 | INFO | fairseq.trainer | begin training epoch 34\n",
      "epoch 034: 100%|9| 1289/1290 [03:17<00:00,  6.61it/s, loss=3.957, nll_loss=2.2782021-10-08 12:22:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001832\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.066261\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052660\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121712\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001554\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.067001\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052340\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.121698\n",
      "2021-10-08 12:22:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 034 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.37it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.06it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02,  9.74it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 11.50it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 13.15it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.58it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.74it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.68it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.98it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.76it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.03it/s]\u001b[A\n",
      "epoch 034 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.12it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:22:40 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.49 | nll_loss 3.948 | ppl 15.43 | wps 53326.7 | wpb 2691.4 | bsz 201.4 | num_updates 43856 | best_loss 5.45\n",
      "2021-10-08 12:22:40 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:22:44 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint34.pt (epoch 34 @ 43856 updates, score 5.49) (writing took 3.555626507000852 seconds)\n",
      "2021-10-08 12:22:44 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
      "2021-10-08 12:22:44 | INFO | train | epoch 034 | loss 3.849 | nll_loss 2.154 | ppl 4.45 | wps 23168.9 | ups 6.27 | wpb 3695.9 | bsz 249.8 | num_updates 43856 | lr 0.000151003 | gnorm 1.159 | loss_scale 16 | train_wall 192 | wall 7112\n",
      "2021-10-08 12:22:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=35/shard_epoch=34\n",
      "2021-10-08 12:22:44 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=35/shard_epoch=35\n",
      "2021-10-08 12:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:22:44 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005514\n",
      "2021-10-08 12:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106182\n",
      "2021-10-08 12:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:44 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[46587]\n",
      "2021-10-08 12:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009588\n",
      "2021-10-08 12:22:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.932928\n",
      "2021-10-08 12:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.049746\n",
      "2021-10-08 12:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087274\n",
      "2021-10-08 12:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:45 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[46587]\n",
      "2021-10-08 12:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008657\n",
      "2021-10-08 12:22:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.882286\n",
      "2021-10-08 12:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.979179\n",
      "2021-10-08 12:22:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 035:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:22:46 | INFO | fairseq.trainer | begin training epoch 35\n",
      "epoch 035: 100%|9| 1289/1290 [03:18<00:00,  6.81it/s, loss=3.918, nll_loss=2.2322021-10-08 12:26:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001759\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063513\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.046562\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112833\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001670\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060050\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041075\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103616\n",
      "2021-10-08 12:26:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 035 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.70it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.28it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.01it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 10.79it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.47it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.28it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.87it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.44it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.83it/s]\u001b[A\n",
      "epoch 035 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:26:06 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.582 | nll_loss 4.056 | ppl 16.63 | wps 53496.5 | wpb 2691.4 | bsz 201.4 | num_updates 45146 | best_loss 5.45\n",
      "2021-10-08 12:26:06 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:26:10 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint35.pt (epoch 35 @ 45146 updates, score 5.582) (writing took 3.519393549999222 seconds)\n",
      "2021-10-08 12:26:10 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
      "2021-10-08 12:26:10 | INFO | train | epoch 035 | loss 3.828 | nll_loss 2.13 | ppl 4.38 | wps 23173.5 | ups 6.27 | wpb 3695.9 | bsz 249.8 | num_updates 45146 | lr 0.00014883 | gnorm 1.166 | loss_scale 16 | train_wall 192 | wall 7318\n",
      "2021-10-08 12:26:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=36/shard_epoch=35\n",
      "2021-10-08 12:26:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=36/shard_epoch=36\n",
      "2021-10-08 12:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:26:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005002\n",
      "2021-10-08 12:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.102727\n",
      "2021-10-08 12:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:10 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[198399]\n",
      "2021-10-08 12:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009683\n",
      "2021-10-08 12:26:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.922594\n",
      "2021-10-08 12:26:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.035941\n",
      "2021-10-08 12:26:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:26:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085528\n",
      "2021-10-08 12:26:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:11 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[198399]\n",
      "2021-10-08 12:26:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010012\n",
      "2021-10-08 12:26:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:26:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.936192\n",
      "2021-10-08 12:26:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.032700\n",
      "2021-10-08 12:26:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 036:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:26:12 | INFO | fairseq.trainer | begin training epoch 36\n",
      "epoch 036: 100%|9| 1289/1290 [03:17<00:00,  6.50it/s, loss=3.795, nll_loss=2.0952021-10-08 12:29:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001916\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060412\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041541\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104549\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001574\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059744\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040297\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102266\n",
      "2021-10-08 12:29:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 036 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.59it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.32it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.10it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.86it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.50it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.88it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.08it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.46it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.36it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.79it/s]\u001b[A\n",
      "epoch 036 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.08it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:29:31 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.49 | nll_loss 3.941 | ppl 15.36 | wps 54137.7 | wpb 2691.4 | bsz 201.4 | num_updates 46436 | best_loss 5.45\n",
      "2021-10-08 12:29:31 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:29:35 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint36.pt (epoch 36 @ 46436 updates, score 5.49) (writing took 3.7824768090013094 seconds)\n",
      "2021-10-08 12:29:37 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
      "2021-10-08 12:29:37 | INFO | train | epoch 036 | loss 3.808 | nll_loss 2.107 | ppl 4.31 | wps 23036.3 | ups 6.23 | wpb 3695.9 | bsz 249.8 | num_updates 46436 | lr 0.000146748 | gnorm 1.174 | loss_scale 16 | train_wall 191 | wall 7524\n",
      "2021-10-08 12:29:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=37/shard_epoch=36\n",
      "2021-10-08 12:29:37 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=37/shard_epoch=37\n",
      "2021-10-08 12:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:29:37 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.006412\n",
      "2021-10-08 12:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108520\n",
      "2021-10-08 12:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:37 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[32581]\n",
      "2021-10-08 12:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009657\n",
      "2021-10-08 12:29:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.942917\n",
      "2021-10-08 12:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.062051\n",
      "2021-10-08 12:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086367\n",
      "2021-10-08 12:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:38 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[32581]\n",
      "2021-10-08 12:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008033\n",
      "2021-10-08 12:29:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.911196\n",
      "2021-10-08 12:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.006532\n",
      "2021-10-08 12:29:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 037:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:29:39 | INFO | fairseq.trainer | begin training epoch 37\n",
      "epoch 037: 100%|9| 1289/1290 [03:17<00:00,  6.63it/s, loss=3.735, nll_loss=2.0272021-10-08 12:32:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001727\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060806\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044680\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108186\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001741\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062017\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042066\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106808\n",
      "2021-10-08 12:32:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 037 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.24it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.93it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.70it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.46it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.11it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.53it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.72it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.16it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.10it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.48it/s]\u001b[A\n",
      "epoch 037 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.84it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:32:59 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.536 | nll_loss 3.997 | ppl 15.96 | wps 53750.5 | wpb 2691.4 | bsz 201.4 | num_updates 47726 | best_loss 5.45\n",
      "2021-10-08 12:32:59 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:33:57 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint37.pt (epoch 37 @ 47726 updates, score 5.536) (writing took 58.549651261999315 seconds)\n",
      "2021-10-08 12:33:57 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
      "2021-10-08 12:33:57 | INFO | train | epoch 037 | loss 3.792 | nll_loss 2.088 | ppl 4.25 | wps 18301.7 | ups 4.95 | wpb 3695.9 | bsz 249.8 | num_updates 47726 | lr 0.000144751 | gnorm 1.185 | loss_scale 16 | train_wall 191 | wall 7785\n",
      "2021-10-08 12:33:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=38/shard_epoch=37\n",
      "2021-10-08 12:33:57 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=38/shard_epoch=38\n",
      "2021-10-08 12:33:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:33:57 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004925\n",
      "2021-10-08 12:33:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.104686\n",
      "2021-10-08 12:33:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:33:58 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[309529]\n",
      "2021-10-08 12:33:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007981\n",
      "2021-10-08 12:33:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:33:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.920563\n",
      "2021-10-08 12:33:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.034229\n",
      "2021-10-08 12:33:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:33:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:33:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085093\n",
      "2021-10-08 12:33:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:33:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[309529]\n",
      "2021-10-08 12:33:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007536\n",
      "2021-10-08 12:33:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:33:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.891209\n",
      "2021-10-08 12:33:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.984835\n",
      "2021-10-08 12:33:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 038:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:33:59 | INFO | fairseq.trainer | begin training epoch 38\n",
      "epoch 038: 100%|9| 1289/1290 [03:15<00:00,  6.33it/s, loss=3.873, nll_loss=2.1792021-10-08 12:37:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001695\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062535\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041344\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106274\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001462\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059991\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.039945\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.101995\n",
      "2021-10-08 12:37:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 038 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.13it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.93it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.72it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.44it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 14.01it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.32it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.76it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.62it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.96it/s]\u001b[A\n",
      "epoch 038 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.69it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:37:16 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.461 | nll_loss 3.905 | ppl 14.98 | wps 53929.9 | wpb 2691.4 | bsz 201.4 | num_updates 49016 | best_loss 5.45\n",
      "2021-10-08 12:37:16 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:37:20 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint38.pt (epoch 38 @ 49016 updates, score 5.461) (writing took 3.3739591749999818 seconds)\n",
      "2021-10-08 12:37:20 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
      "2021-10-08 12:37:20 | INFO | train | epoch 038 | loss 3.777 | nll_loss 2.071 | ppl 4.2 | wps 23545.8 | ups 6.37 | wpb 3695.9 | bsz 249.8 | num_updates 49016 | lr 0.000142834 | gnorm 1.197 | loss_scale 16 | train_wall 190 | wall 7987\n",
      "2021-10-08 12:37:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=39/shard_epoch=38\n",
      "2021-10-08 12:37:20 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=39/shard_epoch=39\n",
      "2021-10-08 12:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:37:20 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004952\n",
      "2021-10-08 12:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.100338\n",
      "2021-10-08 12:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:20 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[218051]\n",
      "2021-10-08 12:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008144\n",
      "2021-10-08 12:37:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.144058\n",
      "2021-10-08 12:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.253582\n",
      "2021-10-08 12:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.081162\n",
      "2021-10-08 12:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:21 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[218051]\n",
      "2021-10-08 12:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007909\n",
      "2021-10-08 12:37:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.075406\n",
      "2021-10-08 12:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.165452\n",
      "2021-10-08 12:37:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 039:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:37:22 | INFO | fairseq.trainer | begin training epoch 39\n",
      "epoch 039: 100%|9| 1289/1290 [03:17<00:00,  6.66it/s, loss=3.691, nll_loss=1.9762021-10-08 12:40:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001724\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061013\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041660\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105109\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001560\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063676\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041372\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107554\n",
      "2021-10-08 12:40:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 039 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.70it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.43it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.23it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.94it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.54it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.94it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 17.04it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.46it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.87it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 19.16it/s]\u001b[A\n",
      "epoch 039 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.32it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:40:42 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.539 | nll_loss 4.009 | ppl 16.1 | wps 53827.5 | wpb 2691.4 | bsz 201.4 | num_updates 50306 | best_loss 5.45\n",
      "2021-10-08 12:40:42 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:40:45 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint39.pt (epoch 39 @ 50306 updates, score 5.539) (writing took 3.5855386620005447 seconds)\n",
      "2021-10-08 12:40:45 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
      "2021-10-08 12:40:45 | INFO | train | epoch 039 | loss 3.76 | nll_loss 2.051 | ppl 4.14 | wps 23215.1 | ups 6.28 | wpb 3695.9 | bsz 249.8 | num_updates 50306 | lr 0.000140991 | gnorm 1.197 | loss_scale 16 | train_wall 192 | wall 8193\n",
      "2021-10-08 12:40:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=40/shard_epoch=39\n",
      "2021-10-08 12:40:45 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=40/shard_epoch=40\n",
      "2021-10-08 12:40:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:40:45 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005270\n",
      "2021-10-08 12:40:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108974\n",
      "2021-10-08 12:40:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:45 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[154612]\n",
      "2021-10-08 12:40:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008370\n",
      "2021-10-08 12:40:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.895303\n",
      "2021-10-08 12:40:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.013625\n",
      "2021-10-08 12:40:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:40:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.080806\n",
      "2021-10-08 12:40:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:46 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[154612]\n",
      "2021-10-08 12:40:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007853\n",
      "2021-10-08 12:40:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:40:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.884339\n",
      "2021-10-08 12:40:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.973943\n",
      "2021-10-08 12:40:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 040:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:40:47 | INFO | fairseq.trainer | begin training epoch 40\n",
      "epoch 040: 100%|9| 1289/1290 [03:16<00:00,  6.51it/s, loss=3.702, nll_loss=1.9882021-10-08 12:44:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001932\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061493\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042549\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106669\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001739\n",
      "2021-10-08 12:44:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059880\n",
      "2021-10-08 12:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042063\n",
      "2021-10-08 12:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104338\n",
      "2021-10-08 12:44:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 040 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.18it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.86it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.64it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.41it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.08it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 11.17it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 12.84it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 14.66it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 16.14it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 17.14it/s]\u001b[A\n",
      "epoch 040 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 19.12it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:44:06 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.473 | nll_loss 3.925 | ppl 15.19 | wps 49091.1 | wpb 2691.4 | bsz 201.4 | num_updates 51596 | best_loss 5.45\n",
      "2021-10-08 12:44:06 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:44:10 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint40.pt (epoch 40 @ 51596 updates, score 5.473) (writing took 3.427840037998976 seconds)\n",
      "2021-10-08 12:44:10 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
      "2021-10-08 12:44:10 | INFO | train | epoch 040 | loss 3.743 | nll_loss 2.032 | ppl 4.09 | wps 23323.1 | ups 6.31 | wpb 3695.9 | bsz 249.8 | num_updates 51596 | lr 0.000139217 | gnorm 1.209 | loss_scale 16 | train_wall 191 | wall 8397\n",
      "2021-10-08 12:44:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=41/shard_epoch=40\n",
      "2021-10-08 12:44:10 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=41/shard_epoch=41\n",
      "2021-10-08 12:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:44:10 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004883\n",
      "2021-10-08 12:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105450\n",
      "2021-10-08 12:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:10 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[280484]\n",
      "2021-10-08 12:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007755\n",
      "2021-10-08 12:44:10 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.907548\n",
      "2021-10-08 12:44:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.021723\n",
      "2021-10-08 12:44:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:44:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087844\n",
      "2021-10-08 12:44:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:11 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[280484]\n",
      "2021-10-08 12:44:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007992\n",
      "2021-10-08 12:44:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.910211\n",
      "2021-10-08 12:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.006989\n",
      "2021-10-08 12:44:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 041:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:44:12 | INFO | fairseq.trainer | begin training epoch 41\n",
      "epoch 041: 100%|9| 1289/1290 [03:17<00:00,  6.93it/s, loss=3.795, nll_loss=2.0882021-10-08 12:47:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001701\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065087\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041078\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108659\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001380\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060207\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041144\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103337\n",
      "2021-10-08 12:47:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 041 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.56it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.29it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.10it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.85it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.84it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.45it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.44it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.88it/s]\u001b[A\n",
      "epoch 041 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:47:31 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.585 | nll_loss 4.061 | ppl 16.7 | wps 54148.5 | wpb 2691.4 | bsz 201.4 | num_updates 52886 | best_loss 5.45\n",
      "2021-10-08 12:47:31 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:47:35 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint41.pt (epoch 41 @ 52886 updates, score 5.585) (writing took 3.4280995870012703 seconds)\n",
      "2021-10-08 12:47:35 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
      "2021-10-08 12:47:35 | INFO | train | epoch 041 | loss 3.728 | nll_loss 2.015 | ppl 4.04 | wps 23231.4 | ups 6.29 | wpb 3695.9 | bsz 249.8 | num_updates 52886 | lr 0.000137509 | gnorm 1.208 | loss_scale 16 | train_wall 192 | wall 8603\n",
      "2021-10-08 12:47:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=42/shard_epoch=41\n",
      "2021-10-08 12:47:35 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=42/shard_epoch=42\n",
      "2021-10-08 12:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:47:35 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004981\n",
      "2021-10-08 12:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108191\n",
      "2021-10-08 12:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:35 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[1215]\n",
      "2021-10-08 12:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008123\n",
      "2021-10-08 12:47:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.036778\n",
      "2021-10-08 12:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.154085\n",
      "2021-10-08 12:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085192\n",
      "2021-10-08 12:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:36 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[1215]\n",
      "2021-10-08 12:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007879\n",
      "2021-10-08 12:47:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.160003\n",
      "2021-10-08 12:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.254111\n",
      "2021-10-08 12:47:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 042:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:47:37 | INFO | fairseq.trainer | begin training epoch 42\n",
      "epoch 042: 100%|9| 1289/1290 [03:16<00:00,  6.54it/s, loss=3.794, nll_loss=2.0892021-10-08 12:50:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001952\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061269\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040951\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104860\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001351\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060973\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040894\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103901\n",
      "2021-10-08 12:50:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 042 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.36it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.07it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.86it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.63it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.26it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.67it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.89it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.29it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.26it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.74it/s]\u001b[A\n",
      "epoch 042 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:50:56 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.533 | nll_loss 3.999 | ppl 15.99 | wps 54150.6 | wpb 2691.4 | bsz 201.4 | num_updates 54176 | best_loss 5.45\n",
      "2021-10-08 12:50:56 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:50:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint42.pt (epoch 42 @ 54176 updates, score 5.533) (writing took 3.513923476000855 seconds)\n",
      "2021-10-08 12:50:59 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
      "2021-10-08 12:50:59 | INFO | train | epoch 042 | loss 3.714 | nll_loss 1.999 | ppl 4 | wps 23341.8 | ups 6.32 | wpb 3695.9 | bsz 249.8 | num_updates 54176 | lr 0.000135862 | gnorm 1.221 | loss_scale 32 | train_wall 191 | wall 8807\n",
      "2021-10-08 12:50:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=43/shard_epoch=42\n",
      "2021-10-08 12:50:59 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=43/shard_epoch=43\n",
      "2021-10-08 12:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:50:59 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004887\n",
      "2021-10-08 12:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105563\n",
      "2021-10-08 12:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:50:59 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[240951]\n",
      "2021-10-08 12:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008146\n",
      "2021-10-08 12:50:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.872171\n",
      "2021-10-08 12:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.986910\n",
      "2021-10-08 12:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084332\n",
      "2021-10-08 12:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:51:00 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[240951]\n",
      "2021-10-08 12:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008276\n",
      "2021-10-08 12:51:00 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:51:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.843425\n",
      "2021-10-08 12:51:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.937029\n",
      "2021-10-08 12:51:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 043:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:51:01 | INFO | fairseq.trainer | begin training epoch 43\n",
      "epoch 043: 100%|9| 1289/1290 [03:15<00:00,  6.68it/s, loss=3.706, nll_loss=1.9892021-10-08 12:54:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001838\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.062112\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.044138\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.109053\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001637\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060779\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041556\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104775\n",
      "2021-10-08 12:54:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 043 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.12it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.90it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.69it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.41it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.95it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.25it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.33it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.66it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.95it/s]\u001b[A\n",
      "epoch 043 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.12it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:54:19 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 5.54 | nll_loss 4.003 | ppl 16.03 | wps 53790.5 | wpb 2691.4 | bsz 201.4 | num_updates 55466 | best_loss 5.45\n",
      "2021-10-08 12:54:19 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:54:22 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint43.pt (epoch 43 @ 55466 updates, score 5.54) (writing took 3.426599813001303 seconds)\n",
      "2021-10-08 12:54:22 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
      "2021-10-08 12:54:22 | INFO | train | epoch 043 | loss 3.699 | nll_loss 1.981 | ppl 3.95 | wps 23464.9 | ups 6.35 | wpb 3695.9 | bsz 249.8 | num_updates 55466 | lr 0.000134272 | gnorm 1.233 | loss_scale 32 | train_wall 190 | wall 9010\n",
      "2021-10-08 12:54:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=44/shard_epoch=43\n",
      "2021-10-08 12:54:22 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=44/shard_epoch=44\n",
      "2021-10-08 12:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:54:22 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004950\n",
      "2021-10-08 12:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.108234\n",
      "2021-10-08 12:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:22 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[3846]\n",
      "2021-10-08 12:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007649\n",
      "2021-10-08 12:54:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.885249\n",
      "2021-10-08 12:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.002157\n",
      "2021-10-08 12:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086407\n",
      "2021-10-08 12:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:23 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[3846]\n",
      "2021-10-08 12:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007724\n",
      "2021-10-08 12:54:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.966215\n",
      "2021-10-08 12:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.061330\n",
      "2021-10-08 12:54:24 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 044:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:54:24 | INFO | fairseq.trainer | begin training epoch 44\n",
      "epoch 044: 100%|9| 1289/1290 [03:18<00:00,  6.58it/s, loss=3.826, nll_loss=2.1222021-10-08 12:57:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001676\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059242\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041359\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102911\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001431\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063633\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042526\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108340\n",
      "2021-10-08 12:57:43 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 044 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.12it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.91it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.73it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.44it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.25it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.71it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.58it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.96it/s]\u001b[A\n",
      "epoch 044 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 12:57:45 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 5.615 | nll_loss 4.099 | ppl 17.14 | wps 53950.4 | wpb 2691.4 | bsz 201.4 | num_updates 56756 | best_loss 5.45\n",
      "2021-10-08 12:57:45 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 12:57:48 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint44.pt (epoch 44 @ 56756 updates, score 5.615) (writing took 3.4363435590003064 seconds)\n",
      "2021-10-08 12:57:48 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
      "2021-10-08 12:57:48 | INFO | train | epoch 044 | loss 3.686 | nll_loss 1.967 | ppl 3.91 | wps 23178.1 | ups 6.27 | wpb 3695.9 | bsz 249.8 | num_updates 56756 | lr 0.000132738 | gnorm 1.227 | loss_scale 32 | train_wall 192 | wall 9216\n",
      "2021-10-08 12:57:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=45/shard_epoch=44\n",
      "2021-10-08 12:57:48 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=45/shard_epoch=45\n",
      "2021-10-08 12:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:57:48 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004964\n",
      "2021-10-08 12:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106103\n",
      "2021-10-08 12:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[58582]\n",
      "2021-10-08 12:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008705\n",
      "2021-10-08 12:57:48 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.903527\n",
      "2021-10-08 12:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.019265\n",
      "2021-10-08 12:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 12:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086797\n",
      "2021-10-08 12:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[58582]\n",
      "2021-10-08 12:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.010030\n",
      "2021-10-08 12:57:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 12:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902615\n",
      "2021-10-08 12:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.000619\n",
      "2021-10-08 12:57:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 045:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 12:57:50 | INFO | fairseq.trainer | begin training epoch 45\n",
      "epoch 045:  63%|6| 818/1290 [02:05<01:11,  6.57it/s, loss=3.581, nll_loss=1.85, 2021-10-08 12:59:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 045: 100%|9| 1289/1290 [03:16<00:00,  6.43it/s, loss=3.655, nll_loss=1.9332021-10-08 13:01:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001680\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060022\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041982\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104351\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001655\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060878\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040896\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104149\n",
      "2021-10-08 13:01:07 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 045 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.66it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.27it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  8.99it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 10.68it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 12.41it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 13.97it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.27it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 18.03it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "epoch 045 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.08it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:01:09 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.488 | nll_loss 3.947 | ppl 15.42 | wps 54078.9 | wpb 2691.4 | bsz 201.4 | num_updates 58045 | best_loss 5.45\n",
      "2021-10-08 13:01:09 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:01:12 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint45.pt (epoch 45 @ 58045 updates, score 5.488) (writing took 3.334026229000301 seconds)\n",
      "2021-10-08 13:01:12 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
      "2021-10-08 13:01:12 | INFO | train | epoch 045 | loss 3.672 | nll_loss 1.95 | ppl 3.86 | wps 23354.6 | ups 6.32 | wpb 3697.1 | bsz 249.5 | num_updates 58045 | lr 0.000131256 | gnorm 1.239 | loss_scale 16 | train_wall 191 | wall 9420\n",
      "2021-10-08 13:01:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=46/shard_epoch=45\n",
      "2021-10-08 13:01:12 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=46/shard_epoch=46\n",
      "2021-10-08 13:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:01:12 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004930\n",
      "2021-10-08 13:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106722\n",
      "2021-10-08 13:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:12 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[254746]\n",
      "2021-10-08 13:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008457\n",
      "2021-10-08 13:01:12 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.910231\n",
      "2021-10-08 13:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.026378\n",
      "2021-10-08 13:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086313\n",
      "2021-10-08 13:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:13 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[254746]\n",
      "2021-10-08 13:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008584\n",
      "2021-10-08 13:01:13 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:01:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902386\n",
      "2021-10-08 13:01:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.998248\n",
      "2021-10-08 13:01:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 046:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:01:14 | INFO | fairseq.trainer | begin training epoch 46\n",
      "epoch 046: 100%|9| 1289/1290 [03:16<00:00,  6.38it/s, loss=3.798, nll_loss=2.0922021-10-08 13:04:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001737\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060876\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040889\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104213\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001477\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061007\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040971\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104065\n",
      "2021-10-08 13:04:31 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 046 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.00it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.78it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:01, 10.59it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.32it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.90it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.20it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.65it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.53it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.93it/s]\u001b[A\n",
      "epoch 046 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:04:32 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.558 | nll_loss 4.026 | ppl 16.29 | wps 53868.5 | wpb 2691.4 | bsz 201.4 | num_updates 59335 | best_loss 5.45\n",
      "2021-10-08 13:04:32 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:04:36 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint46.pt (epoch 46 @ 59335 updates, score 5.558) (writing took 3.378615001000071 seconds)\n",
      "2021-10-08 13:04:36 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
      "2021-10-08 13:04:36 | INFO | train | epoch 046 | loss 3.659 | nll_loss 1.935 | ppl 3.82 | wps 23414.4 | ups 6.34 | wpb 3695.9 | bsz 249.8 | num_updates 59335 | lr 0.000129821 | gnorm 1.247 | loss_scale 16 | train_wall 191 | wall 9623\n",
      "2021-10-08 13:04:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=47/shard_epoch=46\n",
      "2021-10-08 13:04:36 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=47/shard_epoch=47\n",
      "2021-10-08 13:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:04:36 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005006\n",
      "2021-10-08 13:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106173\n",
      "2021-10-08 13:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:36 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[124522]\n",
      "2021-10-08 13:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008411\n",
      "2021-10-08 13:04:36 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.888143\n",
      "2021-10-08 13:04:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.003651\n",
      "2021-10-08 13:04:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:04:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084986\n",
      "2021-10-08 13:04:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:37 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[124522]\n",
      "2021-10-08 13:04:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008362\n",
      "2021-10-08 13:04:37 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.894548\n",
      "2021-10-08 13:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.988839\n",
      "2021-10-08 13:04:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 047:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:04:38 | INFO | fairseq.trainer | begin training epoch 47\n",
      "epoch 047: 100%|9| 1289/1290 [03:17<00:00,  6.75it/s, loss=3.65, nll_loss=1.926,2021-10-08 13:07:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001746\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063327\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041747\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107742\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001718\n",
      "2021-10-08 13:07:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064479\n",
      "2021-10-08 13:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043154\n",
      "2021-10-08 13:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110206\n",
      "2021-10-08 13:07:56 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 047 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.28it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.99it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.78it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.57it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.22it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.64it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.24it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.22it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.68it/s]\u001b[A\n",
      "epoch 047 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:07:57 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.609 | nll_loss 4.083 | ppl 16.95 | wps 54114.1 | wpb 2691.4 | bsz 201.4 | num_updates 60625 | best_loss 5.45\n",
      "2021-10-08 13:07:57 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:08:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint47.pt (epoch 47 @ 60625 updates, score 5.609) (writing took 3.3849993089988857 seconds)\n",
      "2021-10-08 13:08:01 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
      "2021-10-08 13:08:01 | INFO | train | epoch 047 | loss 3.645 | nll_loss 1.919 | ppl 3.78 | wps 23280 | ups 6.3 | wpb 3695.9 | bsz 249.8 | num_updates 60625 | lr 0.000128432 | gnorm 1.25 | loss_scale 16 | train_wall 192 | wall 9828\n",
      "2021-10-08 13:08:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=48/shard_epoch=47\n",
      "2021-10-08 13:08:01 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=48/shard_epoch=48\n",
      "2021-10-08 13:08:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:08:01 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004994\n",
      "2021-10-08 13:08:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106571\n",
      "2021-10-08 13:08:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:08:01 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[176012]\n",
      "2021-10-08 13:08:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007700\n",
      "2021-10-08 13:08:01 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.899851\n",
      "2021-10-08 13:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.015104\n",
      "2021-10-08 13:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083498\n",
      "2021-10-08 13:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:08:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[176012]\n",
      "2021-10-08 13:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008268\n",
      "2021-10-08 13:08:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.906825\n",
      "2021-10-08 13:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.999533\n",
      "2021-10-08 13:08:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 048:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:08:03 | INFO | fairseq.trainer | begin training epoch 48\n",
      "epoch 048: 100%|9| 1289/1290 [03:17<00:00,  6.65it/s, loss=3.715, nll_loss=1.9972021-10-08 13:11:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001871\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060634\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043450\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.106728\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001595\n",
      "2021-10-08 13:11:20 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060221\n",
      "2021-10-08 13:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042172\n",
      "2021-10-08 13:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104673\n",
      "2021-10-08 13:11:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 048 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.14it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.83it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.62it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01,  9.83it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 11.58it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:01, 13.23it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  57%|###4  | 16/28 [00:01<00:00, 14.70it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 16.32it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 17.48it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.16it/s]\u001b[A\n",
      "epoch 048 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.64it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:11:22 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.686 | nll_loss 4.174 | ppl 18.05 | wps 49126.5 | wpb 2691.4 | bsz 201.4 | num_updates 61915 | best_loss 5.45\n",
      "2021-10-08 13:11:22 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:11:26 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint48.pt (epoch 48 @ 61915 updates, score 5.686) (writing took 3.3701052739997976 seconds)\n",
      "2021-10-08 13:11:26 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
      "2021-10-08 13:11:26 | INFO | train | epoch 048 | loss 3.633 | nll_loss 1.905 | ppl 3.75 | wps 23249 | ups 6.29 | wpb 3695.9 | bsz 249.8 | num_updates 61915 | lr 0.000127087 | gnorm 1.264 | loss_scale 16 | train_wall 192 | wall 10033\n",
      "2021-10-08 13:11:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=49/shard_epoch=48\n",
      "2021-10-08 13:11:26 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=49/shard_epoch=49\n",
      "2021-10-08 13:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:11:26 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004912\n",
      "2021-10-08 13:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107988\n",
      "2021-10-08 13:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:26 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[217626]\n",
      "2021-10-08 13:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009132\n",
      "2021-10-08 13:11:26 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.892246\n",
      "2021-10-08 13:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.010347\n",
      "2021-10-08 13:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086768\n",
      "2021-10-08 13:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:27 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[217626]\n",
      "2021-10-08 13:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008691\n",
      "2021-10-08 13:11:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902101\n",
      "2021-10-08 13:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.998496\n",
      "2021-10-08 13:11:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 049:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:11:28 | INFO | fairseq.trainer | begin training epoch 49\n",
      "epoch 049: 100%|9| 1289/1290 [03:16<00:00,  6.51it/s, loss=3.646, nll_loss=1.9232021-10-08 13:14:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001609\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060359\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041196\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103840\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001444\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059712\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041251\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103047\n",
      "2021-10-08 13:14:44 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 049 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  7.36it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  9.14it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  21%|#5     | 6/28 [00:00<00:02, 10.88it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  29%|##     | 8/28 [00:00<00:01, 12.57it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 14.14it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 15.39it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 17.23it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  64%|###8  | 18/28 [00:00<00:00, 17.95it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 19.18it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 19.37it/s]\u001b[A\n",
      "epoch 049 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:14:46 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.588 | nll_loss 4.064 | ppl 16.72 | wps 53400.9 | wpb 2691.4 | bsz 201.4 | num_updates 63205 | best_loss 5.45\n",
      "2021-10-08 13:14:46 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:14:49 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint49.pt (epoch 49 @ 63205 updates, score 5.588) (writing took 3.471991477999836 seconds)\n",
      "2021-10-08 13:14:49 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
      "2021-10-08 13:14:49 | INFO | train | epoch 049 | loss 3.622 | nll_loss 1.892 | ppl 3.71 | wps 23398.2 | ups 6.33 | wpb 3695.9 | bsz 249.8 | num_updates 63205 | lr 0.000125784 | gnorm 1.268 | loss_scale 16 | train_wall 191 | wall 10237\n",
      "2021-10-08 13:14:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=50/shard_epoch=49\n",
      "2021-10-08 13:14:49 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=50/shard_epoch=50\n",
      "2021-10-08 13:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:14:49 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004698\n",
      "2021-10-08 13:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107566\n",
      "2021-10-08 13:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:49 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[32528]\n",
      "2021-10-08 13:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008677\n",
      "2021-10-08 13:14:49 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.965120\n",
      "2021-10-08 13:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.082402\n",
      "2021-10-08 13:14:50 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085057\n",
      "2021-10-08 13:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:51 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[32528]\n",
      "2021-10-08 13:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008029\n",
      "2021-10-08 13:14:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.014244\n",
      "2021-10-08 13:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.108346\n",
      "2021-10-08 13:14:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 050:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:14:52 | INFO | fairseq.trainer | begin training epoch 50\n",
      "epoch 050: 100%|9| 1289/1290 [03:17<00:00,  6.36it/s, loss=3.656, nll_loss=1.9322021-10-08 13:18:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001983\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060282\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041357\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104293\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001348\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059690\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041183\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102939\n",
      "2021-10-08 13:18:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 050 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.75it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.38it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.15it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 10.90it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 12.63it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 14.09it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 15.39it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.06it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 17.84it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.43it/s]\u001b[A\n",
      "epoch 050 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:18:11 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.568 | nll_loss 4.044 | ppl 16.5 | wps 54020.8 | wpb 2691.4 | bsz 201.4 | num_updates 64495 | best_loss 5.45\n",
      "2021-10-08 13:18:11 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:18:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint50.pt (epoch 50 @ 64495 updates, score 5.568) (writing took 3.4111745019999944 seconds)\n",
      "2021-10-08 13:18:14 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
      "2021-10-08 13:18:14 | INFO | train | epoch 050 | loss 3.611 | nll_loss 1.88 | ppl 3.68 | wps 23286 | ups 6.3 | wpb 3695.9 | bsz 249.8 | num_updates 64495 | lr 0.000124519 | gnorm 1.282 | loss_scale 16 | train_wall 191 | wall 10442\n",
      "2021-10-08 13:18:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=51/shard_epoch=50\n",
      "2021-10-08 13:18:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=51/shard_epoch=51\n",
      "2021-10-08 13:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:18:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004680\n",
      "2021-10-08 13:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106007\n",
      "2021-10-08 13:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:14 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[52206]\n",
      "2021-10-08 13:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008400\n",
      "2021-10-08 13:18:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.851618\n",
      "2021-10-08 13:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.966949\n",
      "2021-10-08 13:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086102\n",
      "2021-10-08 13:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:15 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[52206]\n",
      "2021-10-08 13:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008216\n",
      "2021-10-08 13:18:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.847597\n",
      "2021-10-08 13:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.942840\n",
      "2021-10-08 13:18:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 051:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:18:16 | INFO | fairseq.trainer | begin training epoch 51\n",
      "epoch 051: 100%|9| 1289/1290 [03:16<00:00,  6.71it/s, loss=3.747, nll_loss=2.0332021-10-08 13:21:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001764\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061968\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041186\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105775\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001490\n",
      "2021-10-08 13:21:32 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061905\n",
      "2021-10-08 13:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040601\n",
      "2021-10-08 13:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104695\n",
      "2021-10-08 13:21:33 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 051 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:03,  6.82it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.59it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.40it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.16it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.03it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.15it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.49it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.45it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.86it/s]\u001b[A\n",
      "epoch 051 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:21:34 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.566 | nll_loss 4.038 | ppl 16.43 | wps 53959.3 | wpb 2691.4 | bsz 201.4 | num_updates 65785 | best_loss 5.45\n",
      "2021-10-08 13:21:34 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:21:38 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint51.pt (epoch 51 @ 65785 updates, score 5.566) (writing took 3.448805652000374 seconds)\n",
      "2021-10-08 13:21:38 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
      "2021-10-08 13:21:38 | INFO | train | epoch 051 | loss 3.599 | nll_loss 1.866 | ppl 3.65 | wps 23432.3 | ups 6.34 | wpb 3695.9 | bsz 249.8 | num_updates 65785 | lr 0.000123292 | gnorm 1.284 | loss_scale 16 | train_wall 190 | wall 10645\n",
      "2021-10-08 13:21:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=52/shard_epoch=51\n",
      "2021-10-08 13:21:38 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=52/shard_epoch=52\n",
      "2021-10-08 13:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:21:38 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004958\n",
      "2021-10-08 13:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105125\n",
      "2021-10-08 13:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:38 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[66718]\n",
      "2021-10-08 13:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007999\n",
      "2021-10-08 13:21:38 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.902492\n",
      "2021-10-08 13:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.016652\n",
      "2021-10-08 13:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.085691\n",
      "2021-10-08 13:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:39 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[66718]\n",
      "2021-10-08 13:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008175\n",
      "2021-10-08 13:21:39 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.883111\n",
      "2021-10-08 13:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.977919\n",
      "2021-10-08 13:21:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 052:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:21:40 | INFO | fairseq.trainer | begin training epoch 52\n",
      "epoch 052: 100%|9| 1289/1290 [03:16<00:00,  6.70it/s, loss=3.568, nll_loss=1.8352021-10-08 13:24:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001781\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060598\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.045257\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.108354\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001567\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.063099\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040571\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105978\n",
      "2021-10-08 13:24:57 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 052 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.51it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.22it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.00it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01, 11.73it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 13.37it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 14.82it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 15.99it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  64%|###8  | 18/28 [00:00<00:00, 17.04it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.45it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.95it/s]\u001b[A\n",
      "epoch 052 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:24:58 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.6 | nll_loss 4.076 | ppl 16.87 | wps 53772.1 | wpb 2691.4 | bsz 201.4 | num_updates 67075 | best_loss 5.45\n",
      "2021-10-08 13:24:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:25:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint52.pt (epoch 52 @ 67075 updates, score 5.6) (writing took 3.4594421239999065 seconds)\n",
      "2021-10-08 13:25:02 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
      "2021-10-08 13:25:02 | INFO | train | epoch 052 | loss 3.588 | nll_loss 1.853 | ppl 3.61 | wps 23349 | ups 6.32 | wpb 3695.9 | bsz 249.8 | num_updates 67075 | lr 0.000122101 | gnorm 1.295 | loss_scale 16 | train_wall 191 | wall 10849\n",
      "2021-10-08 13:25:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=53/shard_epoch=52\n",
      "2021-10-08 13:25:02 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=53/shard_epoch=53\n",
      "2021-10-08 13:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:25:02 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004951\n",
      "2021-10-08 13:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.106463\n",
      "2021-10-08 13:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:25:02 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[306792]\n",
      "2021-10-08 13:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008724\n",
      "2021-10-08 13:25:02 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.955423\n",
      "2021-10-08 13:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.071584\n",
      "2021-10-08 13:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084627\n",
      "2021-10-08 13:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:25:03 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[306792]\n",
      "2021-10-08 13:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008086\n",
      "2021-10-08 13:25:03 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.920950\n",
      "2021-10-08 13:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.014637\n",
      "2021-10-08 13:25:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 053:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:25:04 | INFO | fairseq.trainer | begin training epoch 53\n",
      "epoch 053: 100%|9| 1289/1290 [03:17<00:00,  6.61it/s, loss=3.534, nll_loss=1.7962021-10-08 13:28:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001804\n",
      "2021-10-08 13:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061028\n",
      "2021-10-08 13:28:21 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041507\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104988\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001558\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059848\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040309\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102264\n",
      "2021-10-08 13:28:22 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 053 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.93it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.58it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.34it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.12it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 12.81it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.28it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.03it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.01it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.52it/s]\u001b[A\n",
      "epoch 053 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:28:23 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.598 | nll_loss 4.074 | ppl 16.85 | wps 54029.2 | wpb 2691.4 | bsz 201.4 | num_updates 68365 | best_loss 5.45\n",
      "2021-10-08 13:28:23 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:28:27 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint53.pt (epoch 53 @ 68365 updates, score 5.598) (writing took 3.4665381779996096 seconds)\n",
      "2021-10-08 13:28:27 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
      "2021-10-08 13:28:27 | INFO | train | epoch 053 | loss 3.577 | nll_loss 1.84 | ppl 3.58 | wps 23267.8 | ups 6.3 | wpb 3695.9 | bsz 249.8 | num_updates 68365 | lr 0.000120944 | gnorm 1.3 | loss_scale 16 | train_wall 192 | wall 11054\n",
      "2021-10-08 13:28:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=54/shard_epoch=53\n",
      "2021-10-08 13:28:27 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=54/shard_epoch=54\n",
      "2021-10-08 13:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:28:27 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004919\n",
      "2021-10-08 13:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105858\n",
      "2021-10-08 13:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:27 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[178695]\n",
      "2021-10-08 13:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008748\n",
      "2021-10-08 13:28:27 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.880395\n",
      "2021-10-08 13:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.995992\n",
      "2021-10-08 13:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086949\n",
      "2021-10-08 13:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:28 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[178695]\n",
      "2021-10-08 13:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008136\n",
      "2021-10-08 13:28:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:28:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.000847\n",
      "2021-10-08 13:28:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.096948\n",
      "2021-10-08 13:28:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 054:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:28:29 | INFO | fairseq.trainer | begin training epoch 54\n",
      "epoch 054: 100%|9| 1289/1290 [03:16<00:00,  6.56it/s, loss=3.718, nll_loss=2.0012021-10-08 13:31:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001777\n",
      "2021-10-08 13:31:45 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065515\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042718\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110769\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001771\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060660\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041556\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104968\n",
      "2021-10-08 13:31:46 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 054 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.19it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.85it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.61it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.38it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.05it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.49it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.74it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.17it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.09it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.55it/s]\u001b[A\n",
      "epoch 054 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.90it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:31:47 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.78 | nll_loss 4.286 | ppl 19.51 | wps 53570.2 | wpb 2691.4 | bsz 201.4 | num_updates 69655 | best_loss 5.45\n",
      "2021-10-08 13:31:47 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:31:51 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint54.pt (epoch 54 @ 69655 updates, score 5.78) (writing took 3.477788132000569 seconds)\n",
      "2021-10-08 13:31:51 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
      "2021-10-08 13:31:51 | INFO | train | epoch 054 | loss 3.568 | nll_loss 1.83 | ppl 3.56 | wps 23361.9 | ups 6.32 | wpb 3695.9 | bsz 249.8 | num_updates 69655 | lr 0.000119818 | gnorm 1.308 | loss_scale 16 | train_wall 191 | wall 11258\n",
      "2021-10-08 13:31:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=55/shard_epoch=54\n",
      "2021-10-08 13:31:51 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=55/shard_epoch=55\n",
      "2021-10-08 13:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:31:51 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004977\n",
      "2021-10-08 13:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.101886\n",
      "2021-10-08 13:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:51 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[197062]\n",
      "2021-10-08 13:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008028\n",
      "2021-10-08 13:31:51 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.914287\n",
      "2021-10-08 13:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.025183\n",
      "2021-10-08 13:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083088\n",
      "2021-10-08 13:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[197062]\n",
      "2021-10-08 13:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007813\n",
      "2021-10-08 13:31:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:31:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.898658\n",
      "2021-10-08 13:31:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.990554\n",
      "2021-10-08 13:31:53 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 055:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:31:53 | INFO | fairseq.trainer | begin training epoch 55\n",
      "epoch 055: 100%|9| 1289/1290 [03:15<00:00,  6.35it/s, loss=3.696, nll_loss=1.9732021-10-08 13:35:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001755\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.065126\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.043151\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.110820\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001623\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061920\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042651\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.107025\n",
      "2021-10-08 13:35:09 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 055 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.28it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.96it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.73it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.50it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.16it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.59it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.81it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.27it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.25it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.70it/s]\u001b[A\n",
      "epoch 055 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 18.94it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:35:10 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.645 | nll_loss 4.132 | ppl 17.54 | wps 53800.8 | wpb 2691.4 | bsz 201.4 | num_updates 70945 | best_loss 5.45\n",
      "2021-10-08 13:35:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:35:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint55.pt (epoch 55 @ 70945 updates, score 5.645) (writing took 3.4141274100002192 seconds)\n",
      "2021-10-08 13:35:14 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
      "2021-10-08 13:35:14 | INFO | train | epoch 055 | loss 3.556 | nll_loss 1.817 | ppl 3.52 | wps 23467 | ups 6.35 | wpb 3695.9 | bsz 249.8 | num_updates 70945 | lr 0.000118724 | gnorm 1.312 | loss_scale 16 | train_wall 190 | wall 11462\n",
      "2021-10-08 13:35:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=56/shard_epoch=55\n",
      "2021-10-08 13:35:14 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=56/shard_epoch=56\n",
      "2021-10-08 13:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:35:14 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005058\n",
      "2021-10-08 13:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.104375\n",
      "2021-10-08 13:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:14 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[211918]\n",
      "2021-10-08 13:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008771\n",
      "2021-10-08 13:35:14 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.899001\n",
      "2021-10-08 13:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.013117\n",
      "2021-10-08 13:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083304\n",
      "2021-10-08 13:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:15 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[211918]\n",
      "2021-10-08 13:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008353\n",
      "2021-10-08 13:35:15 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.898642\n",
      "2021-10-08 13:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.991247\n",
      "2021-10-08 13:35:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 056:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:35:16 | INFO | fairseq.trainer | begin training epoch 56\n",
      "epoch 056: 100%|9| 1289/1290 [03:18<00:00,  6.55it/s, loss=3.596, nll_loss=1.8652021-10-08 13:38:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001689\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060239\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041935\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104565\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001603\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059637\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041701\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.103599\n",
      "2021-10-08 13:38:35 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 056 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  5.95it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.59it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  8.12it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  32%|##2    | 9/28 [00:00<00:01,  9.82it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  39%|##3   | 11/28 [00:00<00:01, 11.58it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 13.23it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  54%|###2  | 15/28 [00:01<00:00, 14.61it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  61%|###6  | 17/28 [00:01<00:00, 15.82it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.46it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.08it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.62it/s]\u001b[A\n",
      "epoch 056 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:38:36 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 5.679 | nll_loss 4.165 | ppl 17.94 | wps 47948.9 | wpb 2691.4 | bsz 201.4 | num_updates 72235 | best_loss 5.45\n",
      "2021-10-08 13:38:36 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:38:40 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint56.pt (epoch 56 @ 72235 updates, score 5.679) (writing took 3.4833234190009534 seconds)\n",
      "2021-10-08 13:38:40 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
      "2021-10-08 13:38:40 | INFO | train | epoch 056 | loss 3.546 | nll_loss 1.805 | ppl 3.5 | wps 23139.8 | ups 6.26 | wpb 3695.9 | bsz 249.8 | num_updates 72235 | lr 0.000117659 | gnorm 1.32 | loss_scale 16 | train_wall 192 | wall 11668\n",
      "2021-10-08 13:38:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=57/shard_epoch=56\n",
      "2021-10-08 13:38:40 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=57/shard_epoch=57\n",
      "2021-10-08 13:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:38:40 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004881\n",
      "2021-10-08 13:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.104742\n",
      "2021-10-08 13:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:40 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[187950]\n",
      "2021-10-08 13:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008905\n",
      "2021-10-08 13:38:40 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.882253\n",
      "2021-10-08 13:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.996928\n",
      "2021-10-08 13:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083283\n",
      "2021-10-08 13:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:41 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[187950]\n",
      "2021-10-08 13:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008355\n",
      "2021-10-08 13:38:41 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.884763\n",
      "2021-10-08 13:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.977386\n",
      "2021-10-08 13:38:42 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 057:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:38:42 | INFO | fairseq.trainer | begin training epoch 57\n",
      "epoch 057: 100%|9| 1289/1290 [03:16<00:00,  6.56it/s, loss=3.581, nll_loss=1.8452021-10-08 13:41:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001676\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.064612\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.052304\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.119330\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001418\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.068065\n",
      "2021-10-08 13:41:58 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042689\n",
      "2021-10-08 13:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.112962\n",
      "2021-10-08 13:41:59 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 057 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.24it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:03,  7.93it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02,  9.73it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.53it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  46%|##7   | 13/28 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  54%|###2  | 15/28 [00:00<00:00, 14.57it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  61%|###6  | 17/28 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  71%|####2 | 20/28 [00:01<00:00, 17.49it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.17it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.65it/s]\u001b[A\n",
      "epoch 057 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.45it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:42:00 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.579 | nll_loss 4.045 | ppl 16.51 | wps 54151.3 | wpb 2691.4 | bsz 201.4 | num_updates 73525 | best_loss 5.45\n",
      "2021-10-08 13:42:00 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:42:03 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint57.pt (epoch 57 @ 73525 updates, score 5.579) (writing took 3.4705857609988016 seconds)\n",
      "2021-10-08 13:42:04 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
      "2021-10-08 13:42:04 | INFO | train | epoch 057 | loss 3.536 | nll_loss 1.793 | ppl 3.47 | wps 23421.6 | ups 6.34 | wpb 3695.9 | bsz 249.8 | num_updates 73525 | lr 0.000116623 | gnorm 1.333 | loss_scale 16 | train_wall 190 | wall 11871\n",
      "2021-10-08 13:42:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=58/shard_epoch=57\n",
      "2021-10-08 13:42:04 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=58/shard_epoch=58\n",
      "2021-10-08 13:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:42:04 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004968\n",
      "2021-10-08 13:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105713\n",
      "2021-10-08 13:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:42:04 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[18147]\n",
      "2021-10-08 13:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008221\n",
      "2021-10-08 13:42:04 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:42:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.914157\n",
      "2021-10-08 13:42:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.029081\n",
      "2021-10-08 13:42:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:42:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:42:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.087819\n",
      "2021-10-08 13:42:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:42:05 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[18147]\n",
      "2021-10-08 13:42:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.007852\n",
      "2021-10-08 13:42:05 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:42:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.868966\n",
      "2021-10-08 13:42:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.965620\n",
      "2021-10-08 13:42:06 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 058:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:42:06 | INFO | fairseq.trainer | begin training epoch 58\n",
      "epoch 058: 100%|9| 1289/1290 [03:17<00:00,  6.46it/s, loss=3.402, nll_loss=1.6472021-10-08 13:45:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001893\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060733\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041181\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.104505\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001446\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059730\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041140\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102924\n",
      "2021-10-08 13:45:23 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 058 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.63it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.38it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.18it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.93it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.54it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.83it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 15.95it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  64%|###8  | 18/28 [00:00<00:00, 16.96it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  75%|####5 | 21/28 [00:01<00:00, 18.36it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  82%|####9 | 23/28 [00:01<00:00, 18.78it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 19.06it/s]\u001b[A\n",
      "epoch 058 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:45:24 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 5.59 | nll_loss 4.069 | ppl 16.78 | wps 53588.4 | wpb 2691.4 | bsz 201.4 | num_updates 74815 | best_loss 5.45\n",
      "2021-10-08 13:45:24 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:45:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint58.pt (epoch 58 @ 74815 updates, score 5.59) (writing took 3.382430072000716 seconds)\n",
      "2021-10-08 13:45:28 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
      "2021-10-08 13:45:28 | INFO | train | epoch 058 | loss 3.526 | nll_loss 1.782 | ppl 3.44 | wps 23327.1 | ups 6.31 | wpb 3695.9 | bsz 249.8 | num_updates 74815 | lr 0.000115613 | gnorm 1.326 | loss_scale 32 | train_wall 191 | wall 12076\n",
      "2021-10-08 13:45:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=59/shard_epoch=58\n",
      "2021-10-08 13:45:28 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=59/shard_epoch=59\n",
      "2021-10-08 13:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:45:28 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.004935\n",
      "2021-10-08 13:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.103894\n",
      "2021-10-08 13:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:28 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[80191]\n",
      "2021-10-08 13:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008240\n",
      "2021-10-08 13:45:28 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.854993\n",
      "2021-10-08 13:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.968055\n",
      "2021-10-08 13:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.083539\n",
      "2021-10-08 13:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:29 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[80191]\n",
      "2021-10-08 13:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008220\n",
      "2021-10-08 13:45:29 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.873492\n",
      "2021-10-08 13:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.966178\n",
      "2021-10-08 13:45:30 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 059:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:45:30 | INFO | fairseq.trainer | begin training epoch 59\n",
      "epoch 059: 100%|9| 1289/1290 [03:17<00:00,  6.60it/s, loss=3.61, nll_loss=1.877,2021-10-08 13:48:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001727\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061596\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041356\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105533\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001583\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.061319\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.041386\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105147\n",
      "2021-10-08 13:48:47 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 059 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.68it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.43it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.24it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 12.01it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.63it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 15.02it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.18it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.55it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.45it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  86%|#####1| 24/28 [00:01<00:00, 18.82it/s]\u001b[A\n",
      "epoch 059 | valid on 'valid' subset:  93%|#####5| 26/28 [00:01<00:00, 19.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:48:49 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 5.614 | nll_loss 4.086 | ppl 16.98 | wps 54014.7 | wpb 2691.4 | bsz 201.4 | num_updates 76105 | best_loss 5.45\n",
      "2021-10-08 13:48:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:48:52 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint59.pt (epoch 59 @ 76105 updates, score 5.614) (writing took 3.4619724589974794 seconds)\n",
      "2021-10-08 13:48:52 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
      "2021-10-08 13:48:52 | INFO | train | epoch 059 | loss 3.517 | nll_loss 1.772 | ppl 3.41 | wps 23325.4 | ups 6.31 | wpb 3695.9 | bsz 249.8 | num_updates 76105 | lr 0.000114629 | gnorm 1.342 | loss_scale 32 | train_wall 191 | wall 12280\n",
      "2021-10-08 13:48:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=60/shard_epoch=59\n",
      "2021-10-08 13:48:52 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=60/shard_epoch=60\n",
      "2021-10-08 13:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:48:52 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005010\n",
      "2021-10-08 13:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.107777\n",
      "2021-10-08 13:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:52 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[99289]\n",
      "2021-10-08 13:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.009772\n",
      "2021-10-08 13:48:52 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.205886\n",
      "2021-10-08 13:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.324452\n",
      "2021-10-08 13:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.086852\n",
      "2021-10-08 13:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:54 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[99289]\n",
      "2021-10-08 13:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008090\n",
      "2021-10-08 13:48:54 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:00.888924\n",
      "2021-10-08 13:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:00.984875\n",
      "2021-10-08 13:48:55 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 060:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:48:55 | INFO | fairseq.trainer | begin training epoch 60\n",
      "epoch 060:  24%|2| 305/1290 [00:46<02:42,  6.06it/s, loss=3.511, nll_loss=1.765,2021-10-08 13:49:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0\n",
      "epoch 060: 100%|9| 1289/1290 [03:15<00:00,  6.66it/s, loss=3.435, nll_loss=1.6832021-10-08 13:52:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001753\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.060645\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.042192\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.105367\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler order indices time: 0:00:00.001556\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler filter_by_size time: 0:00:00.059419\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] @batch_sampler batch_by_size time: 0:00:00.040704\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | [valid] per epoch batch_sampler set-up time: 0:00:00.102333\n",
      "2021-10-08 13:52:11 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "\n",
      "epoch 060 | valid on 'valid' subset:   0%|               | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:   4%|2      | 1/28 [00:00<00:04,  6.59it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  14%|#      | 4/28 [00:00<00:02,  8.30it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  25%|#7     | 7/28 [00:00<00:02, 10.09it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  36%|##1   | 10/28 [00:00<00:01, 11.84it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  43%|##5   | 12/28 [00:00<00:01, 13.44it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  50%|###   | 14/28 [00:00<00:00, 14.83it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  57%|###4  | 16/28 [00:00<00:00, 16.03it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  68%|####  | 19/28 [00:01<00:00, 17.46it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  79%|####7 | 22/28 [00:01<00:00, 18.26it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset:  89%|#####3| 25/28 [00:01<00:00, 18.73it/s]\u001b[A\n",
      "epoch 060 | valid on 'valid' subset: 100%|######| 28/28 [00:01<00:00, 20.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2021-10-08 13:52:12 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 5.677 | nll_loss 4.164 | ppl 17.93 | wps 53641.9 | wpb 2691.4 | bsz 201.4 | num_updates 77394 | best_loss 5.45\n",
      "2021-10-08 13:52:12 | INFO | fairseq_cli.train | begin save checkpoint\n",
      "2021-10-08 13:52:16 | INFO | fairseq.checkpoint_utils | saved checkpoint /storage/master-thesis/models/resdrop_quy-es+es-en/checkpoint60.pt (epoch 60 @ 77394 updates, score 5.677) (writing took 3.4249695029975555 seconds)\n",
      "2021-10-08 13:52:16 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
      "2021-10-08 13:52:16 | INFO | train | epoch 060 | loss 3.507 | nll_loss 1.76 | ppl 3.39 | wps 23397.4 | ups 6.33 | wpb 3695.9 | bsz 249.9 | num_updates 77394 | lr 0.00011367 | gnorm 1.348 | loss_scale 16 | train_wall 190 | wall 12484\n",
      "2021-10-08 13:52:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | establishing a new set of global virtual indices for epoch=61/shard_epoch=60\n",
      "2021-10-08 13:52:16 | INFO | fairseq.data.multilingual.sampled_multi_epoch_dataset | to load next epoch/shard in next load_dataset: epoch=61/shard_epoch=61\n",
      "2021-10-08 13:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:52:16 | INFO | fairseq.data.multilingual.sampled_multi_dataset | sizes() calling time: 0:00:00.005004\n",
      "2021-10-08 13:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.105999\n",
      "2021-10-08 13:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:16 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[165178]\n",
      "2021-10-08 13:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008670\n",
      "2021-10-08 13:52:16 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.034815\n",
      "2021-10-08 13:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.150495\n",
      "2021-10-08 13:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | start batch sampler: mem usage: N/A\n",
      "2021-10-08 13:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler order indices time: 0:00:00.084937\n",
      "2021-10-08 13:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:17 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[165178]\n",
      "2021-10-08 13:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler filter_by_size time: 0:00:00.008430\n",
      "2021-10-08 13:52:17 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "2021-10-08 13:52:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] @batch_sampler batch_by_size time: 0:00:01.018029\n",
      "2021-10-08 13:52:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | [train] per epoch batch_sampler set-up time: 0:00:01.112457\n",
      "2021-10-08 13:52:18 | INFO | fairseq.tasks.translation_multi_simple_epoch | mem usage: N/A\n",
      "epoch 061:   0%|                                       | 0/1290 [00:00<?, ?it/s]2021-10-08 13:52:18 | INFO | fairseq.trainer | begin training epoch 61\n",
      "epoch 061:  16%|1| 206/1290 [00:31<02:36,  6.93it/s, loss=3.367, nll_loss=1.605,"
     ]
    }
   ],
   "source": [
    "! fairseq-train $BIN_DIR \\\n",
    "    --user-dir $CODE_STORAGE/fairseq/examples/residual_drop/residual_drop_src/ \\\n",
    "    --arch=residual_drop_transformer --share-all-embeddings \\\n",
    "    --task translation_multi_simple_epoch --lang-pairs nah-es,es-en \\\n",
    "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4096 \\\n",
    "    --restore-file $MODEL_DIR/checkpoint_last.pt \\\n",
    "    --save-dir $MODEL_DIR/ \\\n",
    "    --keep-last-epochs 2 \\\n",
    "    --reset-optimizer \\\n",
    "    --encoder-langtok \"src\" \\\n",
    "    --decoder-langtok \\\n",
    "    --fp16 \\\n",
    "    --max-epoch 200 \\\n",
    "    --patience 10 \\\n",
    "    --encoder-drop-residual 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "! echo $MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis/models/quy-es+es-en\n"
     ]
    }
   ],
   "source": [
    "cd /storage/master-thesis/models/quy-es+es-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "! rm checkpoint108.pt\n",
    "! rm checkpoint109.pt\n",
    "! rm checkpoint_best.pt\n",
    "! rm checkpoint_last.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "! rm dict.en.txt\n",
    "! rm dict.es.txt\n",
    "! rm dict.quy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5K\t/notebooks/CITATION.cff\n",
      "5.5K\t/notebooks/CODE_OF_CONDUCT.md\n",
      "15K\t/notebooks/CONTRIBUTING.md\n",
      "19K\t/notebooks/ISSUES.md\n",
      "12K\t/notebooks/LICENSE\n",
      "512\t/notebooks/MANIFEST.in\n",
      "3.5K\t/notebooks/Makefile\n",
      "41K\t/notebooks/README.md\n",
      "41K\t/notebooks/README_zh-hans.md\n",
      "42K\t/notebooks/README_zh-hant.md\n",
      "12K\t/notebooks/docker\n",
      "4.6M\t/notebooks/docs\n",
      "5.0M\t/notebooks/examples\n",
      "8.5K\t/notebooks/hubconf.py\n",
      "7.8G\t/notebooks/master-thesis\n",
      "1.5K\t/notebooks/model_cards\n",
      "9.5K\t/notebooks/notebooks\n",
      "512\t/notebooks/pyproject.toml\n",
      "64K\t/notebooks/scripts\n",
      "1.0K\t/notebooks/setup.cfg\n",
      "13K\t/notebooks/setup.py\n",
      "13M\t/notebooks/src\n",
      "731K\t/notebooks/templates\n",
      "4.5K\t/notebooks/test quy-es -> es-en model.ipynb\n",
      "6.8M\t/notebooks/tests\n",
      "147K\t/notebooks/train es-en model.ipynb\n",
      "158K\t/notebooks/train quy-es + es-en model.ipynb\n",
      "160K\t/notebooks/utils\n",
      "3.5K\t/notebooks/valohai.yaml\n",
      "7.9G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\t/notebooks/master-thesis/Untitled.ipynb\n",
      "362M\t/notebooks/master-thesis/corpora\n",
      "7.5G\t/notebooks/master-thesis/models\n",
      "7.8G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6G\t/notebooks/master-thesis/models/es-en\n",
      "802M\t/notebooks/master-thesis/models/quy-es\n",
      "5.2G\t/notebooks/master-thesis/models/quy-es+es-en\n",
      "7.5G\ttotal\n"
     ]
    }
   ],
   "source": [
    "! du -shc /notebooks/master-thesis/models/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19M\t/apex\n",
      "5.0M\t/bin\n",
      "4.0K\t/boot\n",
      "24K\t/content\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! du -shc /*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CITATION.cff         \u001b[0m\u001b[01;34mdocker\u001b[0m/          setup.py\n",
      " CODE_OF_CONDUCT.md   \u001b[01;34mdocs\u001b[0m/            \u001b[01;34msrc\u001b[0m/\n",
      " CONTRIBUTING.md      \u001b[01;34mexamples\u001b[0m/        \u001b[01;34mtemplates\u001b[0m/\n",
      " ISSUES.md            hubconf.py      'test quy-es -> es-en model.ipynb'\n",
      " LICENSE              \u001b[01;34mmaster-thesis\u001b[0m/   \u001b[01;34mtests\u001b[0m/\n",
      " MANIFEST.in          \u001b[01;34mmodel_cards\u001b[0m/    'train es-en model.ipynb'\n",
      " Makefile             \u001b[01;34mnotebooks\u001b[0m/      'train quy-es + es-en model.ipynb'\n",
      " README.md            pyproject.toml   \u001b[01;34mutils\u001b[0m/\n",
      " README_zh-hans.md    \u001b[01;34mscripts\u001b[0m/         valohai.yaml\n",
      " README_zh-hant.md    setup.cfg\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/master-thesis\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
